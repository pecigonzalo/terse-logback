{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Terse Logback \u00b6 Terse Logback is a collection of Logback extensions that shows how to use Logback effectively for structured logging , ringbuffer logging , system instrumentation , and JDBC . Using Terse Logback increases the observability of your application. Or as @mipsytipsy puts it: HOLY ROYAL SHITBALLS. To all of you who have been asking (for YEARS) if there are any alternatives to honeycomb for observability: yes, finally YES! @will_sargent has hacked together the most ingenious little solution using only logs and feature flags: https://t.co/xwdHWMlcEl \u2014 Charity Majors (@mipsytipsy) July 24, 2019 I've written about the reasoning and internal architecture in a series of blog posts. The full list is available on https://tersesystems.com . Showcase \u00b6 If you want to see a running application, there is a showcase web application that run out of the box that demonstrates some of the more advanced features, and shows you can integrate terse-logback with Sentry and Honeycomb . Quickstart \u00b6 You want to start up a project immediately and figure things out? Okay then. The project is configured into several modules. The most relevant one to start with is logback-structured-config which gives you a starting point. The logback-structured-config module contains all the logback code and the appenders, and is intended to be deployed as a small helper library for your other projects, managed through Maven and an artifact manager, or just by packaging the JAR. You can see it on mvnrepository but you will need a custom resolver, so better to read through the whole thing. This is not intended to be a drop in solution or a straight library dependency. You will want to modify this to your own tastes. Installation \u00b6 See the installation guide. Configuration \u00b6 After you've set up the resolvers and library dependencies for your build, you'll add the following to src/main/resources/logback.xml : <configuration debug= \"true\" > <include resource= \"terse-logback/default.xml\" /> </configuration> Then add a logback.conf file that contains the following: levels { ROOT = DEBUG } That should give you a fairly verbose logging setup and allow you to change the configuration. See the reference section for more details.","title":"Home"},{"location":"#terse-logback","text":"Terse Logback is a collection of Logback extensions that shows how to use Logback effectively for structured logging , ringbuffer logging , system instrumentation , and JDBC . Using Terse Logback increases the observability of your application. Or as @mipsytipsy puts it: HOLY ROYAL SHITBALLS. To all of you who have been asking (for YEARS) if there are any alternatives to honeycomb for observability: yes, finally YES! @will_sargent has hacked together the most ingenious little solution using only logs and feature flags: https://t.co/xwdHWMlcEl \u2014 Charity Majors (@mipsytipsy) July 24, 2019 I've written about the reasoning and internal architecture in a series of blog posts. The full list is available on https://tersesystems.com .","title":"Terse Logback"},{"location":"#showcase","text":"If you want to see a running application, there is a showcase web application that run out of the box that demonstrates some of the more advanced features, and shows you can integrate terse-logback with Sentry and Honeycomb .","title":"Showcase"},{"location":"#quickstart","text":"You want to start up a project immediately and figure things out? Okay then. The project is configured into several modules. The most relevant one to start with is logback-structured-config which gives you a starting point. The logback-structured-config module contains all the logback code and the appenders, and is intended to be deployed as a small helper library for your other projects, managed through Maven and an artifact manager, or just by packaging the JAR. You can see it on mvnrepository but you will need a custom resolver, so better to read through the whole thing. This is not intended to be a drop in solution or a straight library dependency. You will want to modify this to your own tastes.","title":"Quickstart"},{"location":"#installation","text":"See the installation guide.","title":"Installation"},{"location":"#configuration","text":"After you've set up the resolvers and library dependencies for your build, you'll add the following to src/main/resources/logback.xml : <configuration debug= \"true\" > <include resource= \"terse-logback/default.xml\" /> </configuration> Then add a logback.conf file that contains the following: levels { ROOT = DEBUG } That should give you a fairly verbose logging setup and allow you to change the configuration. See the reference section for more details.","title":"Configuration"},{"location":"installation/","text":"Installation \u00b6 Installation is pretty straight-forward with the exception of the instrumentation agent, which is covered seperately. Create a subproject logging and make your main codebase depend on it, but only provide slf4j-api to the main codebase. Maven \u00b6 You should install at least logback-classic : <dependency> <groupId> com.tersesystems.logback </groupId> <artifactId> logback-classic </artifactId> <version> $latestVersion </version> <type> pom </type> </dependency> and if you want a \"preconfigured\" set up you can start with logback-structured-config : <dependency> <groupId> com.tersesystems.logback </groupId> <artifactId> logback-structured-config </artifactId> <version> $latestVersion </version> <type> pom </type> </dependency> Gradle \u00b6 You should install at least logback-classic : implementation 'com.tersesystems.logback:logback-classic:<latestVersion>' and if you want a \"preconfigured\" set up you can start with logback-structured-config : implementation 'com.tersesystems.logback:logback-structured-config:<latestVersion>' SBT \u00b6 Same as Maven and Gradle. Create an SBT subproject and include it with your main build. lazy val logging = (project in file(\"logging\")).settings( libraryDependencies += \"com.tersesystems.logback\" % \"logback-structured-config\" % \"<latestVersion>\" ) lazy val impl = (project in file(\"impl\")).settings( // all your code dependencies + slf4j-api libraryDependencies += \"org.slf4j\" % \"slf4j-api\" % \"1.7.25\" ).dependsOn(logging) lazy val root = project in file(\".\").aggregate(logging, impl)","title":"Installation"},{"location":"installation/#installation","text":"Installation is pretty straight-forward with the exception of the instrumentation agent, which is covered seperately. Create a subproject logging and make your main codebase depend on it, but only provide slf4j-api to the main codebase.","title":"Installation"},{"location":"installation/#maven","text":"You should install at least logback-classic : <dependency> <groupId> com.tersesystems.logback </groupId> <artifactId> logback-classic </artifactId> <version> $latestVersion </version> <type> pom </type> </dependency> and if you want a \"preconfigured\" set up you can start with logback-structured-config : <dependency> <groupId> com.tersesystems.logback </groupId> <artifactId> logback-structured-config </artifactId> <version> $latestVersion </version> <type> pom </type> </dependency>","title":"Maven"},{"location":"installation/#gradle","text":"You should install at least logback-classic : implementation 'com.tersesystems.logback:logback-classic:<latestVersion>' and if you want a \"preconfigured\" set up you can start with logback-structured-config : implementation 'com.tersesystems.logback:logback-structured-config:<latestVersion>'","title":"Gradle"},{"location":"installation/#sbt","text":"Same as Maven and Gradle. Create an SBT subproject and include it with your main build. lazy val logging = (project in file(\"logging\")).settings( libraryDependencies += \"com.tersesystems.logback\" % \"logback-structured-config\" % \"<latestVersion>\" ) lazy val impl = (project in file(\"impl\")).settings( // all your code dependencies + slf4j-api libraryDependencies += \"org.slf4j\" % \"slf4j-api\" % \"1.7.25\" ).dependsOn(logging) lazy val root = project in file(\".\").aggregate(logging, impl)","title":"SBT"},{"location":"release-notes/","text":"Release notes were automatically generated by Shipkit 0.16.2 \u00b6 2020-04-26 - 0 commits by Will Sargent - published to Maven Central Shipkit stopped doing these automatically :-( 0.16.1 \u00b6 2020-04-23 - 5 commits by Will Sargent - published to Remove mqtools direct api exposure (#120) 0.16.0 \u00b6 2020-03-14 - 6 commits by Will Sargent - published to Travis (#119) Add copyright to everything. (#118) Add component support (#117) Use nanotime component (#116) Nanotime support (#115) 0.15.2 \u00b6 2020-03-08 - 13 commits by Will Sargent - published to Ring buffer cleanup (#114) Use lock free ring buffer (#113) Mkdocs (#112) Add uniqueid field to correlation id (#111) 0.15.1 \u00b6 2020-03-01 - 1 commit by Will Sargent - published to No pull requests referenced in commit messages. 0.15.0 \u00b6 2020-03-01 - 1 commit by Will Sargent - published to Better ring buffer (#110) 0.14.2 \u00b6 2020-02-24 - 2 commits by Will Sargent - published to Add queue to jdbc appender (#109) Add jdbc safeguards (#108) 0.14.1 \u00b6 2020-02-22 - 2 commits by Will Sargent - published to Make honeycomb service (#107) 0.14.0 \u00b6 2020-02-22 - 17 commits by Will Sargent (16), sullis (1) - published to Make honeycomb service (#107) fix the case on jdbc (#106) Update logstash (#105) Update gradle (#104) Add more jdbc changes (#103) Fix correlation tapfilter (#102) bytebuddy 1.10.8 (#101) Update readme (#100) Ensure that jdbc runs in its own pool (#99) Add JDBC appender (#98) Add correlation id (#97) Add tap filter (#96) Add span events (#94) 0.14.0 \u00b6 2020-02-17 - 15 commits by Will Sargent (14), sullis (1) - published to fix the case on jdbc (#106) Update logstash (#105) Update gradle (#104) Add more jdbc changes (#103) Fix correlation tapfilter (#102) bytebuddy 1.10.8 (#101) Update readme (#100) Ensure that jdbc runs in its own pool (#99) Add JDBC appender (#98) Add correlation id (#97) Add tap filter (#96) Add span events (#94) 0.13.2 \u00b6 2019-12-31 - 10 commits by Will Sargent - published to Update the manifest for logback-bytebuddy (#92) Remove graphstream markers. (#91) Add graphstream JSON (#90) 0.13.1 \u00b6 2019-10-05 - 228 commits by Will Sargent (226), eudes (1), Will Sargent (eero) (1) - published to More cleanup, use java formatting plugin (#89) Remove Kordamp (#88) Add versioning (#86) Use safe arguments (#85) Make return values and args safe for JSON serialization (#84) Refactor tracing (#83) Add tracing info (#82) Add wildcards and return values to logback-bytebuddy (#81) Merge back the bytebuddy-impl (#80) Move the bytebuddy advice to system classloader (#79) Add line number and source info to instrumentation (#78) Make start time available across projects (#77) Update with postgresql JSON appender (#76) Add OKHTTP honeycomb client (#75) Add OKHTTP client (#74) Make playws be 2.12 specific (#73) this one goes up to 11 (#72) Add playws as a service (#71) Add honeycomb support (#70) ByteBuddy agent (#69) Add sifting ringbuffer (#68) Allow bytebuddy instrumentation from config (#67) Ring buffer appender (#66) Add sigar (#65) Add ringbuffer-event (#64) Add ring buffer logging (#63) Add launch darkly test (#62) Add turbomarker (#61) Commit more providers (#60) Add JSON exception of structured config (#59) Exception Mapping (#58) Add exception message converter (#57) Update example (#56) Update example (#55) Update example (#54) Add budget evaluator (#53) Misc improvements to bytebuddy module (#52) Add transforming encoder (#51) Make core and classic distinct (#50) compress encoder (#49) add logslack (#48) Move the API stuff out to the different project (#47) Add decorator pattern (#46) First pass of select/composite appenders (#45) Audio markers and appenders (#44) Add level change propagator (#43) Update the versions and strip down builds (#42) Update README (#41) Add config list converter (#40) Fix config bugs (#38) Add slf4j generator (#37) Use an ID generator and sequence number for centralized logging issues (#35) Update the readme. (#34) better censor tests (#33) Make censor implementation not depend on typesafe-config, add docs (#32) Add external javadocs (#31) Add typesafe-config as dependency of structured config. (#30) More typesafeconfig stuff (#29) Update references to classic in README (#28) Rationalize context (#27) Fix typesafe config action to use local scope (#26) Move censor to logback-core (#25) fix copyright (#24) Add tests (#23) Add better type names (#22) Enable censored filter (#21) Add enabled filter (#20) Add guide submodule (#16) Add javadoc with external links (#15) Move from 'classic' to 'structured-config' (#14) Add travis flag (#13) Use default interfaces for proxy (#12) Update published README (#11) signing (#10) set gradle to 0.1.0 (#9) Add signing and bintray publishing (#8) Update description (#7) fix logs (#6) enable travis check (#5) switch to gradle (#4) Make logback-core, remove tracer from context (#3) Update to use maven (#2) Update with better lazy logging (#1)","title":"Release notes"},{"location":"release-notes/#0162","text":"2020-04-26 - 0 commits by Will Sargent - published to Maven Central Shipkit stopped doing these automatically :-(","title":"0.16.2"},{"location":"release-notes/#0161","text":"2020-04-23 - 5 commits by Will Sargent - published to Remove mqtools direct api exposure (#120)","title":"0.16.1"},{"location":"release-notes/#0160","text":"2020-03-14 - 6 commits by Will Sargent - published to Travis (#119) Add copyright to everything. (#118) Add component support (#117) Use nanotime component (#116) Nanotime support (#115)","title":"0.16.0"},{"location":"release-notes/#0152","text":"2020-03-08 - 13 commits by Will Sargent - published to Ring buffer cleanup (#114) Use lock free ring buffer (#113) Mkdocs (#112) Add uniqueid field to correlation id (#111)","title":"0.15.2"},{"location":"release-notes/#0151","text":"2020-03-01 - 1 commit by Will Sargent - published to No pull requests referenced in commit messages.","title":"0.15.1"},{"location":"release-notes/#0150","text":"2020-03-01 - 1 commit by Will Sargent - published to Better ring buffer (#110)","title":"0.15.0"},{"location":"release-notes/#0142","text":"2020-02-24 - 2 commits by Will Sargent - published to Add queue to jdbc appender (#109) Add jdbc safeguards (#108)","title":"0.14.2"},{"location":"release-notes/#0141","text":"2020-02-22 - 2 commits by Will Sargent - published to Make honeycomb service (#107)","title":"0.14.1"},{"location":"release-notes/#0140","text":"2020-02-22 - 17 commits by Will Sargent (16), sullis (1) - published to Make honeycomb service (#107) fix the case on jdbc (#106) Update logstash (#105) Update gradle (#104) Add more jdbc changes (#103) Fix correlation tapfilter (#102) bytebuddy 1.10.8 (#101) Update readme (#100) Ensure that jdbc runs in its own pool (#99) Add JDBC appender (#98) Add correlation id (#97) Add tap filter (#96) Add span events (#94)","title":"0.14.0"},{"location":"release-notes/#0140_1","text":"2020-02-17 - 15 commits by Will Sargent (14), sullis (1) - published to fix the case on jdbc (#106) Update logstash (#105) Update gradle (#104) Add more jdbc changes (#103) Fix correlation tapfilter (#102) bytebuddy 1.10.8 (#101) Update readme (#100) Ensure that jdbc runs in its own pool (#99) Add JDBC appender (#98) Add correlation id (#97) Add tap filter (#96) Add span events (#94)","title":"0.14.0"},{"location":"release-notes/#0132","text":"2019-12-31 - 10 commits by Will Sargent - published to Update the manifest for logback-bytebuddy (#92) Remove graphstream markers. (#91) Add graphstream JSON (#90)","title":"0.13.2"},{"location":"release-notes/#0131","text":"2019-10-05 - 228 commits by Will Sargent (226), eudes (1), Will Sargent (eero) (1) - published to More cleanup, use java formatting plugin (#89) Remove Kordamp (#88) Add versioning (#86) Use safe arguments (#85) Make return values and args safe for JSON serialization (#84) Refactor tracing (#83) Add tracing info (#82) Add wildcards and return values to logback-bytebuddy (#81) Merge back the bytebuddy-impl (#80) Move the bytebuddy advice to system classloader (#79) Add line number and source info to instrumentation (#78) Make start time available across projects (#77) Update with postgresql JSON appender (#76) Add OKHTTP honeycomb client (#75) Add OKHTTP client (#74) Make playws be 2.12 specific (#73) this one goes up to 11 (#72) Add playws as a service (#71) Add honeycomb support (#70) ByteBuddy agent (#69) Add sifting ringbuffer (#68) Allow bytebuddy instrumentation from config (#67) Ring buffer appender (#66) Add sigar (#65) Add ringbuffer-event (#64) Add ring buffer logging (#63) Add launch darkly test (#62) Add turbomarker (#61) Commit more providers (#60) Add JSON exception of structured config (#59) Exception Mapping (#58) Add exception message converter (#57) Update example (#56) Update example (#55) Update example (#54) Add budget evaluator (#53) Misc improvements to bytebuddy module (#52) Add transforming encoder (#51) Make core and classic distinct (#50) compress encoder (#49) add logslack (#48) Move the API stuff out to the different project (#47) Add decorator pattern (#46) First pass of select/composite appenders (#45) Audio markers and appenders (#44) Add level change propagator (#43) Update the versions and strip down builds (#42) Update README (#41) Add config list converter (#40) Fix config bugs (#38) Add slf4j generator (#37) Use an ID generator and sequence number for centralized logging issues (#35) Update the readme. (#34) better censor tests (#33) Make censor implementation not depend on typesafe-config, add docs (#32) Add external javadocs (#31) Add typesafe-config as dependency of structured config. (#30) More typesafeconfig stuff (#29) Update references to classic in README (#28) Rationalize context (#27) Fix typesafe config action to use local scope (#26) Move censor to logback-core (#25) fix copyright (#24) Add tests (#23) Add better type names (#22) Enable censored filter (#21) Add enabled filter (#20) Add guide submodule (#16) Add javadoc with external links (#15) Move from 'classic' to 'structured-config' (#14) Add travis flag (#13) Use default interfaces for proxy (#12) Update published README (#11) signing (#10) set gradle to 0.1.0 (#9) Add signing and bintray publishing (#8) Update description (#7) fix logs (#6) enable travis check (#5) switch to gradle (#4) Make logback-core, remove tracer from context (#3) Update to use maven (#2) Update with better lazy logging (#1)","title":"0.13.1"},{"location":"structured-logging/","text":"What is Structured Logging? \u00b6 Structured logging is logging data in a structure, so everything has a specific key and a value and can be parsed and processed by tools. Technically, you could be logging in another structure like XML or YAML, but almost everyone uses JSON. Technically, since there are several JSON objects all in one file / stream, this is called \"newline delimited JSON\" or NDJSON or jsonlines . If you only output JSON it's not a huge deal, because you can read JSON logs as text with a special log viewer such as jl . Semantically, a log entry typically has multiple pieces of information associated with it, described as \"high cardinality\" by observability geeks. Structured logging means that the cardinality goes from \"closed\" -- you can only log things that you have defined fields for -- to \"open\", where you can add arbitrary fields and objects to your log entry as long as it's JSON. Structured logging means that you can add more context to logs and do more with them without having to do regexes. History \u00b6 Structured logging has been around for a while. Using JSON for structured logging is a little more recent. The earliest reference I can find to JSON logging is Logging in JSON in 2006, followed by Write Logs for Machines, use JSON in 2011, both referencing Node.js. Structured Logging in 2015 discusses JSON based logging in Ruby. Adding Context \u00b6 There is a question of what you want to add when you log. This is a matter of taste, but in general you should log so that you create a consistent narrative . As previously mentioned, a log may indicate a portion of an event, so you want to log where doing so would help tell a story of what happened afterwards. There are some things you should always add to an event , such as who is talking to your service, what they're asking, business relevant fields, additional context around your service / environment, response time and particulars. You should add units to your field names when you measure a quantity, i.e. response_time_ms , and add a \"human readable\" version of internal information if available. You should add context to your logs that helps differentiate it from its peers, so you never have to guess where the source of a log is coming from. Adding a correlation id helps you design for results and tie your logs into a coherent event. You don't need to use a UUID: a flake id will probably be better for you. I'm using idem here, but most things will work. So, we know what structured logging is now. What does it look like in SLF4J? Adding Structure to Logging \u00b6 SLF4J doesn't have specific support for structured logging, but logstash-logback-encoder does. It's complete and comprehensive, but buried in a section called Event specific custom fields . Event specific custom fields are implemented in two ways: through net.logstash.logback.argument.StructuredArguments , which adds structured information through parameters, and net.logstash.logback.marker.Markers , which adds structured information through the org.slf4j.Marker API. StructuredArguments \u00b6 StructuredArguments write out both to the text appenders and to the JSON appenders. There is extra \"key information\" added to the JSON, and you see the value show up in the message. package example ; import org.slf4j.Logger ; import static net.logstash.logback.argument.StructuredArguments.* ; import static org.slf4j.LoggerFactory.* ; public class ClassWithStructuredArguments { private final Logger logger = getLogger ( getClass ()); public void logValue ( String correlationId ) { if ( logger . isInfoEnabled ()) { logger . info ( \"id is {}\" , value ( \"correlationId\" , correlationId )); } } public void logNameAndValue ( String correlationId ) { logger . info ( \"id is {}\" , keyValue ( \"correlationId\" , correlationId )); } public void logNameAndValueWithFormat ( String correlationId ) { logger . info ( \"id is {}\" , keyValue ( \"correlationId\" , correlationId , \"{0}=[{1}]\" )); } public void doThings ( String correlationId ) { logValue ( correlationId ); logNameAndValue ( correlationId ); logNameAndValueWithFormat ( correlationId ); } public static void main ( String [] args ) { String correlationId = IdGenerator . getInstance (). generateCorrelationId (); ClassWithStructuredArguments classWithStructuredArguments = new ClassWithStructuredArguments (); classWithStructuredArguments . doThings ( correlationId ); } } This produces the following output in text: 2019-01-20T23:24:40.004+0000 [INFO ] example.ClassWithStructuredArguments in main - id is FXtylIyzDbj9rfs7BRCAAA 2019-01-20T23:24:40.006+0000 [INFO ] example.ClassWithStructuredArguments in main - id is correlationId=FXtylIyzDbj9rfs7BRCAAA 2019-01-20T23:24:40.006+0000 [INFO ] example.ClassWithStructuredArguments in main - id is correlationId=[FXtylIyzDbj9rfs7BRCAAA] and in JSON: { \"@timestamp\" : \"2019-01-20T23:24:40.004+00:00\" , \"@version\" : \"1\" , \"message\" : \"id is FXtylIyzDbj9rfs7BRCAAA\" , \"logger_name\" : \"example.ClassWithStructuredArguments\" , \"thread_name\" : \"main\" , \"level\" : \"INFO\" , \"level_value\" : 20000 , \"correlationId\" : \"FXtylIyzDbj9rfs7BRCAAA\" } { \"@timestamp\" : \"2019-01-20T23:24:40.006+00:00\" , \"@version\" : \"1\" , \"message\" : \"id is correlationId=FXtylIyzDbj9rfs7BRCAAA\" , \"logger_name\" : \"example.ClassWithStructuredArguments\" , \"thread_name\" : \"main\" , \"level\" : \"INFO\" , \"level_value\" : 20000 , \"correlationId\" : \"FXtylIyzDbj9rfs7BRCAAA\" } { \"@timestamp\" : \"2019-01-20T23:24:40.006+00:00\" , \"@version\" : \"1\" , \"message\" : \"id is correlationId=[FXtylIyzDbj9rfs7BRCAAA]\" , \"logger_name\" : \"example.ClassWithStructuredArguments\" , \"thread_name\" : \"main\" , \"level\" : \"INFO\" , \"level_value\" : 20000 , \"correlationId\" : \"FXtylIyzDbj9rfs7BRCAAA\" } Markers \u00b6 If you want to add more context and don't want it to show up in the message, you can use net.logstash.logback.marker.Markers instead. package example ; import net.logstash.logback.marker.LogstashMarker ; import net.logstash.logback.marker.Markers ; import org.slf4j.Logger ; import static org.slf4j.LoggerFactory.getLogger ; public class ClassWithMarkers { private final Logger logger = getLogger ( getClass ()); public void doThingsWithMarker ( String correlationId ) { LogstashMarker logstashMarker = Markers . append ( \"correlationId\" , correlationId ); logger . info ( logstashMarker , \"log with marker explicitly\" ); } public static void main ( String [] args ) { String correlationId = IdGenerator . getInstance (). generateCorrelationId (); ClassWithMarkers classWithMarkers = new ClassWithMarkers (); classWithMarkers . doThingsWithMarker ( correlationId ); } } This produces the following text: 2019-01-20T23:26:50.351+0000 [INFO ] example.ClassWithMarkers in main - log with marker explicitly and the following JSON: { \"@timestamp\" : \"2019-01-20T23:26:50.351+00:00\" , \"@version\" : \"1\" , \"message\" : \"log with marker explicitly\" , \"logger_name\" : \"example.ClassWithMarkers\" , \"thread_name\" : \"main\" , \"level\" : \"INFO\" , \"level_value\" : 20000 , \"correlationId\" : \"FXtylIy0T878gCNIdfWAAA\" } When you're using structured logging, you'll inevitably have to pass around the LogstashMarker or StructuredArgument with it so that you can add context to your logging. In the past, the recommended way to do this was MDC. Avoid Mapped Diagnostic Context . MDC is a well known way of adding context to logging, but there are several things that make it problematic. MDC does not deal well with multi-threaded applications which may pass execution between several threads. Code that uses CompletableFuture and ExecutorService may not work reliably with MDC. A child thread does not automatically inherit a copy of the mapped diagnostic context of its parent. MDC also breaks silently: when MDC assumptions are violated, there is no indication that the wrong contextual information is being displayed.","title":"Structured Logging"},{"location":"structured-logging/#what-is-structured-logging","text":"Structured logging is logging data in a structure, so everything has a specific key and a value and can be parsed and processed by tools. Technically, you could be logging in another structure like XML or YAML, but almost everyone uses JSON. Technically, since there are several JSON objects all in one file / stream, this is called \"newline delimited JSON\" or NDJSON or jsonlines . If you only output JSON it's not a huge deal, because you can read JSON logs as text with a special log viewer such as jl . Semantically, a log entry typically has multiple pieces of information associated with it, described as \"high cardinality\" by observability geeks. Structured logging means that the cardinality goes from \"closed\" -- you can only log things that you have defined fields for -- to \"open\", where you can add arbitrary fields and objects to your log entry as long as it's JSON. Structured logging means that you can add more context to logs and do more with them without having to do regexes.","title":"What is Structured Logging?"},{"location":"structured-logging/#history","text":"Structured logging has been around for a while. Using JSON for structured logging is a little more recent. The earliest reference I can find to JSON logging is Logging in JSON in 2006, followed by Write Logs for Machines, use JSON in 2011, both referencing Node.js. Structured Logging in 2015 discusses JSON based logging in Ruby.","title":"History"},{"location":"structured-logging/#adding-context","text":"There is a question of what you want to add when you log. This is a matter of taste, but in general you should log so that you create a consistent narrative . As previously mentioned, a log may indicate a portion of an event, so you want to log where doing so would help tell a story of what happened afterwards. There are some things you should always add to an event , such as who is talking to your service, what they're asking, business relevant fields, additional context around your service / environment, response time and particulars. You should add units to your field names when you measure a quantity, i.e. response_time_ms , and add a \"human readable\" version of internal information if available. You should add context to your logs that helps differentiate it from its peers, so you never have to guess where the source of a log is coming from. Adding a correlation id helps you design for results and tie your logs into a coherent event. You don't need to use a UUID: a flake id will probably be better for you. I'm using idem here, but most things will work. So, we know what structured logging is now. What does it look like in SLF4J?","title":"Adding Context"},{"location":"structured-logging/#adding-structure-to-logging","text":"SLF4J doesn't have specific support for structured logging, but logstash-logback-encoder does. It's complete and comprehensive, but buried in a section called Event specific custom fields . Event specific custom fields are implemented in two ways: through net.logstash.logback.argument.StructuredArguments , which adds structured information through parameters, and net.logstash.logback.marker.Markers , which adds structured information through the org.slf4j.Marker API.","title":"Adding Structure to Logging"},{"location":"structured-logging/#structuredarguments","text":"StructuredArguments write out both to the text appenders and to the JSON appenders. There is extra \"key information\" added to the JSON, and you see the value show up in the message. package example ; import org.slf4j.Logger ; import static net.logstash.logback.argument.StructuredArguments.* ; import static org.slf4j.LoggerFactory.* ; public class ClassWithStructuredArguments { private final Logger logger = getLogger ( getClass ()); public void logValue ( String correlationId ) { if ( logger . isInfoEnabled ()) { logger . info ( \"id is {}\" , value ( \"correlationId\" , correlationId )); } } public void logNameAndValue ( String correlationId ) { logger . info ( \"id is {}\" , keyValue ( \"correlationId\" , correlationId )); } public void logNameAndValueWithFormat ( String correlationId ) { logger . info ( \"id is {}\" , keyValue ( \"correlationId\" , correlationId , \"{0}=[{1}]\" )); } public void doThings ( String correlationId ) { logValue ( correlationId ); logNameAndValue ( correlationId ); logNameAndValueWithFormat ( correlationId ); } public static void main ( String [] args ) { String correlationId = IdGenerator . getInstance (). generateCorrelationId (); ClassWithStructuredArguments classWithStructuredArguments = new ClassWithStructuredArguments (); classWithStructuredArguments . doThings ( correlationId ); } } This produces the following output in text: 2019-01-20T23:24:40.004+0000 [INFO ] example.ClassWithStructuredArguments in main - id is FXtylIyzDbj9rfs7BRCAAA 2019-01-20T23:24:40.006+0000 [INFO ] example.ClassWithStructuredArguments in main - id is correlationId=FXtylIyzDbj9rfs7BRCAAA 2019-01-20T23:24:40.006+0000 [INFO ] example.ClassWithStructuredArguments in main - id is correlationId=[FXtylIyzDbj9rfs7BRCAAA] and in JSON: { \"@timestamp\" : \"2019-01-20T23:24:40.004+00:00\" , \"@version\" : \"1\" , \"message\" : \"id is FXtylIyzDbj9rfs7BRCAAA\" , \"logger_name\" : \"example.ClassWithStructuredArguments\" , \"thread_name\" : \"main\" , \"level\" : \"INFO\" , \"level_value\" : 20000 , \"correlationId\" : \"FXtylIyzDbj9rfs7BRCAAA\" } { \"@timestamp\" : \"2019-01-20T23:24:40.006+00:00\" , \"@version\" : \"1\" , \"message\" : \"id is correlationId=FXtylIyzDbj9rfs7BRCAAA\" , \"logger_name\" : \"example.ClassWithStructuredArguments\" , \"thread_name\" : \"main\" , \"level\" : \"INFO\" , \"level_value\" : 20000 , \"correlationId\" : \"FXtylIyzDbj9rfs7BRCAAA\" } { \"@timestamp\" : \"2019-01-20T23:24:40.006+00:00\" , \"@version\" : \"1\" , \"message\" : \"id is correlationId=[FXtylIyzDbj9rfs7BRCAAA]\" , \"logger_name\" : \"example.ClassWithStructuredArguments\" , \"thread_name\" : \"main\" , \"level\" : \"INFO\" , \"level_value\" : 20000 , \"correlationId\" : \"FXtylIyzDbj9rfs7BRCAAA\" }","title":"StructuredArguments"},{"location":"structured-logging/#markers","text":"If you want to add more context and don't want it to show up in the message, you can use net.logstash.logback.marker.Markers instead. package example ; import net.logstash.logback.marker.LogstashMarker ; import net.logstash.logback.marker.Markers ; import org.slf4j.Logger ; import static org.slf4j.LoggerFactory.getLogger ; public class ClassWithMarkers { private final Logger logger = getLogger ( getClass ()); public void doThingsWithMarker ( String correlationId ) { LogstashMarker logstashMarker = Markers . append ( \"correlationId\" , correlationId ); logger . info ( logstashMarker , \"log with marker explicitly\" ); } public static void main ( String [] args ) { String correlationId = IdGenerator . getInstance (). generateCorrelationId (); ClassWithMarkers classWithMarkers = new ClassWithMarkers (); classWithMarkers . doThingsWithMarker ( correlationId ); } } This produces the following text: 2019-01-20T23:26:50.351+0000 [INFO ] example.ClassWithMarkers in main - log with marker explicitly and the following JSON: { \"@timestamp\" : \"2019-01-20T23:26:50.351+00:00\" , \"@version\" : \"1\" , \"message\" : \"log with marker explicitly\" , \"logger_name\" : \"example.ClassWithMarkers\" , \"thread_name\" : \"main\" , \"level\" : \"INFO\" , \"level_value\" : 20000 , \"correlationId\" : \"FXtylIy0T878gCNIdfWAAA\" } When you're using structured logging, you'll inevitably have to pass around the LogstashMarker or StructuredArgument with it so that you can add context to your logging. In the past, the recommended way to do this was MDC. Avoid Mapped Diagnostic Context . MDC is a well known way of adding context to logging, but there are several things that make it problematic. MDC does not deal well with multi-threaded applications which may pass execution between several threads. Code that uses CompletableFuture and ExecutorService may not work reliably with MDC. A child thread does not automatically inherit a copy of the mapped diagnostic context of its parent. MDC also breaks silently: when MDC assumptions are violated, there is no indication that the wrong contextual information is being displayed.","title":"Markers"},{"location":"guide/audio/","text":"Audio \u00b6 The audio appender uses a system beep configured through SystemPlayer to notify on warnings and errors, and limits excessive beeps with a budget evaluator. The XML is as follows: <included> <appender name= \"AUDIO-WARN\" class= \"com.tersesystems.logback.audio.AudioAppender\" > <filter class= \"ch.qos.logback.classic.filter.LevelFilter\" > <level> WARN </level> <onMatch> NEUTRAL </onMatch> <onMismatch> DENY </onMismatch> </filter> <player class= \"com.tersesystems.logback.audio.SystemPlayer\" /> </appender> <appender name= \"AUDIO-ERROR\" class= \"com.tersesystems.logback.audio.AudioAppender\" > <filter class= \"ch.qos.logback.classic.filter.LevelFilter\" > <level> ERROR </level> <onMatch> ACCEPT </onMatch> <onMismatch> DENY </onMismatch> </filter> <player class= \"com.tersesystems.logback.audio.SystemPlayer\" /> </appender> <appender name= \"AUDIO\" class= \"com.tersesystems.logback.core.CompositeAppender\" > <filter class= \"ch.qos.logback.core.filter.EvaluatorFilter\" > <evaluator class= \"com.tersesystems.logback.budget.BudgetEvaluator\" > <budgetRule name= \"WARN\" threshold= \"1\" interval= \"5\" timeUnit= \"seconds\" /> <budgetRule name= \"ERROR\" threshold= \"1\" interval= \"5\" timeUnit= \"seconds\" /> </evaluator> <OnMismatch> DENY </OnMismatch> <OnMatch> NEUTRAL </OnMatch> </filter> <appender-ref ref= \"AUDIO-WARN\" /> <appender-ref ref= \"AUDIO-ERROR\" /> </appender> </included> See Application Logging in Java: Appenders for more details.","title":"Audio"},{"location":"guide/audio/#audio","text":"The audio appender uses a system beep configured through SystemPlayer to notify on warnings and errors, and limits excessive beeps with a budget evaluator. The XML is as follows: <included> <appender name= \"AUDIO-WARN\" class= \"com.tersesystems.logback.audio.AudioAppender\" > <filter class= \"ch.qos.logback.classic.filter.LevelFilter\" > <level> WARN </level> <onMatch> NEUTRAL </onMatch> <onMismatch> DENY </onMismatch> </filter> <player class= \"com.tersesystems.logback.audio.SystemPlayer\" /> </appender> <appender name= \"AUDIO-ERROR\" class= \"com.tersesystems.logback.audio.AudioAppender\" > <filter class= \"ch.qos.logback.classic.filter.LevelFilter\" > <level> ERROR </level> <onMatch> ACCEPT </onMatch> <onMismatch> DENY </onMismatch> </filter> <player class= \"com.tersesystems.logback.audio.SystemPlayer\" /> </appender> <appender name= \"AUDIO\" class= \"com.tersesystems.logback.core.CompositeAppender\" > <filter class= \"ch.qos.logback.core.filter.EvaluatorFilter\" > <evaluator class= \"com.tersesystems.logback.budget.BudgetEvaluator\" > <budgetRule name= \"WARN\" threshold= \"1\" interval= \"5\" timeUnit= \"seconds\" /> <budgetRule name= \"ERROR\" threshold= \"1\" interval= \"5\" timeUnit= \"seconds\" /> </evaluator> <OnMismatch> DENY </OnMismatch> <OnMatch> NEUTRAL </OnMatch> </filter> <appender-ref ref= \"AUDIO-WARN\" /> <appender-ref ref= \"AUDIO-ERROR\" /> </appender> </included> See Application Logging in Java: Appenders for more details.","title":"Audio"},{"location":"guide/budget/","text":"Budget Aware Logging \u00b6 There are instances where loggers may be overly chatty, and will log more than necessary. Rather than hunt down all the individual loggers and whitelist or blacklist the lot of them, you may want to assign a budget that will budget INFO messages to 5 statements a second. This is easy to do with the logback-budget module, which uses an internal circuit breaker to regulate the flow of messages. <configuration> <newRule pattern= \"*/budget-rule\" actionClass= \"com.tersesystems.logback.budget.BudgetRuleAction\" /> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <filter class= \"ch.qos.logback.core.filter.EvaluatorFilter\" > <evaluator class= \"com.tersesystems.logback.budget.BudgetEvaluator\" > <budget-rule name= \"INFO\" threshold= \"5\" interval= \"1\" timeUnit= \"seconds\" /> </evaluator> <OnMismatch> DENY </OnMismatch> <OnMatch> NEUTRAL </OnMatch> </filter> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <root level= \"TRACE\" > <appender-ref ref= \"STDOUT\" /> </root> </configuration> See Application Logging in Java: Filters for more details.","title":"Budgeting"},{"location":"guide/budget/#budget-aware-logging","text":"There are instances where loggers may be overly chatty, and will log more than necessary. Rather than hunt down all the individual loggers and whitelist or blacklist the lot of them, you may want to assign a budget that will budget INFO messages to 5 statements a second. This is easy to do with the logback-budget module, which uses an internal circuit breaker to regulate the flow of messages. <configuration> <newRule pattern= \"*/budget-rule\" actionClass= \"com.tersesystems.logback.budget.BudgetRuleAction\" /> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <filter class= \"ch.qos.logback.core.filter.EvaluatorFilter\" > <evaluator class= \"com.tersesystems.logback.budget.BudgetEvaluator\" > <budget-rule name= \"INFO\" threshold= \"5\" interval= \"1\" timeUnit= \"seconds\" /> </evaluator> <OnMismatch> DENY </OnMismatch> <OnMatch> NEUTRAL </OnMatch> </filter> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <root level= \"TRACE\" > <appender-ref ref= \"STDOUT\" /> </root> </configuration> See Application Logging in Java: Filters for more details.","title":"Budget Aware Logging"},{"location":"guide/censor/","text":"Censors \u00b6 There may be sensitive information that you don't want to show up in the logs. You can get around this by passing your information through a censor. This is a custom bit of code written for Logback, but it's not too complex. There are two rules and a converter that are used in Logback to define and reference censors: CensorAction , CensorRefAction and the censor converter. <configuration> <newRule pattern= \"*/censor\" actionClass= \"com.tersesystems.logback.censor.CensorAction\" /> <newRule pattern= \"*/censor-ref\" actionClass= \"com.tersesystems.logback.censor.CensorRefAction\" /> <conversionRule conversionWord= \"censor\" converterClass= \"com.tersesystems.logback.censor.CensorConverter\" /> <!-- ... --> </configuration> The CensorAction defines a censor that can be referred to by the CensorRef action and the censor conversionWord, using the censor name. The default implementation is the regex censor, which will look for a regular expression and replace it with the replacement text defined: <configuration> <censor name= \"censor-name1\" class= \"com.tersesystems.logback.censor.RegexCensor\" > <replacementText> [CENSORED BY CENSOR1] </replacementText> <regex> hunter1 </regex> </censor> <censor name= \"censor-name2\" class= \"com.tersesystems.logback.censor.RegexCensor\" > <replacementText> [CENSORED BY CENSOR2] </replacementText> <regex> hunter2 </regex> </censor> </configuration> Once you have the censors defined, you can use the censor word by specifying the target as defined in the pattern encoder format , and adding the name as the option list using curly braces, i.e. %censor(%msg){censor-name1} . If you don't define the censor, then the first available censor will be picked. <configuration> <appender name= \"TEST1\" class= \"ch.qos.logback.core.FileAppender\" > <file> file1.log </file> <encoder> <pattern> %censor(%msg){censor-name1}%n </pattern> </encoder> </appender> <appender name= \"TEST2\" class= \"ch.qos.logback.core.FileAppender\" > <file> file2.log </file> <encoder> <pattern> %censor(%msg){censor-name2}%n </pattern> </encoder> </appender> </configuration> If you are working with a componentized framework, you'll want to use the censor-ref action instead. Here's an example using logstash-logback-encoder. <configuration> <appender name= \"TEST3\" class= \"ch.qos.logback.core.FileAppender\" > <file> file3.log </file> <encoder class= \"net.logstash.logback.encoder.LogstashEncoder\" > <jsonGeneratorDecorator class= \"com.tersesystems.logback.censor.CensoringJsonGeneratorDecorator\" > <censor-ref ref= \"json-censor\" /> </jsonGeneratorDecorator> </encoder> </appender> </configuration> In this case, CensoringJsonGeneratorDecorator implements the CensorAttachable interface and so will run message text through the censor if it exists. See Application Logging in Java: Converters for more details.","title":"Censors"},{"location":"guide/censor/#censors","text":"There may be sensitive information that you don't want to show up in the logs. You can get around this by passing your information through a censor. This is a custom bit of code written for Logback, but it's not too complex. There are two rules and a converter that are used in Logback to define and reference censors: CensorAction , CensorRefAction and the censor converter. <configuration> <newRule pattern= \"*/censor\" actionClass= \"com.tersesystems.logback.censor.CensorAction\" /> <newRule pattern= \"*/censor-ref\" actionClass= \"com.tersesystems.logback.censor.CensorRefAction\" /> <conversionRule conversionWord= \"censor\" converterClass= \"com.tersesystems.logback.censor.CensorConverter\" /> <!-- ... --> </configuration> The CensorAction defines a censor that can be referred to by the CensorRef action and the censor conversionWord, using the censor name. The default implementation is the regex censor, which will look for a regular expression and replace it with the replacement text defined: <configuration> <censor name= \"censor-name1\" class= \"com.tersesystems.logback.censor.RegexCensor\" > <replacementText> [CENSORED BY CENSOR1] </replacementText> <regex> hunter1 </regex> </censor> <censor name= \"censor-name2\" class= \"com.tersesystems.logback.censor.RegexCensor\" > <replacementText> [CENSORED BY CENSOR2] </replacementText> <regex> hunter2 </regex> </censor> </configuration> Once you have the censors defined, you can use the censor word by specifying the target as defined in the pattern encoder format , and adding the name as the option list using curly braces, i.e. %censor(%msg){censor-name1} . If you don't define the censor, then the first available censor will be picked. <configuration> <appender name= \"TEST1\" class= \"ch.qos.logback.core.FileAppender\" > <file> file1.log </file> <encoder> <pattern> %censor(%msg){censor-name1}%n </pattern> </encoder> </appender> <appender name= \"TEST2\" class= \"ch.qos.logback.core.FileAppender\" > <file> file2.log </file> <encoder> <pattern> %censor(%msg){censor-name2}%n </pattern> </encoder> </appender> </configuration> If you are working with a componentized framework, you'll want to use the censor-ref action instead. Here's an example using logstash-logback-encoder. <configuration> <appender name= \"TEST3\" class= \"ch.qos.logback.core.FileAppender\" > <file> file3.log </file> <encoder class= \"net.logstash.logback.encoder.LogstashEncoder\" > <jsonGeneratorDecorator class= \"com.tersesystems.logback.censor.CensoringJsonGeneratorDecorator\" > <censor-ref ref= \"json-censor\" /> </jsonGeneratorDecorator> </encoder> </appender> </configuration> In this case, CensoringJsonGeneratorDecorator implements the CensorAttachable interface and so will run message text through the censor if it exists. See Application Logging in Java: Converters for more details.","title":"Censors"},{"location":"guide/composite/","text":"Composite Appender \u00b6 The composite appender presents a single appender and appends to several appenders. It is very useful for referring to a list of appenders by a single name. <configuration debug= \"true\" > <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.read.ListAppender\" > </appender> <appender name= \"FILE\" class= \"ch.qos.logback.core.read.ListAppender\" > </appender> <appender name= \"CONSOLE_AND_FILE\" class= \"com.tersesystems.logback.core.CompositeAppender\" > <appender-ref ref= \"CONSOLE\" /> <appender-ref ref= \"FILE\" /> </appender> <root level= \"TRACE\" > <appender-ref ref= \"CONSOLE_AND_FILE\" /> </root> </configuration> You can leverage nesting to keep your filtering logic under control. For example, you may want to have several things happen when you hit an error in your logs. Appenders will always write when they receive an event, unless they are filtered. Using nesting, you can declare the filter once, and have the child appenders \"inherit\" that filter: <!-- Filter is on the appender chain --> <appender name=\"ERROR-APPENDER\" class=\"com.tersesystems.logback.CompositeAppender\"> <filter class=\"ch.qos.logback.classic.filter.LevelFilter\"> <level>ERROR</level> <onMatch>ACCEPT</onMatch> <onMismatch>DENY</onMismatch> </filter> <appender class=\"ch.qos.logback.core.FileAppender\"> <file>error.log</file> <encoder> <pattern>%date - %message</pattern> </encoder> </appender> <appender class=\"com.tersesystems.logback.audio.AudioAppender\"> <player class=\"com.tersesystems.logback.audio.ResourcePlayer\"> <resource>/error.ogg</resource> </player> </appender> </appender> <root level=\"TRACE\"> <appender-ref ref=\"ALL-APPENDER\"/> <appender-ref ref=\"TRACE-APPENDER\"/> <appender-ref ref=\"DEBUG-APPENDER\"/> <appender-ref ref=\"INFO-APPENDER\"/> <appender-ref ref=\"WARN-APPENDER\"/> <appender-ref ref=\"ERROR-APPENDER\"/> </root> This makes your appender logic much cleaner. See Application Logging in Java: Appenders for more details.","title":"Composite"},{"location":"guide/composite/#composite-appender","text":"The composite appender presents a single appender and appends to several appenders. It is very useful for referring to a list of appenders by a single name. <configuration debug= \"true\" > <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.read.ListAppender\" > </appender> <appender name= \"FILE\" class= \"ch.qos.logback.core.read.ListAppender\" > </appender> <appender name= \"CONSOLE_AND_FILE\" class= \"com.tersesystems.logback.core.CompositeAppender\" > <appender-ref ref= \"CONSOLE\" /> <appender-ref ref= \"FILE\" /> </appender> <root level= \"TRACE\" > <appender-ref ref= \"CONSOLE_AND_FILE\" /> </root> </configuration> You can leverage nesting to keep your filtering logic under control. For example, you may want to have several things happen when you hit an error in your logs. Appenders will always write when they receive an event, unless they are filtered. Using nesting, you can declare the filter once, and have the child appenders \"inherit\" that filter: <!-- Filter is on the appender chain --> <appender name=\"ERROR-APPENDER\" class=\"com.tersesystems.logback.CompositeAppender\"> <filter class=\"ch.qos.logback.classic.filter.LevelFilter\"> <level>ERROR</level> <onMatch>ACCEPT</onMatch> <onMismatch>DENY</onMismatch> </filter> <appender class=\"ch.qos.logback.core.FileAppender\"> <file>error.log</file> <encoder> <pattern>%date - %message</pattern> </encoder> </appender> <appender class=\"com.tersesystems.logback.audio.AudioAppender\"> <player class=\"com.tersesystems.logback.audio.ResourcePlayer\"> <resource>/error.ogg</resource> </player> </appender> </appender> <root level=\"TRACE\"> <appender-ref ref=\"ALL-APPENDER\"/> <appender-ref ref=\"TRACE-APPENDER\"/> <appender-ref ref=\"DEBUG-APPENDER\"/> <appender-ref ref=\"INFO-APPENDER\"/> <appender-ref ref=\"WARN-APPENDER\"/> <appender-ref ref=\"ERROR-APPENDER\"/> </root> This makes your appender logic much cleaner. See Application Logging in Java: Appenders for more details.","title":"Composite Appender"},{"location":"guide/compression/","text":"Compression \u00b6 Encoders are powerful and useful. They give you access to the raw bytes, and let you manipulate them before they get to an appender. But you'll have to put them together inside an appender if you want to do byte transformation. As an example, say that we want to write out files directly in zstandard or brotli using Logback. The easiest way to do this is to provide a FileAppender with a swapped out compression encoder, while presenting a public API that looks just like a regular encoder. Here's the appender as logback.xml sees it: <appender name= \"COMPRESS_FILE\" class= \"com.tersesystems.logback.compress.CompressingFileAppender\" > <file> encoded.zst </file> <compressAlgo> zstd </compressAlgo> <bufferSize> 1024000 </bufferSize> <encoder class= \"ch.qos.logback.classic.encoder.PatternLayoutEncoder\" > <charset> UTF-8 </charset> <pattern> %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> Under the hood, CompressingFileAppender delegates to a regular file appender, but uses commons-compress and a CompressingEncoder to wrap PatternLayoutEncoder : public class CompressingFileAppender < E > extends UnsynchronizedAppenderBase < E > { // ... @Override public void start () { fileAppender = new FileAppender <> (); fileAppender . setContext ( getContext ()); fileAppender . setFile ( getFile ()); fileAppender . setImmediateFlush ( false ); fileAppender . setPrudent ( isPrudent ()); fileAppender . setAppend ( isAppend ()); fileAppender . setName ( name + \"-embedded-file\" ); CompressingEncoder < E > compressedEncoder = createCompressingEncoder ( getEncoder ()); fileAppender . setEncoder ( compressedEncoder ); fileAppender . start (); super . start (); } public void stop () { fileAppender . stop (); super . stop (); } @Override protected void append ( E eventObject ) { fileAppender . doAppend ( eventObject ); } protected CompressingEncoder < E > createCompressingEncoder ( Encoder < E > e ) { int bufferSize = getBufferSize (); String compressAlgo = getCompressAlgo (); CompressorStreamFactory factory = CompressorStreamFactory . getSingleton (); Set < String > names = factory . getOutputStreamCompressorNames (); if ( names . contains ( getCompressAlgo ())) { try { return new CompressingEncoder <> ( e , compressAlgo , factory , bufferSize ); } catch ( CompressorException ex ) { throw new RuntimeException ( \"Cannot create CompressingEncoder\" , ex ); } } else { throw new RuntimeException ( \"No such compression algorithm: \" + compressAlgo ); } } } From there, the encoder will shove all the input bytes into a compressed stream until there's enough data to make compression worthwhile, and then flush the compressed bytes out through a byte array output stream: public class CompressingEncoder < E > extends EncoderBase < E > { private final Accumulator accumulator ; private final Encoder < E > encoder ; public CompressingEncoder ( Encoder < E > encoder , String compressAlgo , CompressorStreamFactory factory , int bufferSize ) throws CompressorException { this . encoder = encoder ; this . accumulator = new Accumulator ( compressAlgo , factory , bufferSize ); } @Override public byte [] headerBytes () { try { return accumulator . apply ( encoder . headerBytes ()); } catch ( IOException e ) { throw new RuntimeException ( e ); } } @Override public byte [] encode ( E event ) { try { return accumulator . apply ( encoder . encode ( event )); } catch ( IOException e ) { throw new RuntimeException ( e ); } } @Override public byte [] footerBytes () { try { return accumulator . drain ( encoder . footerBytes ()); } catch ( IOException e ) { throw new RuntimeException ( e ); } } static class Accumulator { private final ByteArrayOutputStream byteOutputStream ; private final CompressorOutputStream stream ; private final LongAdder count = new LongAdder (); private final int bufferSize ; public Accumulator ( String compressAlgo , CompressorStreamFactory factory , int bufferSize ) throws CompressorException { this . bufferSize = bufferSize ; this . byteOutputStream = new ByteArrayOutputStream (); this . stream = factory . createCompressorOutputStream ( compressAlgo , byteOutputStream ); } boolean isFlushable () { return count . intValue () >= bufferSize ; } byte [] apply ( byte [] bytes ) throws IOException { count . add ( bytes . length ); stream . write ( bytes ); if ( isFlushable ()) { stream . flush (); byte [] output = byteOutputStream . toByteArray (); byteOutputStream . reset (); count . reset (); return output ; } else { return new byte [ 0 ] ; } } byte [] drain ( byte [] inputBytes ) throws IOException { if ( inputBytes != null ) { stream . write ( inputBytes ); } stream . close (); count . reset (); return byteOutputStream . toByteArray (); } } } This keeps both FileAppender and PatternLayoutEncoder happy, while feeding compressed bytes as the stream. Using delegation is generally much easier than trying to extend from FileAppender , because FileAppender has very definite ideas about what kind of output stream it is using, and has all the logic of file rotation and backups encorporated into it, including its own gzip compression scheme for rotated files. You can also extend this to add dictionary support for ZStandard, and that would remove the need for a buffer to provide effective compression. This does come with the downside of needing to pass the dictionary out of band though. See Application Logging in Java: Encoders for more details.","title":"Compression"},{"location":"guide/compression/#compression","text":"Encoders are powerful and useful. They give you access to the raw bytes, and let you manipulate them before they get to an appender. But you'll have to put them together inside an appender if you want to do byte transformation. As an example, say that we want to write out files directly in zstandard or brotli using Logback. The easiest way to do this is to provide a FileAppender with a swapped out compression encoder, while presenting a public API that looks just like a regular encoder. Here's the appender as logback.xml sees it: <appender name= \"COMPRESS_FILE\" class= \"com.tersesystems.logback.compress.CompressingFileAppender\" > <file> encoded.zst </file> <compressAlgo> zstd </compressAlgo> <bufferSize> 1024000 </bufferSize> <encoder class= \"ch.qos.logback.classic.encoder.PatternLayoutEncoder\" > <charset> UTF-8 </charset> <pattern> %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> Under the hood, CompressingFileAppender delegates to a regular file appender, but uses commons-compress and a CompressingEncoder to wrap PatternLayoutEncoder : public class CompressingFileAppender < E > extends UnsynchronizedAppenderBase < E > { // ... @Override public void start () { fileAppender = new FileAppender <> (); fileAppender . setContext ( getContext ()); fileAppender . setFile ( getFile ()); fileAppender . setImmediateFlush ( false ); fileAppender . setPrudent ( isPrudent ()); fileAppender . setAppend ( isAppend ()); fileAppender . setName ( name + \"-embedded-file\" ); CompressingEncoder < E > compressedEncoder = createCompressingEncoder ( getEncoder ()); fileAppender . setEncoder ( compressedEncoder ); fileAppender . start (); super . start (); } public void stop () { fileAppender . stop (); super . stop (); } @Override protected void append ( E eventObject ) { fileAppender . doAppend ( eventObject ); } protected CompressingEncoder < E > createCompressingEncoder ( Encoder < E > e ) { int bufferSize = getBufferSize (); String compressAlgo = getCompressAlgo (); CompressorStreamFactory factory = CompressorStreamFactory . getSingleton (); Set < String > names = factory . getOutputStreamCompressorNames (); if ( names . contains ( getCompressAlgo ())) { try { return new CompressingEncoder <> ( e , compressAlgo , factory , bufferSize ); } catch ( CompressorException ex ) { throw new RuntimeException ( \"Cannot create CompressingEncoder\" , ex ); } } else { throw new RuntimeException ( \"No such compression algorithm: \" + compressAlgo ); } } } From there, the encoder will shove all the input bytes into a compressed stream until there's enough data to make compression worthwhile, and then flush the compressed bytes out through a byte array output stream: public class CompressingEncoder < E > extends EncoderBase < E > { private final Accumulator accumulator ; private final Encoder < E > encoder ; public CompressingEncoder ( Encoder < E > encoder , String compressAlgo , CompressorStreamFactory factory , int bufferSize ) throws CompressorException { this . encoder = encoder ; this . accumulator = new Accumulator ( compressAlgo , factory , bufferSize ); } @Override public byte [] headerBytes () { try { return accumulator . apply ( encoder . headerBytes ()); } catch ( IOException e ) { throw new RuntimeException ( e ); } } @Override public byte [] encode ( E event ) { try { return accumulator . apply ( encoder . encode ( event )); } catch ( IOException e ) { throw new RuntimeException ( e ); } } @Override public byte [] footerBytes () { try { return accumulator . drain ( encoder . footerBytes ()); } catch ( IOException e ) { throw new RuntimeException ( e ); } } static class Accumulator { private final ByteArrayOutputStream byteOutputStream ; private final CompressorOutputStream stream ; private final LongAdder count = new LongAdder (); private final int bufferSize ; public Accumulator ( String compressAlgo , CompressorStreamFactory factory , int bufferSize ) throws CompressorException { this . bufferSize = bufferSize ; this . byteOutputStream = new ByteArrayOutputStream (); this . stream = factory . createCompressorOutputStream ( compressAlgo , byteOutputStream ); } boolean isFlushable () { return count . intValue () >= bufferSize ; } byte [] apply ( byte [] bytes ) throws IOException { count . add ( bytes . length ); stream . write ( bytes ); if ( isFlushable ()) { stream . flush (); byte [] output = byteOutputStream . toByteArray (); byteOutputStream . reset (); count . reset (); return output ; } else { return new byte [ 0 ] ; } } byte [] drain ( byte [] inputBytes ) throws IOException { if ( inputBytes != null ) { stream . write ( inputBytes ); } stream . close (); count . reset (); return byteOutputStream . toByteArray (); } } } This keeps both FileAppender and PatternLayoutEncoder happy, while feeding compressed bytes as the stream. Using delegation is generally much easier than trying to extend from FileAppender , because FileAppender has very definite ideas about what kind of output stream it is using, and has all the logic of file rotation and backups encorporated into it, including its own gzip compression scheme for rotated files. You can also extend this to add dictionary support for ZStandard, and that would remove the need for a buffer to provide effective compression. This does come with the downside of needing to pass the dictionary out of band though. See Application Logging in Java: Encoders for more details.","title":"Compression"},{"location":"guide/correlationid/","text":"Correlation ID \u00b6 The logback-correlationid module is a set of classes designed to encompass the idea of a correlation id in events. It consists of a correlation id filter, a tap filter that always logs events with a correlation id to an appender, a JDBC appender that writes correlation id to a column in a database schema, and a correlation id marker. Correlation ID Filter \u00b6 A correlation id filter will filter for a correlation id set either as an MDC value, or as a marker created from CorrelationIdMarker . <appender name= \"LIST\" class= \"ch.qos.logback.core.read.ListAppender\" > <filter class= \"com.tersesystems.logback.correlationid.CorrelationIdFilter\" > <mdcKey> correlationId </mdcKey> </filter> </appender> If an appender passes the filter, it will log the event. public class CorrelationIdFilterTest { public void testFilter () { // Write something that never gets logged explicitly... Logger logger = loggerFactory . getLogger ( \"com.example.Debug\" ); String correlationId = \"12345\" ; CorrelationIdMarker correlationIdMarker = CorrelationIdMarker . create ( correlationId ); // should be logged because marker logger . info ( correlationIdMarker , \"info one\" ); logger . info ( \"info two\" ); // should not be logged // Everything below this point should be logged. MDC . put ( \"correlationId\" , correlationId ); logger . info ( \"info three\" ); // should not be logged logger . info ( correlationIdMarker , \"info four\" ); } } CorrelationIdTapFilter \u00b6 The CorrelationIdTapFilter is a turbofilter that always logs to a given appender if the correlation id appears, even if the appender is not configured for logging. <configuration> <newRule pattern= \"configuration/turboFilter/appender-ref\" actionClass= \"ch.qos.logback.core.joran.action.AppenderRefAction\" /> <appender name= \"TAP_LIST\" class= \"ch.qos.logback.core.read.ListAppender\" > </appender> <turboFilter class= \"com.tersesystems.logback.correlationid.CorrelationIdTapFilter\" > <mdcKey> correlationId </mdcKey> <appender-ref ref= \"TAP_LIST\" /> </turboFilter> <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <root level= \"INFO\" > <appender-ref ref= \"CONSOLE\" /> </root> </configuration> CorrelationIdMarker \u00b6 A CorrelationIdMarker implements the CorrelationIdProvider interface to expose a marker which is known to contain a correlation id. CorrelationIdMarker correlationIdMarker = CorrelationIdMarker . create ( correlationId ); String sameId = correlationIdMarker . getCorrelationId (); CorrelationIdJDBCAppender \u00b6 A CorrelationIdJDBCAppender is a JDBC appender that can write out a correlation id to a row, extending the normal JDBC correlator. <configuration> <appender name= \"ASYNC_JDBC\" class= \"net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender\" > <appender class= \"com.tersesystems.logback.correlationid.CorrelationIdJDBCAppender\" > <mdcKey> correlationId </mdcKey> <!-- <driver>com.p6spy.engine.spy.P6SpyDriver</driver>--> <!-- <url>jdbc:p6spy:h2:mem:terse-logback;DB_CLOSE_DELAY=-1</url>--> <driver> org.h2.Driver </driver> <url> jdbc:h2:mem:terse-logback;DB_CLOSE_DELAY=-1 </url> <username> sa </username> <password></password> <createStatements> CREATE TABLE IF NOT EXISTS events ( ID NUMERIC NOT NULL PRIMARY KEY AUTO_INCREMENT, ts TIMESTAMP(9) WITH TIME ZONE NOT NULL, relative_ns BIGINT NULL, start_ms BIGINT NULL, level_value int NOT NULL, level VARCHAR(7) NOT NULL, evt JSON NOT NULL, correlation_id VARCHAR(255) NOT NULL, event_id VARCHAR(255) NULL ); CREATE INDEX IF NOT EXISTS event_id_idx ON events(event_id); CREATE INDEX IF NOT EXISTS correlation_id_idx ON events(correlation_id); </createStatements> <insertStatement> insert into events(ts, relative_ns, start_ms, level_value, level, evt, correlation_id, event_id) values(?, ?, ?, ?, ?, ?, ?, ?) </insertStatement> <encoder class= \"net.logstash.logback.encoder.LogstashEncoder\" > </encoder> </appender> </appender> </configuration> CorrelationIdUtils \u00b6 CorrelationIdUtils contains utility methods like get which retrieve a correlation id from either a marker or MDC.","title":"Correlation Id"},{"location":"guide/correlationid/#correlation-id","text":"The logback-correlationid module is a set of classes designed to encompass the idea of a correlation id in events. It consists of a correlation id filter, a tap filter that always logs events with a correlation id to an appender, a JDBC appender that writes correlation id to a column in a database schema, and a correlation id marker.","title":"Correlation ID"},{"location":"guide/correlationid/#correlation-id-filter","text":"A correlation id filter will filter for a correlation id set either as an MDC value, or as a marker created from CorrelationIdMarker . <appender name= \"LIST\" class= \"ch.qos.logback.core.read.ListAppender\" > <filter class= \"com.tersesystems.logback.correlationid.CorrelationIdFilter\" > <mdcKey> correlationId </mdcKey> </filter> </appender> If an appender passes the filter, it will log the event. public class CorrelationIdFilterTest { public void testFilter () { // Write something that never gets logged explicitly... Logger logger = loggerFactory . getLogger ( \"com.example.Debug\" ); String correlationId = \"12345\" ; CorrelationIdMarker correlationIdMarker = CorrelationIdMarker . create ( correlationId ); // should be logged because marker logger . info ( correlationIdMarker , \"info one\" ); logger . info ( \"info two\" ); // should not be logged // Everything below this point should be logged. MDC . put ( \"correlationId\" , correlationId ); logger . info ( \"info three\" ); // should not be logged logger . info ( correlationIdMarker , \"info four\" ); } }","title":"Correlation ID Filter"},{"location":"guide/correlationid/#correlationidtapfilter","text":"The CorrelationIdTapFilter is a turbofilter that always logs to a given appender if the correlation id appears, even if the appender is not configured for logging. <configuration> <newRule pattern= \"configuration/turboFilter/appender-ref\" actionClass= \"ch.qos.logback.core.joran.action.AppenderRefAction\" /> <appender name= \"TAP_LIST\" class= \"ch.qos.logback.core.read.ListAppender\" > </appender> <turboFilter class= \"com.tersesystems.logback.correlationid.CorrelationIdTapFilter\" > <mdcKey> correlationId </mdcKey> <appender-ref ref= \"TAP_LIST\" /> </turboFilter> <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <root level= \"INFO\" > <appender-ref ref= \"CONSOLE\" /> </root> </configuration>","title":"CorrelationIdTapFilter"},{"location":"guide/correlationid/#correlationidmarker","text":"A CorrelationIdMarker implements the CorrelationIdProvider interface to expose a marker which is known to contain a correlation id. CorrelationIdMarker correlationIdMarker = CorrelationIdMarker . create ( correlationId ); String sameId = correlationIdMarker . getCorrelationId ();","title":"CorrelationIdMarker"},{"location":"guide/correlationid/#correlationidjdbcappender","text":"A CorrelationIdJDBCAppender is a JDBC appender that can write out a correlation id to a row, extending the normal JDBC correlator. <configuration> <appender name= \"ASYNC_JDBC\" class= \"net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender\" > <appender class= \"com.tersesystems.logback.correlationid.CorrelationIdJDBCAppender\" > <mdcKey> correlationId </mdcKey> <!-- <driver>com.p6spy.engine.spy.P6SpyDriver</driver>--> <!-- <url>jdbc:p6spy:h2:mem:terse-logback;DB_CLOSE_DELAY=-1</url>--> <driver> org.h2.Driver </driver> <url> jdbc:h2:mem:terse-logback;DB_CLOSE_DELAY=-1 </url> <username> sa </username> <password></password> <createStatements> CREATE TABLE IF NOT EXISTS events ( ID NUMERIC NOT NULL PRIMARY KEY AUTO_INCREMENT, ts TIMESTAMP(9) WITH TIME ZONE NOT NULL, relative_ns BIGINT NULL, start_ms BIGINT NULL, level_value int NOT NULL, level VARCHAR(7) NOT NULL, evt JSON NOT NULL, correlation_id VARCHAR(255) NOT NULL, event_id VARCHAR(255) NULL ); CREATE INDEX IF NOT EXISTS event_id_idx ON events(event_id); CREATE INDEX IF NOT EXISTS correlation_id_idx ON events(correlation_id); </createStatements> <insertStatement> insert into events(ts, relative_ns, start_ms, level_value, level, evt, correlation_id, event_id) values(?, ?, ?, ?, ?, ?, ?, ?) </insertStatement> <encoder class= \"net.logstash.logback.encoder.LogstashEncoder\" > </encoder> </appender> </appender> </configuration>","title":"CorrelationIdJDBCAppender"},{"location":"guide/correlationid/#correlationidutils","text":"CorrelationIdUtils contains utility methods like get which retrieve a correlation id from either a marker or MDC.","title":"CorrelationIdUtils"},{"location":"guide/exception-mapping/","text":"Exception Mapping \u00b6 Exception Mapping is done to show the important details of an exception, including the root cause in a summary format. This is especially useful in line oriented formats, because rendering a stacktrace can take up screen real estate without providing much value. An example will help. Given the following program: public class Thrower { private static final Logger logger = LoggerFactory . getLogger ( Thrower . class ); public static void main ( String [] progArgs ) { try { doSomethingExceptional (); } catch ( RuntimeException e ) { logger . error ( \"domain specific message\" , e ); } } static void doSomethingExceptional () { Throwable cause = new BatchUpdateException (); throw new MyCustomException ( \"This is my message\" , \"one is one\" , \"two is more than one\" , \"three is more than two and one\" , cause ); } } public class MyCustomException extends RuntimeException { public MyCustomException ( String message , String one , String two , String three , Throwable cause ) { // ... } public String getOne () { return one ; } public String getTwo () { return two ; } public String getThree () { return three ; } } and the Logback file: <configuration> <newRule pattern= \"*/exceptionMappings\" actionClass= \"com.tersesystems.logback.exceptionmapping.ExceptionMappingRegistryAction\" /> <newRule pattern= \"*/exceptionMappings/mapping\" actionClass= \"com.tersesystems.logback.exceptionmapping.ExceptionMappingAction\" /> <conversionRule conversionWord= \"richex\" converterClass= \"com.tersesystems.logback.exceptionmapping.ExceptionMessageWithMappingsConverter\" /> <exceptionMappings> <!-- comes with default mappings for JDK exceptions, but you can add your own --> <mapping name= \"com.tersesystems.logback.exceptionmapping.MyCustomException\" properties= \"one,two,three\" /> </exceptionMappings> <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%richex{1, 10, exception=[}%n </pattern> </encoder> </appender> <root level= \"TRACE\" > <appender-ref ref= \"CONSOLE\" /> </root> </configuration> Then this renders the following: 184 ERROR c.t.l.exceptionmapping.Thrower - domain specific message exception=[com.tersesystems.logback.exceptionmapping.MyCustomException(one=\"one is one\" two=\"two is more than one\" three=\"three is more than two and one\" message=\"This is my message\") > java.sql.BatchUpdateException(updateCounts=\"null\" errorCode=\"0\" SQLState=\"null\" message=\"null\")] You can integrate exception mapping with Typesafe Config and logstash-logback-encoder by adding extra mappings. For example, you can map a whole bunch of exceptions at once in HOCON, and not have to do it line by line in XML: <configuration> <newRule pattern= \"*/exceptionMappings/configMappings\" actionClass= \"com.tersesystems.logback.exceptionmapping.config.TypesafeConfigMappingsAction\" /> <exceptionMappings> <!-- Or point to HOCON path --> <configMappings path= \"exceptionmappings\" /> </exceptionMappings> </configuration> and exceptionmappings { example.MySpecialException: [\"timestamp\"] } and configure it in JSON using ExceptionArgumentsProvider : <encoder class= \"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\" > <providers> <provider class= \"com.tersesystems.logback.exceptionmapping.json.ExceptionArgumentsProvider\" > <fieldName> exception </fieldName> </provider> </providers> </encoder> and get the following exception that contains an array of exceptions and the associated properties, in this case timestamp : { \"id\" : \"Fa6x8H0EqomdHaINzdiAAA\" , \"sequence\" : 3 , \"@timestamp\" : \"2019-07-06T03:52:48.730+00:00\" , \"@version\" : \"1\" , \"message\" : \"I am an error\" , \"logger_name\" : \"example.Main$Runner\" , \"thread_name\" : \"pool-1-thread-1\" , \"level\" : \"ERROR\" , \"stack_hash\" : \"233f3cf1\" , \"exception\" : [ { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 1\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 2\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 3\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 4\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 5\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 6\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 7\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 8\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 9\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } } ], \"stack_trace\" : \"<#1165e3b1> example.MySpecialException: Level 9\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 9 common frames omitted\\nWrapped by: <#eb336a2d> example.MySpecialException: Level 8\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 10 common frames omitted\\nWrapped by: <#cc1fb404> example.MySpecialException: Level 7\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 11 common frames omitted\\nWrapped by: <#2af187a0> example.MySpecialException: Level 6\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 12 common frames omitted\\nWrapped by: <#7dac62d1> example.MySpecialException: Level 5\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 13 common frames omitted\\nWrapped by: <#2ea4460d> example.MySpecialException: Level 4\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 14 common frames omitted\\nWrapped by: <#261bed64> example.MySpecialException: Level 3\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 15 common frames omitted\\nWrapped by: <#e660d440> example.MySpecialException: Level 2\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 16 common frames omitted\\nWrapped by: <#233f3cf1> example.MySpecialException: Level 1\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.generateException(Main.java:51)\\n\\tat example.Main$Runner.doError(Main.java:44)\\n\\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\\n\\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\\n\\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\\n\\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\" } This is a lot easier for structured logging parsers to grok than the associated stacktrace. See How to Log an Exception and How to Log an Exception, Part 2 for more details.","title":"Exception Mapping"},{"location":"guide/exception-mapping/#exception-mapping","text":"Exception Mapping is done to show the important details of an exception, including the root cause in a summary format. This is especially useful in line oriented formats, because rendering a stacktrace can take up screen real estate without providing much value. An example will help. Given the following program: public class Thrower { private static final Logger logger = LoggerFactory . getLogger ( Thrower . class ); public static void main ( String [] progArgs ) { try { doSomethingExceptional (); } catch ( RuntimeException e ) { logger . error ( \"domain specific message\" , e ); } } static void doSomethingExceptional () { Throwable cause = new BatchUpdateException (); throw new MyCustomException ( \"This is my message\" , \"one is one\" , \"two is more than one\" , \"three is more than two and one\" , cause ); } } public class MyCustomException extends RuntimeException { public MyCustomException ( String message , String one , String two , String three , Throwable cause ) { // ... } public String getOne () { return one ; } public String getTwo () { return two ; } public String getThree () { return three ; } } and the Logback file: <configuration> <newRule pattern= \"*/exceptionMappings\" actionClass= \"com.tersesystems.logback.exceptionmapping.ExceptionMappingRegistryAction\" /> <newRule pattern= \"*/exceptionMappings/mapping\" actionClass= \"com.tersesystems.logback.exceptionmapping.ExceptionMappingAction\" /> <conversionRule conversionWord= \"richex\" converterClass= \"com.tersesystems.logback.exceptionmapping.ExceptionMessageWithMappingsConverter\" /> <exceptionMappings> <!-- comes with default mappings for JDK exceptions, but you can add your own --> <mapping name= \"com.tersesystems.logback.exceptionmapping.MyCustomException\" properties= \"one,two,three\" /> </exceptionMappings> <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%richex{1, 10, exception=[}%n </pattern> </encoder> </appender> <root level= \"TRACE\" > <appender-ref ref= \"CONSOLE\" /> </root> </configuration> Then this renders the following: 184 ERROR c.t.l.exceptionmapping.Thrower - domain specific message exception=[com.tersesystems.logback.exceptionmapping.MyCustomException(one=\"one is one\" two=\"two is more than one\" three=\"three is more than two and one\" message=\"This is my message\") > java.sql.BatchUpdateException(updateCounts=\"null\" errorCode=\"0\" SQLState=\"null\" message=\"null\")] You can integrate exception mapping with Typesafe Config and logstash-logback-encoder by adding extra mappings. For example, you can map a whole bunch of exceptions at once in HOCON, and not have to do it line by line in XML: <configuration> <newRule pattern= \"*/exceptionMappings/configMappings\" actionClass= \"com.tersesystems.logback.exceptionmapping.config.TypesafeConfigMappingsAction\" /> <exceptionMappings> <!-- Or point to HOCON path --> <configMappings path= \"exceptionmappings\" /> </exceptionMappings> </configuration> and exceptionmappings { example.MySpecialException: [\"timestamp\"] } and configure it in JSON using ExceptionArgumentsProvider : <encoder class= \"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\" > <providers> <provider class= \"com.tersesystems.logback.exceptionmapping.json.ExceptionArgumentsProvider\" > <fieldName> exception </fieldName> </provider> </providers> </encoder> and get the following exception that contains an array of exceptions and the associated properties, in this case timestamp : { \"id\" : \"Fa6x8H0EqomdHaINzdiAAA\" , \"sequence\" : 3 , \"@timestamp\" : \"2019-07-06T03:52:48.730+00:00\" , \"@version\" : \"1\" , \"message\" : \"I am an error\" , \"logger_name\" : \"example.Main$Runner\" , \"thread_name\" : \"pool-1-thread-1\" , \"level\" : \"ERROR\" , \"stack_hash\" : \"233f3cf1\" , \"exception\" : [ { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 1\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 2\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 3\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 4\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 5\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 6\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 7\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 8\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 9\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } } ], \"stack_trace\" : \"<#1165e3b1> example.MySpecialException: Level 9\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 9 common frames omitted\\nWrapped by: <#eb336a2d> example.MySpecialException: Level 8\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 10 common frames omitted\\nWrapped by: <#cc1fb404> example.MySpecialException: Level 7\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 11 common frames omitted\\nWrapped by: <#2af187a0> example.MySpecialException: Level 6\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 12 common frames omitted\\nWrapped by: <#7dac62d1> example.MySpecialException: Level 5\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 13 common frames omitted\\nWrapped by: <#2ea4460d> example.MySpecialException: Level 4\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 14 common frames omitted\\nWrapped by: <#261bed64> example.MySpecialException: Level 3\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 15 common frames omitted\\nWrapped by: <#e660d440> example.MySpecialException: Level 2\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 16 common frames omitted\\nWrapped by: <#233f3cf1> example.MySpecialException: Level 1\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.generateException(Main.java:51)\\n\\tat example.Main$Runner.doError(Main.java:44)\\n\\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\\n\\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\\n\\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\\n\\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\" } This is a lot easier for structured logging parsers to grok than the associated stacktrace. See How to Log an Exception and How to Log an Exception, Part 2 for more details.","title":"Exception Mapping"},{"location":"guide/instrumentation/","text":"Instrumentation \u00b6 If you have library code that doesn't pass around ILoggerFactory and doesn't let you add information to logging, then you can get around this by instrumenting the code with Byte Buddy . Using Byte Buddy, you can do fun things like override Security.setSystemManager with your own implementation , so using Byte Buddy to decorate code with enter and exit logging statements is relatively straightforward. Instrumentation is configuration driven and simple. Instead of debugging using printf statements and recompiling or stepping through a debugger, you can just add lines to a config file. I like this approach better than the annotation or aspect-oriented programming approaches, because it is completely transparent to the code and gives roughly the same performance as inline code, adding 130 ns/op by calling class.getMethod . A major advantage of instrumentation is that because it logs throwing exceptions in instrumented code, you can log exceptions that would be swallowed by the caller. For example, imagine that a library has the following method: public class Foo { public void throwException () throws Exception { throw new PlumException ( \"I am sweet and cold\" ); } public void swallowException () { try { throwException (); } catch ( Exception e ) { // forgive me, the exception was delicious } } } By instrumenting the throwException method, you can see the logged exception at runtime when swallowException is called. See Application Logging in Java: Tracing 3 rd Party Code and Hierarchical Instrumented Tracing with Logback for more details. Installation \u00b6 You'll need to install logback-bytebuddy and logback-tracing , and provide a byte-buddy implementation. implementation group: 'com.tersesystems.logback', name: 'logback-classic', version: '0.16.2' implementation group: 'com.tersesystems.logback', name: 'logback-bytebuddy', version: '0.16.2' implementation group: 'com.tersesystems.logback', name: 'logback-tracing', version: '0.16.2' implementation group: 'net.bytebuddy', name: 'byte-buddy', version: '1.11.0' There are two ways you can install instrumentation -- you can do it using an agent, or you can do it manually. NOTE: Because Byte Buddy must inspect each class on JVM initialization, it will have a (generally small) impact on the start up time of your application. Agent Installation \u00b6 Using the agent is generally easier (less code) and more powerful (can change JDK classes), but it does require some explicit command line options. First, you set the java agent, either directly on the command line: java \\ -javaagent:path/to/logback-bytebuddy-x.x.x.jar = debug \\ -Dterse.logback.configurationFile = conf/logback.conf \\ -Dlogback.configurationFile = conf/logback-test.xml \\ com.example.PreloadedInstrumentationExample or by using the JAVA_TOOLS_OPTIONS environment variable . export JAVA_TOOLS_OPTIONS = \"...\" Generally you'll be setting up these options in a build system. There are example projects in Gradle and sbt set up with agent-based instrumentation at https://github.com/tersesystems/logging-instrumentation-example . Manual Installation \u00b6 You also have the option of installing the agent manually. The in process instrumentation is done with com.tersesystems.logback.bytebuddy.LoggingInstrumentationByteBuddyBuilder , which takes in some configuration and then installs itself on the byte buddy agent. new LoggingInstrumentationByteBuddyBuilder () . builderFromConfig ( loggingInstrumentationAdviceConfig ) . with ( debugListener ) . installOnByteBuddyAgent (); Configuration \u00b6 There are two parts to seeing tracing logs with instrumentation -- indicating the classes and methods you want instrumented, and then setting those loggers to TRACE. Setting Instrumented Classes and Methods \u00b6 The instrumentation is configured using HOCON in a logback.conf file in src/main/resources . Settings are under the logback.bytebuddy section. The tracing section contains a mapping of class names and methods, or the wildcard \"*\" to indicate all methods. logback.bytebuddy { service-name = \"my-service\" tracing { \"fully.qualified.class.Name\" = [\"method1\", \"method2\"] \"play.api.mvc.ActionBuilder\" = [\"*\"] } } NOTE: There are some limitations to what you can trace. You can only instrument JDK classes when using the agent, and you cannot instrument native methods like java.lang.System.currentTimeMillis() for example. Setting Loggers to TRACE \u00b6 Because instrumentation inserts logger.trace calls into the code, you must enable logging at TRACE level for those loggers to see output. Setting the level from logback.xml works fine: <configuration> <!-- ... --> <logger name= \"fully.qualified.class.Name\" level= \"TRACE\" /> <logger name= \"play.api.mvc.ActionBuilder\" level= \"TRACE\" /> <!-- ... --> </configuration> If you are using the Config module, you can also do this from logback.conf : levels { fully.qualified.class.Name = TRACE play.api.mvc.ActionBuilder = TRACE } Or you can use ChangeLogLevel at run time. Examples \u00b6 Instrumentation is a tool that can be hard to explain, so here's some use cases showing how you can quickly instrument your code. Also don't forget the example projects at https://github.com/tersesystems/logging-instrumentation-example . Instrumenting java.lang.Thread \u00b6 Assuming an agent based instrumentation, in logback.conf : levels { java.lang.Thread = TRACE } logback.bytebuddy { service-name = \"some-service\" tracing { \"java.lang.Thread\" = [ \"run\" ] } } and the code as follows: public class PreloadedInstrumentationExample { public static void main ( String [] args ) throws Exception { Thread thread = Thread . currentThread (); thread . run (); } } yields [Byte Buddy] DISCOVERY java.lang.Thread [null, null, loaded=true] [Byte Buddy] TRANSFORM java.lang.Thread [null, null, loaded=true] [Byte Buddy] COMPLETE java.lang.Thread [null, null, loaded=true] 92 TRACE java.lang.Thread - entering: java.lang.Thread.run() with arguments=[] 93 TRACE java.lang.Thread - exiting: java.lang.Thread.run() with arguments=[] => returnType=void Instrumenting javax.net.ssl.SSLContext \u00b6 This is especially helpful when you're trying to debug SSL issues: levels { sun.security.ssl = TRACE javax.net.ssl = TRACE } logback.bytebuddy { service-name = \"some-service\" tracing { \"javax.net.ssl.SSLContext\" = [\"*\"] } } will result in: FcJ3XfsdKnM6O0Qbm7EAAA 12:31:55.498 [TRACE] j.n.s.SSLContext - entering: javax.net.ssl.SSLContext.getInstance(java.lang.String) with arguments=[TLS] from source SSLContext.java:155 FcJ3XfsdKng6O0Qbm7EAAA 12:31:55.503 [TRACE] j.n.s.SSLContext - exiting: javax.net.ssl.SSLContext.getInstance(java.lang.String) with arguments=[TLS] => returnType=javax.net.ssl.SSLContext from source SSLContext.java:157 FcJ3XfsdKng6O0Qbm7EAAB 12:31:55.504 [TRACE] j.n.s.SSLContext - entering: javax.net.ssl.SSLContext.init([Ljavax.net.ssl.KeyManager;,[Ljavax.net.ssl.TrustManager;,java.security.SecureRandom) with arguments=[[org.postgresql.ssl.LazyKeyManager@27a97e08], [org.postgresql.ssl.NonValidatingFactory$NonValidatingTM@5918c260], null] from source SSLContext.java:282 FcJ3XfsdKnk6O0Qbm7EAAA 12:31:55.504 [TRACE] j.n.s.SSLContext - exiting: javax.net.ssl.SSLContext.init([Ljavax.net.ssl.KeyManager;,[Ljavax.net.ssl.TrustManager;,java.security.SecureRandom) with arguments=[[org.postgresql.ssl.LazyKeyManager@27a97e08], [org.postgresql.ssl.NonValidatingFactory$NonValidatingTM@5918c260], null] => returnType=void from source SSLContext.java:283 Be warned that JSSE can be extremely verbose in its toString output. Instrumenting ClassCalledByAgent \u00b6 If you are already developing an agent, or want finer grained control over Byte Buddy, you can create the agent in process and inspect how Byte Buddy works. This is an advanced use case, but it's useful to get familiar. With the following code: public class ClassCalledByAgent { public void printStatement () { System . out . println ( \"I am a simple println method with no logging\" ); } public void printArgument ( String arg ) { System . out . println ( \"I am a simple println, printing \" + arg ); } public void throwException ( String arg ) { throw new RuntimeException ( \"I'm a squirrel!\" ); } } And the following configuration in logback.conf : logback.bytebuddy { service-name = \"example-service\" tracing { \"com.tersesystems.logback.bytebuddy.ClassCalledByAgent\" = [ \"printStatement\", \"printArgument\", \"throwException\", ] } } and have com.tersesystems.logback.bytebuddy.ClassCalledByAgent logging level set to TRACE in logback.xml . We can start up the agent, add in the builder and run through the methods: public class InProcessInstrumentationExample { public static AgentBuilder . Listener createDebugListener ( List < String > classNames ) { return new AgentBuilder . Listener . Filtering ( LoggingInstrumentationAdvice . stringMatcher ( classNames ), AgentBuilder . Listener . StreamWriting . toSystemOut ()); } public static void main ( String [] args ) throws Exception { // Helps if you install the byte buddy agents before anything else at all happens... ByteBuddyAgent . install (); Logger logger = LoggerFactory . getLogger ( InProcessInstrumentationExample . class ); SystemFlow . setLoggerResolver ( new FixedLoggerResolver ( logger )); Config config = LoggingInstrumentationAdvice . generateConfig ( ClassLoader . getSystemClassLoader (), false ); LoggingInstrumentationAdviceConfig adviceConfig = LoggingInstrumentationAdvice . generateAdviceConfig ( config ); // The debugging listener shows what classes are being picked up by the instrumentation Listener debugListener = createDebugListener ( adviceConfig . classNames ()); new LoggingInstrumentationByteBuddyBuilder () . builderFromConfig ( adviceConfig ) . with ( debugListener ) . installOnByteBuddyAgent (); // No code change necessary here, you can wrap completely in the agent... ClassCalledByAgent classCalledByAgent = new ClassCalledByAgent (); classCalledByAgent . printStatement (); classCalledByAgent . printArgument ( \"42\" ); try { classCalledByAgent . throwException ( \"hello world\" ); } catch ( Exception e ) { // I am too lazy to catch this exception. I hope someone does it for me. } } } And get the following: [Byte Buddy] DISCOVERY com.tersesystems.logback.bytebuddy.ClassCalledByAgent [sun.misc.Launcher$AppClassLoader@75b84c92, null, loaded=true] [Byte Buddy] TRANSFORM com.tersesystems.logback.bytebuddy.ClassCalledByAgent [sun.misc.Launcher$AppClassLoader@75b84c92, null, loaded=true] [Byte Buddy] COMPLETE com.tersesystems.logback.bytebuddy.ClassCalledByAgent [sun.misc.Launcher$AppClassLoader@75b84c92, null, loaded=true] 524 TRACE c.t.l.b.InProcessInstrumentationExample - entering: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printStatement() with arguments=[] from source ClassCalledByAgent.java:18 I am a simple println method with no logging 529 TRACE c.t.l.b.InProcessInstrumentationExample - exiting: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printStatement() with arguments=[] => returnType=void from source ClassCalledByAgent.java:19 529 TRACE c.t.l.b.InProcessInstrumentationExample - entering: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printArgument(java.lang.String) with arguments=[42] from source ClassCalledByAgent.java:22 I am a simple println, printing 42 529 TRACE c.t.l.b.InProcessInstrumentationExample - exiting: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printArgument(java.lang.String) with arguments=[42] => returnType=void from source ClassCalledByAgent.java:23 529 TRACE c.t.l.b.InProcessInstrumentationExample - entering: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.throwException(java.lang.String) with arguments=[hello world] from source ClassCalledByAgent.java:26 532 ERROR c.t.l.b.InProcessInstrumentationExample - throwing: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.throwException(java.lang.String) with arguments=[hello world] ! thrown=java.lang.RuntimeException: I'm a squirrel! java.lang.RuntimeException: I'm a squirrel! at com.tersesystems.logback.bytebuddy.ClassCalledByAgent.throwException(ClassCalledByAgent.java:26) at com.tersesystems.logback.bytebuddy.InProcessInstrumentationExample.main(InProcessInstrumentationExample.java:65) The [Byte Buddy] statements up top are caused by the debug listener, and let you know that Byte Buddy has successfully instrumented the class. Note also that there is no runtime overhead in pulling line numbers or source files into the enter/exit methods, as these are pulled directly from bytecode and do not involve fillInStackTrace .","title":"Instrumentation"},{"location":"guide/instrumentation/#instrumentation","text":"If you have library code that doesn't pass around ILoggerFactory and doesn't let you add information to logging, then you can get around this by instrumenting the code with Byte Buddy . Using Byte Buddy, you can do fun things like override Security.setSystemManager with your own implementation , so using Byte Buddy to decorate code with enter and exit logging statements is relatively straightforward. Instrumentation is configuration driven and simple. Instead of debugging using printf statements and recompiling or stepping through a debugger, you can just add lines to a config file. I like this approach better than the annotation or aspect-oriented programming approaches, because it is completely transparent to the code and gives roughly the same performance as inline code, adding 130 ns/op by calling class.getMethod . A major advantage of instrumentation is that because it logs throwing exceptions in instrumented code, you can log exceptions that would be swallowed by the caller. For example, imagine that a library has the following method: public class Foo { public void throwException () throws Exception { throw new PlumException ( \"I am sweet and cold\" ); } public void swallowException () { try { throwException (); } catch ( Exception e ) { // forgive me, the exception was delicious } } } By instrumenting the throwException method, you can see the logged exception at runtime when swallowException is called. See Application Logging in Java: Tracing 3 rd Party Code and Hierarchical Instrumented Tracing with Logback for more details.","title":"Instrumentation"},{"location":"guide/instrumentation/#installation","text":"You'll need to install logback-bytebuddy and logback-tracing , and provide a byte-buddy implementation. implementation group: 'com.tersesystems.logback', name: 'logback-classic', version: '0.16.2' implementation group: 'com.tersesystems.logback', name: 'logback-bytebuddy', version: '0.16.2' implementation group: 'com.tersesystems.logback', name: 'logback-tracing', version: '0.16.2' implementation group: 'net.bytebuddy', name: 'byte-buddy', version: '1.11.0' There are two ways you can install instrumentation -- you can do it using an agent, or you can do it manually. NOTE: Because Byte Buddy must inspect each class on JVM initialization, it will have a (generally small) impact on the start up time of your application.","title":"Installation"},{"location":"guide/instrumentation/#agent-installation","text":"Using the agent is generally easier (less code) and more powerful (can change JDK classes), but it does require some explicit command line options. First, you set the java agent, either directly on the command line: java \\ -javaagent:path/to/logback-bytebuddy-x.x.x.jar = debug \\ -Dterse.logback.configurationFile = conf/logback.conf \\ -Dlogback.configurationFile = conf/logback-test.xml \\ com.example.PreloadedInstrumentationExample or by using the JAVA_TOOLS_OPTIONS environment variable . export JAVA_TOOLS_OPTIONS = \"...\" Generally you'll be setting up these options in a build system. There are example projects in Gradle and sbt set up with agent-based instrumentation at https://github.com/tersesystems/logging-instrumentation-example .","title":"Agent Installation"},{"location":"guide/instrumentation/#manual-installation","text":"You also have the option of installing the agent manually. The in process instrumentation is done with com.tersesystems.logback.bytebuddy.LoggingInstrumentationByteBuddyBuilder , which takes in some configuration and then installs itself on the byte buddy agent. new LoggingInstrumentationByteBuddyBuilder () . builderFromConfig ( loggingInstrumentationAdviceConfig ) . with ( debugListener ) . installOnByteBuddyAgent ();","title":"Manual Installation"},{"location":"guide/instrumentation/#configuration","text":"There are two parts to seeing tracing logs with instrumentation -- indicating the classes and methods you want instrumented, and then setting those loggers to TRACE.","title":"Configuration"},{"location":"guide/instrumentation/#setting-instrumented-classes-and-methods","text":"The instrumentation is configured using HOCON in a logback.conf file in src/main/resources . Settings are under the logback.bytebuddy section. The tracing section contains a mapping of class names and methods, or the wildcard \"*\" to indicate all methods. logback.bytebuddy { service-name = \"my-service\" tracing { \"fully.qualified.class.Name\" = [\"method1\", \"method2\"] \"play.api.mvc.ActionBuilder\" = [\"*\"] } } NOTE: There are some limitations to what you can trace. You can only instrument JDK classes when using the agent, and you cannot instrument native methods like java.lang.System.currentTimeMillis() for example.","title":"Setting Instrumented Classes and Methods"},{"location":"guide/instrumentation/#setting-loggers-to-trace","text":"Because instrumentation inserts logger.trace calls into the code, you must enable logging at TRACE level for those loggers to see output. Setting the level from logback.xml works fine: <configuration> <!-- ... --> <logger name= \"fully.qualified.class.Name\" level= \"TRACE\" /> <logger name= \"play.api.mvc.ActionBuilder\" level= \"TRACE\" /> <!-- ... --> </configuration> If you are using the Config module, you can also do this from logback.conf : levels { fully.qualified.class.Name = TRACE play.api.mvc.ActionBuilder = TRACE } Or you can use ChangeLogLevel at run time.","title":"Setting Loggers to TRACE"},{"location":"guide/instrumentation/#examples","text":"Instrumentation is a tool that can be hard to explain, so here's some use cases showing how you can quickly instrument your code. Also don't forget the example projects at https://github.com/tersesystems/logging-instrumentation-example .","title":"Examples"},{"location":"guide/instrumentation/#instrumenting-javalangthread","text":"Assuming an agent based instrumentation, in logback.conf : levels { java.lang.Thread = TRACE } logback.bytebuddy { service-name = \"some-service\" tracing { \"java.lang.Thread\" = [ \"run\" ] } } and the code as follows: public class PreloadedInstrumentationExample { public static void main ( String [] args ) throws Exception { Thread thread = Thread . currentThread (); thread . run (); } } yields [Byte Buddy] DISCOVERY java.lang.Thread [null, null, loaded=true] [Byte Buddy] TRANSFORM java.lang.Thread [null, null, loaded=true] [Byte Buddy] COMPLETE java.lang.Thread [null, null, loaded=true] 92 TRACE java.lang.Thread - entering: java.lang.Thread.run() with arguments=[] 93 TRACE java.lang.Thread - exiting: java.lang.Thread.run() with arguments=[] => returnType=void","title":"Instrumenting java.lang.Thread"},{"location":"guide/instrumentation/#instrumenting-javaxnetsslsslcontext","text":"This is especially helpful when you're trying to debug SSL issues: levels { sun.security.ssl = TRACE javax.net.ssl = TRACE } logback.bytebuddy { service-name = \"some-service\" tracing { \"javax.net.ssl.SSLContext\" = [\"*\"] } } will result in: FcJ3XfsdKnM6O0Qbm7EAAA 12:31:55.498 [TRACE] j.n.s.SSLContext - entering: javax.net.ssl.SSLContext.getInstance(java.lang.String) with arguments=[TLS] from source SSLContext.java:155 FcJ3XfsdKng6O0Qbm7EAAA 12:31:55.503 [TRACE] j.n.s.SSLContext - exiting: javax.net.ssl.SSLContext.getInstance(java.lang.String) with arguments=[TLS] => returnType=javax.net.ssl.SSLContext from source SSLContext.java:157 FcJ3XfsdKng6O0Qbm7EAAB 12:31:55.504 [TRACE] j.n.s.SSLContext - entering: javax.net.ssl.SSLContext.init([Ljavax.net.ssl.KeyManager;,[Ljavax.net.ssl.TrustManager;,java.security.SecureRandom) with arguments=[[org.postgresql.ssl.LazyKeyManager@27a97e08], [org.postgresql.ssl.NonValidatingFactory$NonValidatingTM@5918c260], null] from source SSLContext.java:282 FcJ3XfsdKnk6O0Qbm7EAAA 12:31:55.504 [TRACE] j.n.s.SSLContext - exiting: javax.net.ssl.SSLContext.init([Ljavax.net.ssl.KeyManager;,[Ljavax.net.ssl.TrustManager;,java.security.SecureRandom) with arguments=[[org.postgresql.ssl.LazyKeyManager@27a97e08], [org.postgresql.ssl.NonValidatingFactory$NonValidatingTM@5918c260], null] => returnType=void from source SSLContext.java:283 Be warned that JSSE can be extremely verbose in its toString output.","title":"Instrumenting javax.net.ssl.SSLContext"},{"location":"guide/instrumentation/#instrumenting-classcalledbyagent","text":"If you are already developing an agent, or want finer grained control over Byte Buddy, you can create the agent in process and inspect how Byte Buddy works. This is an advanced use case, but it's useful to get familiar. With the following code: public class ClassCalledByAgent { public void printStatement () { System . out . println ( \"I am a simple println method with no logging\" ); } public void printArgument ( String arg ) { System . out . println ( \"I am a simple println, printing \" + arg ); } public void throwException ( String arg ) { throw new RuntimeException ( \"I'm a squirrel!\" ); } } And the following configuration in logback.conf : logback.bytebuddy { service-name = \"example-service\" tracing { \"com.tersesystems.logback.bytebuddy.ClassCalledByAgent\" = [ \"printStatement\", \"printArgument\", \"throwException\", ] } } and have com.tersesystems.logback.bytebuddy.ClassCalledByAgent logging level set to TRACE in logback.xml . We can start up the agent, add in the builder and run through the methods: public class InProcessInstrumentationExample { public static AgentBuilder . Listener createDebugListener ( List < String > classNames ) { return new AgentBuilder . Listener . Filtering ( LoggingInstrumentationAdvice . stringMatcher ( classNames ), AgentBuilder . Listener . StreamWriting . toSystemOut ()); } public static void main ( String [] args ) throws Exception { // Helps if you install the byte buddy agents before anything else at all happens... ByteBuddyAgent . install (); Logger logger = LoggerFactory . getLogger ( InProcessInstrumentationExample . class ); SystemFlow . setLoggerResolver ( new FixedLoggerResolver ( logger )); Config config = LoggingInstrumentationAdvice . generateConfig ( ClassLoader . getSystemClassLoader (), false ); LoggingInstrumentationAdviceConfig adviceConfig = LoggingInstrumentationAdvice . generateAdviceConfig ( config ); // The debugging listener shows what classes are being picked up by the instrumentation Listener debugListener = createDebugListener ( adviceConfig . classNames ()); new LoggingInstrumentationByteBuddyBuilder () . builderFromConfig ( adviceConfig ) . with ( debugListener ) . installOnByteBuddyAgent (); // No code change necessary here, you can wrap completely in the agent... ClassCalledByAgent classCalledByAgent = new ClassCalledByAgent (); classCalledByAgent . printStatement (); classCalledByAgent . printArgument ( \"42\" ); try { classCalledByAgent . throwException ( \"hello world\" ); } catch ( Exception e ) { // I am too lazy to catch this exception. I hope someone does it for me. } } } And get the following: [Byte Buddy] DISCOVERY com.tersesystems.logback.bytebuddy.ClassCalledByAgent [sun.misc.Launcher$AppClassLoader@75b84c92, null, loaded=true] [Byte Buddy] TRANSFORM com.tersesystems.logback.bytebuddy.ClassCalledByAgent [sun.misc.Launcher$AppClassLoader@75b84c92, null, loaded=true] [Byte Buddy] COMPLETE com.tersesystems.logback.bytebuddy.ClassCalledByAgent [sun.misc.Launcher$AppClassLoader@75b84c92, null, loaded=true] 524 TRACE c.t.l.b.InProcessInstrumentationExample - entering: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printStatement() with arguments=[] from source ClassCalledByAgent.java:18 I am a simple println method with no logging 529 TRACE c.t.l.b.InProcessInstrumentationExample - exiting: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printStatement() with arguments=[] => returnType=void from source ClassCalledByAgent.java:19 529 TRACE c.t.l.b.InProcessInstrumentationExample - entering: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printArgument(java.lang.String) with arguments=[42] from source ClassCalledByAgent.java:22 I am a simple println, printing 42 529 TRACE c.t.l.b.InProcessInstrumentationExample - exiting: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printArgument(java.lang.String) with arguments=[42] => returnType=void from source ClassCalledByAgent.java:23 529 TRACE c.t.l.b.InProcessInstrumentationExample - entering: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.throwException(java.lang.String) with arguments=[hello world] from source ClassCalledByAgent.java:26 532 ERROR c.t.l.b.InProcessInstrumentationExample - throwing: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.throwException(java.lang.String) with arguments=[hello world] ! thrown=java.lang.RuntimeException: I'm a squirrel! java.lang.RuntimeException: I'm a squirrel! at com.tersesystems.logback.bytebuddy.ClassCalledByAgent.throwException(ClassCalledByAgent.java:26) at com.tersesystems.logback.bytebuddy.InProcessInstrumentationExample.main(InProcessInstrumentationExample.java:65) The [Byte Buddy] statements up top are caused by the debug listener, and let you know that Byte Buddy has successfully instrumented the class. Note also that there is no runtime overhead in pulling line numbers or source files into the enter/exit methods, as these are pulled directly from bytecode and do not involve fillInStackTrace .","title":"Instrumenting ClassCalledByAgent"},{"location":"guide/jdbc/","text":"JDBC \u00b6 There is a JDBC appender included which can be subclassed and extended as necessary in the logback-jdbc-appender module. Using a database for logging can be a big help when you just want to get at the logs of the last 30 seconds from inside the application. Because JDBC is both accessible and understandable, there's very little work required for querying. Logback does have a native JDBC appender, but unfortunately it requires three tables and is not set up for easy subclassing. This one is better. This implementation assumes a single table, with a user defined extensible schema, and is set up with HikariCP and a thread pool executor to serve JDBC with minimal blocking. Note that you should always use a JDBC appender behind an LoggingEventAsyncDisruptorAppender and you should have an appropriately sized connection pool for your database traffic. Database timestamps record time with microsecond resolution, whereas millisecond resolution is commonplace for logging, so for convenience both the timestamp with time zone and the time since epoch are recorded. For span information, the start time must also be recorded as TSE. Likewise, the level is recorded as both a text string for visual reference, and a level value so that you can order and filter database queries. Querying a database can be particuarly helpful when errors occur, because you can pull out all logs with a correlation id. See the logback-correlationid module for an example. Logging using in-memory H2 Database \u00b6 Using an in memory H2 database is a cheap and easy way to expose logs from inside the application without having to parse files. <appender name= \"H2_JDBC\" class= \"com.tersesystems.logback.jdbc.JDBCAppender\" > <driver> jdbc:h2:mem:logback </driver> <url> org.h2.Driver </url> <username> sa </username> <password></password> <createStatements> CREATE TABLE IF NOT EXISTS events ( ID INT NOT NULL PRIMARY KEY AUTO_INCREMENT, ts TIMESTAMP(9) WITH TIME ZONE NOT NULL, tse_ms numeric NOT NULL, start_ms numeric NULL, level_value int NOT NULL, level VARCHAR(7) NOT NULL, evt JSON NOT NULL ); </createStatements> <insertStatement> insert into events(ts, tse_ms, start_ms, level_value, level, evt) values(?, ?, ?, ?, ?, ?) </insertStatement> <reaperStatement> delete from events where ts &lt; ? </reaperStatement> <reaperSchedule> PT30 </reaperSchedule> <encoder class= \"net.logstash.logback.encoder.LogstashEncoder\" > </encoder> </appender> Logging using PostgresSQL \u00b6 If you want something larger scale, you'll probably be using Postgres instead of H2. You can log JSON to PostgreSQL, using the built-in JSON datatype . Postgres uses a custom JDBC type of PGObject , so the insertEvent method must be subclassed. This is what's in the logback-postgresjson-appender module: public class PostgresJsonAppender extends JDBCAppender { private String objectType = \"json\" ; public String getObjectType () { return objectType ; } public void setObjectType ( String objectType ) { this . objectType = objectType ; } @Override public void start () { super . start (); setDriver ( \"org.postgresql.Driver\" ); } @Override protected void insertEvent ( ILoggingEvent event , LongAdder adder , PreparedStatement statement ) throws SQLException { PGobject jsonObject = new PGobject (); jsonObject . setType ( getObjectType ()); byte [] bytes = getEncoder (). encode ( event ); jsonObject . setValue ( new String ( bytes , StandardCharsets . UTF_8 )); statement . setObject ( adder . intValue (), jsonObject ); adder . increment (); } } First, install PostgreSQL, create a database logback , a role logback and a password logback and add the following table: CREATE TABLE logging_table ( ID serial NOT NULL PRIMARY KEY , ts TIMESTAMPTZ ( 6 ) NOT NULL , tse_ms numeric NOT NULL , start_ms numeric NULL , level_value int NOT NULL , level VARCHAR ( 7 ) NOT NULL , evt jsonb NOT NULL ); CREATE INDEX idxgin ON logging_table USING gin ( evt ); Because logs are inherently time-series data, you can use the timescaleDB postgresql extension as described in Store application logs in timescaleDB/postgres , but that's not required. Then, add the following logback.xml : <configuration> <!-- async appender needs a shutdown hook to make sure this clears --> <shutdownHook class= \"ch.qos.logback.core.hook.DelayingShutdownHook\" /> <conversionRule conversionWord= \"startTime\" converterClass= \"com.tersesystems.logback.classic.StartTimeConverter\" /> <!-- SQL is blocking, so use an async lmax appender here --> <appender name= \"ASYNC_POSTGRES\" class= \"net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender\" > <appender class= \"com.tersesystems.logback.postgresjson.PostgresJsonAppender\" > <createStatements> CREATE TABLE IF NOT EXISTS logging_table ( ID serial NOT NULL PRIMARY KEY, ts TIMESTAMPTZ(6) NOT NULL, tse_ms bigint NOT NULL, start_ms bigint NULL, level_value int NOT NULL, level VARCHAR(7) NOT NULL, evt jsonb NOT NULL ); CREATE INDEX idxgin ON logging_table USING gin (evt); </createStatements> <!-- SQL statement takes a TIMESTAMP, LONG, INT, VARCHAR, PGObject --> <insertStatement> insert into logging_table(ts, tse_ms, start_ms, level_value, level, evt) values(?, ?, ?, ?, ?, ?) </insertStatement> <url> jdbc:postgresql://localhost:5432/logback </url> <username> logback </username> <password> logback </password> <encoder class= \"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\" > <providers> <message/> <loggerName/> <threadName/> <logLevel/> <stackHash/> <mdc/> <logstashMarkers/> <pattern> <pattern> { \"start_ms\": \"#asLong{%%startTime}\" } </pattern> </pattern> <arguments/> <stackTrace> <throwableConverter class= \"net.logstash.logback.stacktrace.ShortenedThrowableConverter\" > <rootCauseFirst> true </rootCauseFirst> </throwableConverter> </stackTrace> </providers> </encoder> </appender> </appender> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <root level= \"INFO\" > <appender-ref ref= \"ASYNC_POSTGRES\" /> <appender-ref ref= \"STDOUT\" /> </root> </configuration> Querying requires a little bit of extra syntax, using evt->'myfield' to select: select ts as end_date , start_ms as epoch_start , tse_ms as epoch_end , evt -> 'trace.span_id' as span_id , evt -> 'name' as name , evt -> 'message' as message , evt -> 'trace.parent_id' as parent , evt -> 'duration_ms' as duration_ms from logging_table where evt -> 'trace.trace_id' IS NOT NULL order by ts desc limit 5 If you have extra logs that you want to import into PostgreSQL, you can use PSQL to do that . Extending JDBC Appender with extra fields \u00b6 The JDBC appender can be extended so you can add extra information to the table. In the logback-correlationid module, there's a CorrelationIdJdbcAppender that adds extra information into the event so you can query by the correlation id specifically, by using the insertAdditionalData hook: public class CorrelationIdJdbcAppender extends JDBCAppender { private String mdcKey = \"correlation_id\" ; public String getMdcKey () { return mdcKey ; } public void setMdcKey ( String mdcKey ) { this . mdcKey = mdcKey ; } protected CorrelationIdUtils utils ; @Override public void start () { super . start (); utils = new CorrelationIdUtils ( mdcKey ); } @Override protected void insertAdditionalData ( ILoggingEvent event , LongAdder adder , PreparedStatement statement ) throws SQLException { insertCorrelationId ( event , adder , statement ); } private void insertCorrelationId ( ILoggingEvent event , LongAdder adder , PreparedStatement statement ) throws SQLException { Optional < String > maybeCorrelationId = utils . get ( event . getMarker ()); if ( maybeCorrelationId . isPresent ()) { statement . setString ( adder . intValue (), maybeCorrelationId . get ()); } else { statement . setNull ( adder . intValue (), Types . VARCHAR ); } adder . increment (); } } Then set up the table and add an index on the correlation id: CREATE TABLE IF NOT EXISTS events ( ID NUMERIC NOT NULL PRIMARY KEY AUTO_INCREMENT , ts TIMESTAMP ( 9 ) WITH TIME ZONE NOT NULL , tse_ms numeric NOT NULL , start_ms numeric NULL , level_value int NOT NULL , level VARCHAR ( 7 ) NOT NULL , evt JSON NOT NULL , correlation_id VARCHAR ( 255 ) NULL ); CREATE INDEX correlation_id_idx ON events ( correlation_id ); And then you can query from there. See Logging Structured Data to Database for more details.","title":"JDBC"},{"location":"guide/jdbc/#jdbc","text":"There is a JDBC appender included which can be subclassed and extended as necessary in the logback-jdbc-appender module. Using a database for logging can be a big help when you just want to get at the logs of the last 30 seconds from inside the application. Because JDBC is both accessible and understandable, there's very little work required for querying. Logback does have a native JDBC appender, but unfortunately it requires three tables and is not set up for easy subclassing. This one is better. This implementation assumes a single table, with a user defined extensible schema, and is set up with HikariCP and a thread pool executor to serve JDBC with minimal blocking. Note that you should always use a JDBC appender behind an LoggingEventAsyncDisruptorAppender and you should have an appropriately sized connection pool for your database traffic. Database timestamps record time with microsecond resolution, whereas millisecond resolution is commonplace for logging, so for convenience both the timestamp with time zone and the time since epoch are recorded. For span information, the start time must also be recorded as TSE. Likewise, the level is recorded as both a text string for visual reference, and a level value so that you can order and filter database queries. Querying a database can be particuarly helpful when errors occur, because you can pull out all logs with a correlation id. See the logback-correlationid module for an example.","title":"JDBC"},{"location":"guide/jdbc/#logging-using-in-memory-h2-database","text":"Using an in memory H2 database is a cheap and easy way to expose logs from inside the application without having to parse files. <appender name= \"H2_JDBC\" class= \"com.tersesystems.logback.jdbc.JDBCAppender\" > <driver> jdbc:h2:mem:logback </driver> <url> org.h2.Driver </url> <username> sa </username> <password></password> <createStatements> CREATE TABLE IF NOT EXISTS events ( ID INT NOT NULL PRIMARY KEY AUTO_INCREMENT, ts TIMESTAMP(9) WITH TIME ZONE NOT NULL, tse_ms numeric NOT NULL, start_ms numeric NULL, level_value int NOT NULL, level VARCHAR(7) NOT NULL, evt JSON NOT NULL ); </createStatements> <insertStatement> insert into events(ts, tse_ms, start_ms, level_value, level, evt) values(?, ?, ?, ?, ?, ?) </insertStatement> <reaperStatement> delete from events where ts &lt; ? </reaperStatement> <reaperSchedule> PT30 </reaperSchedule> <encoder class= \"net.logstash.logback.encoder.LogstashEncoder\" > </encoder> </appender>","title":"Logging using in-memory H2 Database"},{"location":"guide/jdbc/#logging-using-postgressql","text":"If you want something larger scale, you'll probably be using Postgres instead of H2. You can log JSON to PostgreSQL, using the built-in JSON datatype . Postgres uses a custom JDBC type of PGObject , so the insertEvent method must be subclassed. This is what's in the logback-postgresjson-appender module: public class PostgresJsonAppender extends JDBCAppender { private String objectType = \"json\" ; public String getObjectType () { return objectType ; } public void setObjectType ( String objectType ) { this . objectType = objectType ; } @Override public void start () { super . start (); setDriver ( \"org.postgresql.Driver\" ); } @Override protected void insertEvent ( ILoggingEvent event , LongAdder adder , PreparedStatement statement ) throws SQLException { PGobject jsonObject = new PGobject (); jsonObject . setType ( getObjectType ()); byte [] bytes = getEncoder (). encode ( event ); jsonObject . setValue ( new String ( bytes , StandardCharsets . UTF_8 )); statement . setObject ( adder . intValue (), jsonObject ); adder . increment (); } } First, install PostgreSQL, create a database logback , a role logback and a password logback and add the following table: CREATE TABLE logging_table ( ID serial NOT NULL PRIMARY KEY , ts TIMESTAMPTZ ( 6 ) NOT NULL , tse_ms numeric NOT NULL , start_ms numeric NULL , level_value int NOT NULL , level VARCHAR ( 7 ) NOT NULL , evt jsonb NOT NULL ); CREATE INDEX idxgin ON logging_table USING gin ( evt ); Because logs are inherently time-series data, you can use the timescaleDB postgresql extension as described in Store application logs in timescaleDB/postgres , but that's not required. Then, add the following logback.xml : <configuration> <!-- async appender needs a shutdown hook to make sure this clears --> <shutdownHook class= \"ch.qos.logback.core.hook.DelayingShutdownHook\" /> <conversionRule conversionWord= \"startTime\" converterClass= \"com.tersesystems.logback.classic.StartTimeConverter\" /> <!-- SQL is blocking, so use an async lmax appender here --> <appender name= \"ASYNC_POSTGRES\" class= \"net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender\" > <appender class= \"com.tersesystems.logback.postgresjson.PostgresJsonAppender\" > <createStatements> CREATE TABLE IF NOT EXISTS logging_table ( ID serial NOT NULL PRIMARY KEY, ts TIMESTAMPTZ(6) NOT NULL, tse_ms bigint NOT NULL, start_ms bigint NULL, level_value int NOT NULL, level VARCHAR(7) NOT NULL, evt jsonb NOT NULL ); CREATE INDEX idxgin ON logging_table USING gin (evt); </createStatements> <!-- SQL statement takes a TIMESTAMP, LONG, INT, VARCHAR, PGObject --> <insertStatement> insert into logging_table(ts, tse_ms, start_ms, level_value, level, evt) values(?, ?, ?, ?, ?, ?) </insertStatement> <url> jdbc:postgresql://localhost:5432/logback </url> <username> logback </username> <password> logback </password> <encoder class= \"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\" > <providers> <message/> <loggerName/> <threadName/> <logLevel/> <stackHash/> <mdc/> <logstashMarkers/> <pattern> <pattern> { \"start_ms\": \"#asLong{%%startTime}\" } </pattern> </pattern> <arguments/> <stackTrace> <throwableConverter class= \"net.logstash.logback.stacktrace.ShortenedThrowableConverter\" > <rootCauseFirst> true </rootCauseFirst> </throwableConverter> </stackTrace> </providers> </encoder> </appender> </appender> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <root level= \"INFO\" > <appender-ref ref= \"ASYNC_POSTGRES\" /> <appender-ref ref= \"STDOUT\" /> </root> </configuration> Querying requires a little bit of extra syntax, using evt->'myfield' to select: select ts as end_date , start_ms as epoch_start , tse_ms as epoch_end , evt -> 'trace.span_id' as span_id , evt -> 'name' as name , evt -> 'message' as message , evt -> 'trace.parent_id' as parent , evt -> 'duration_ms' as duration_ms from logging_table where evt -> 'trace.trace_id' IS NOT NULL order by ts desc limit 5 If you have extra logs that you want to import into PostgreSQL, you can use PSQL to do that .","title":"Logging using PostgresSQL"},{"location":"guide/jdbc/#extending-jdbc-appender-with-extra-fields","text":"The JDBC appender can be extended so you can add extra information to the table. In the logback-correlationid module, there's a CorrelationIdJdbcAppender that adds extra information into the event so you can query by the correlation id specifically, by using the insertAdditionalData hook: public class CorrelationIdJdbcAppender extends JDBCAppender { private String mdcKey = \"correlation_id\" ; public String getMdcKey () { return mdcKey ; } public void setMdcKey ( String mdcKey ) { this . mdcKey = mdcKey ; } protected CorrelationIdUtils utils ; @Override public void start () { super . start (); utils = new CorrelationIdUtils ( mdcKey ); } @Override protected void insertAdditionalData ( ILoggingEvent event , LongAdder adder , PreparedStatement statement ) throws SQLException { insertCorrelationId ( event , adder , statement ); } private void insertCorrelationId ( ILoggingEvent event , LongAdder adder , PreparedStatement statement ) throws SQLException { Optional < String > maybeCorrelationId = utils . get ( event . getMarker ()); if ( maybeCorrelationId . isPresent ()) { statement . setString ( adder . intValue (), maybeCorrelationId . get ()); } else { statement . setNull ( adder . intValue (), Types . VARCHAR ); } adder . increment (); } } Then set up the table and add an index on the correlation id: CREATE TABLE IF NOT EXISTS events ( ID NUMERIC NOT NULL PRIMARY KEY AUTO_INCREMENT , ts TIMESTAMP ( 9 ) WITH TIME ZONE NOT NULL , tse_ms numeric NOT NULL , start_ms numeric NULL , level_value int NOT NULL , level VARCHAR ( 7 ) NOT NULL , evt JSON NOT NULL , correlation_id VARCHAR ( 255 ) NULL ); CREATE INDEX correlation_id_idx ON events ( correlation_id ); And then you can query from there. See Logging Structured Data to Database for more details.","title":"Extending JDBC Appender with extra fields"},{"location":"guide/relativens/","text":"Relative Nanoseconds Appender \u00b6 LoggingEvent already has a timestamp associated with it, but that timestamp is generated by System.currentTimeMillis . For most appenders, this would be fine, but appending to ringbuffer is so fast that several events can be appended in the same millisecond, exceeding the resolution and making direct time comparison difficult when those records are dumped to JDBC. Adding a relative_ns field provides resolution down to the nanosecond, sort of . For example, here's two different records with the same millisecond. { \"id\" : \"FfwJtsNLLWo6O0Qbm7EAAA\" , \"relative_ns\" : 11808036 , \"tse_ms\" : 1584163603315 , \"start_ms\" : null , \"@timestamp\" : \"2020-03-14T05:26:43.315Z\" , \"@version\" : \"1\" , \"message\" : \"HikariPool-2 - Start completed.\" , \"logger_name\" : \"com.zaxxer.hikari.HikariDataSource\" , \"thread_name\" : \"play-dev-mode-akka.actor.default-dispatcher-7\" , \"level\" : \"INFO\" , \"level_value\" : 20000 } { \"id\" : \"FfwJtsNLLWo6O0Qbm7EAAB\" , \"relative_ns\" : 11981656 , \"tse_ms\" : 1584163603315 , \"start_ms\" : null , \"@timestamp\" : \"2020-03-14T05:26:43.315Z\" , \"@version\" : \"1\" , \"message\" : \"jdbc-appender-pool-1584163602961 - Start completed.\" , \"logger_name\" : \"com.zaxxer.hikari.HikariDataSource\" , \"thread_name\" : \"logback-appender-ASYNC_JDBC-2\" , \"level\" : \"INFO\" , \"level_value\" : 20000 } Note that the timestamp is 2020-03-14T05:26:43.315Z and the time since epoch is 1584163603315 . The flake ids distinguish between log entries by using a counter when millisecond precision is exceeded, so the first record is FfwJtsNLLWo6O0Qbm7EAAA ending in A and the second record is FfwJtsNLLWo6O0Qbm7EAAB ending in B . The relative time tells us exactly how much time has elapsed between the two events: 11981656 - 11808036 is 0.17362 milliseconds. All logging events are computed using System.nanoTime - NanoTime.start , where NanoTime.start is a static final field initialized JVM start time (technically at class loading but close enough). This value may be negative to begin with, but always increments. See the showcase for an example. Usage \u00b6 <appender class= \"com.tersesystems.logback.classic.NanoTimeComponentAppender\" > <appender ... > </appender> </appender> You can extract the nanotime using a converter: <!-- available as \"%nanoTime\" in a pattern layout --> <conversionRule conversionWord= \"nanoTime\" converterClass= \"com.tersesystems.logback.classic.NanoTimeConverter\" /> There are no configuration options.","title":"Relative Nanos"},{"location":"guide/relativens/#relative-nanoseconds-appender","text":"LoggingEvent already has a timestamp associated with it, but that timestamp is generated by System.currentTimeMillis . For most appenders, this would be fine, but appending to ringbuffer is so fast that several events can be appended in the same millisecond, exceeding the resolution and making direct time comparison difficult when those records are dumped to JDBC. Adding a relative_ns field provides resolution down to the nanosecond, sort of . For example, here's two different records with the same millisecond. { \"id\" : \"FfwJtsNLLWo6O0Qbm7EAAA\" , \"relative_ns\" : 11808036 , \"tse_ms\" : 1584163603315 , \"start_ms\" : null , \"@timestamp\" : \"2020-03-14T05:26:43.315Z\" , \"@version\" : \"1\" , \"message\" : \"HikariPool-2 - Start completed.\" , \"logger_name\" : \"com.zaxxer.hikari.HikariDataSource\" , \"thread_name\" : \"play-dev-mode-akka.actor.default-dispatcher-7\" , \"level\" : \"INFO\" , \"level_value\" : 20000 } { \"id\" : \"FfwJtsNLLWo6O0Qbm7EAAB\" , \"relative_ns\" : 11981656 , \"tse_ms\" : 1584163603315 , \"start_ms\" : null , \"@timestamp\" : \"2020-03-14T05:26:43.315Z\" , \"@version\" : \"1\" , \"message\" : \"jdbc-appender-pool-1584163602961 - Start completed.\" , \"logger_name\" : \"com.zaxxer.hikari.HikariDataSource\" , \"thread_name\" : \"logback-appender-ASYNC_JDBC-2\" , \"level\" : \"INFO\" , \"level_value\" : 20000 } Note that the timestamp is 2020-03-14T05:26:43.315Z and the time since epoch is 1584163603315 . The flake ids distinguish between log entries by using a counter when millisecond precision is exceeded, so the first record is FfwJtsNLLWo6O0Qbm7EAAA ending in A and the second record is FfwJtsNLLWo6O0Qbm7EAAB ending in B . The relative time tells us exactly how much time has elapsed between the two events: 11981656 - 11808036 is 0.17362 milliseconds. All logging events are computed using System.nanoTime - NanoTime.start , where NanoTime.start is a static final field initialized JVM start time (technically at class loading but close enough). This value may be negative to begin with, but always increments. See the showcase for an example.","title":"Relative Nanoseconds Appender"},{"location":"guide/relativens/#usage","text":"<appender class= \"com.tersesystems.logback.classic.NanoTimeComponentAppender\" > <appender ... > </appender> </appender> You can extract the nanotime using a converter: <!-- available as \"%nanoTime\" in a pattern layout --> <conversionRule conversionWord= \"nanoTime\" converterClass= \"com.tersesystems.logback.classic.NanoTimeConverter\" /> There are no configuration options.","title":"Usage"},{"location":"guide/ringbuffer/","text":"Ring Buffers \u00b6 There may be situations where there is no visible target for diagnostic logging, for example in the case where there is a race condition or a subtle data corruption that only shows up every so often. In this case, the ideal workflow would be to keep the most recent diagnostic information available, but only see it when the appropriate condition is triggered. This is a pattern called ring buffer logging, described in Using Ring Buffer Logging to Help Find Bugs by Brian Marick . In ring buffer logging, all debug events related to the logger are stored, but are stored in a circular buffer that is overwritten by the latest logs. When triggered, the entire buffer is flushed to appenders. This is in contrast to tap filters, which will immediately create events and then flush them to appenders, and do not keep them in memory. Using Ring Buffers \u00b6 Ring Buffers are first class objects that must be referenced at several points. You need to set up an appender that can add logging events to a ring buffer, and another that can dump elements. NOTE : Measuring time is a real concern with ring buffer logging. When you dump the ring buffer contents, unless you are dumping into a JDBC database, the elements will be out of sequence to the logs as a whole. Also, because logging to an in-memory ringbuffer is extremely fast and involves no processing, multiple events can be logged in the same millisecond. The showcase contains an example of ringbuffer logging that we'll describe in more detail here. To create a ring buffer, add the following actions: <!-- loosen rule to include for encoders as well as just top level --> <newRule pattern= \"*/include\" actionClass= \"ch.qos.logback.core.joran.action.IncludeAction\" /> <!-- loosen the rule on appender refs so appenders can reference them --> <newRule pattern= \"*/appender/appender-ref\" actionClass= \"ch.qos.logback.core.joran.action.AppenderRefAction\" /> <newRule pattern= \"*/ringBuffer\" actionClass= \"com.tersesystems.logback.ringbuffer.RingBufferAction\" /> <newRule pattern= \"*/ringBuffer-ref\" actionClass= \"com.tersesystems.logback.ringbuffer.RingBufferRefAction\" /> and then define the ring buffer itself: <ringBuffer name= \"JDBC_RINGBUFFER\" > <capacity> ${jdbc.ringBuffer.capacity} </capacity> </ringBuffer> The ring buffer uses a multi-producer/multi-consumer array queue from JCTools , which has better performance than java.util.concurrent.ArrayBlockingQueue . Adding to Ring Buffer \u00b6 This works well with a JDBC appender. NOTE : The CorrelationIdJDBCAppender is used here so that the correlation_id and event_id fields are available, where the event_id is specified using a UniqueIdEventAppender . The UniqueIdEventAppender decorates logging events to have a flake id using idem . Because inserts into a database can happen out of order and the timestamp does not have enough resolution, a flake id is the only sure way to keep ordering consistent. < appender name = \"ASYNC_JDBC\" class = \"net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender\" > <!-- A JDBC appender that adds the correlation id as a field . --> < appender class = \"com.tersesystems.logback.correlationid.CorrelationIdJDBCAppender\" > <!-- ... --> </ appender > </ appender > <!-- assume something like this for unique event id appender --> < root > < appender class = \"com.tersesystems.logback.uniqueid.UniqueIdComponentAppender\" > < appender - ref ref = \"ASYNC_JDBC\" /> </ appender > </ root > Appending to a ring buffer is done with com.tersesystems.logback.ringbuffer.RingBufferAwareAppender . The logic on RingBufferAwareAppender is different from most other appenders in that any logging events that are rejected by the filter will be logged to the ringbuffer without further processing. <appender name= \"ASYNC_JDBC_WITH_RINGBUFFER\" class= \"com.tersesystems.logback.ringbuffer.RingBufferAwareAppender\" > <!-- log to jdbc if INFO or above, otherwise log to ring buffer --> <filter class= \"ch.qos.logback.classic.filter.ThresholdFilter\" > <level> ${jdbc.threshold} </level> </filter> <!-- anything denied goes to the ring buffer --> <ringBuffer-ref ref= \"JDBC_RINGBUFFER\" /> <!-- anything accepted goes to the JDBC appender --> <appender-ref ref= \"ASYNC_JDBC\" /> </appender> Dumping Ring Buffers \u00b6 Finally, there needs to be a way to dump the contents of a ring buffer. This is done from a targetted logger that contains an instance of com.tersesystems.logback.ringbuffer.DumpRingBufferAppender . The entire contents of the ring buffer is drained to the appender. NOTE : If you are draining to an asynchronous logger, you should ensure that the queue is large enough to accommodate the contents of the entire ring buffer in addition to normal traffic. This is because many asynchronous loggers such as AsyncAppender are lossy and will drop events if the queue fills up passes a comfortable threshold. <logger name= \"JDBC_RINGBUFFER_LOGGER\" level= \"TRACE\" additivity= \"false\" > <!-- This appender dumps contents of the ring buffer when an event is received. --> <appender class= \"com.tersesystems.logback.ringbuffer.DumpRingBufferAppender\" > <!-- Event source --> <ringBuffer-ref ref= \"JDBC_RINGBUFFER\" /> <!-- Event source --> <appender-ref ref= \"ASYNC_JDBC\" /> </appender> </logger> Once you've defined a logger, you can trigger a dump just by logging to that specific appender: Logger bufferControl = LoggerFactory . getLogger ( \"JDBC_RINGBUFFER_LOGGER\" ); bufferControl . error ( \"Dump the ringbuffer to JDBC here!\" ); RingBuffer Markers \u00b6 A marker factory that contains a ringbuffer and two inner classes, RecordMarker and DumpMarker. Using a ring buffer marker factory means that you can build up a thread of messages and dump the ring buffer at a later point, for example: public class Foo { public void logAndDump () { RingBuffer ringBuffer = getRingBuffer (); RingBufferMarkerFactory markerFactory = new RingBufferMarkerFactory ( ringBuffer ); Marker recordMarker = markerFactory . createRecordMarker (); Marker dumpMarker = markerFactory . createTriggerMarker (); Logger logger = loggerFactory . getLogger ( \"com.example.Test\" ); logger . debug ( recordMarker , \"debug one\" ); logger . debug ( recordMarker , \"debug two\" ); logger . debug ( recordMarker , \"debug three\" ); logger . debug ( recordMarker , \"debug four\" ); logger . error ( dumpMarker , \"Dump all the messages\" ); } } Further Reading \u00b6 See Triggering Diagnostic Logging on Exception and Diagnostic Logging: Citations and Sources for more details.","title":"Ring Buffers"},{"location":"guide/ringbuffer/#ring-buffers","text":"There may be situations where there is no visible target for diagnostic logging, for example in the case where there is a race condition or a subtle data corruption that only shows up every so often. In this case, the ideal workflow would be to keep the most recent diagnostic information available, but only see it when the appropriate condition is triggered. This is a pattern called ring buffer logging, described in Using Ring Buffer Logging to Help Find Bugs by Brian Marick . In ring buffer logging, all debug events related to the logger are stored, but are stored in a circular buffer that is overwritten by the latest logs. When triggered, the entire buffer is flushed to appenders. This is in contrast to tap filters, which will immediately create events and then flush them to appenders, and do not keep them in memory.","title":"Ring Buffers"},{"location":"guide/ringbuffer/#using-ring-buffers","text":"Ring Buffers are first class objects that must be referenced at several points. You need to set up an appender that can add logging events to a ring buffer, and another that can dump elements. NOTE : Measuring time is a real concern with ring buffer logging. When you dump the ring buffer contents, unless you are dumping into a JDBC database, the elements will be out of sequence to the logs as a whole. Also, because logging to an in-memory ringbuffer is extremely fast and involves no processing, multiple events can be logged in the same millisecond. The showcase contains an example of ringbuffer logging that we'll describe in more detail here. To create a ring buffer, add the following actions: <!-- loosen rule to include for encoders as well as just top level --> <newRule pattern= \"*/include\" actionClass= \"ch.qos.logback.core.joran.action.IncludeAction\" /> <!-- loosen the rule on appender refs so appenders can reference them --> <newRule pattern= \"*/appender/appender-ref\" actionClass= \"ch.qos.logback.core.joran.action.AppenderRefAction\" /> <newRule pattern= \"*/ringBuffer\" actionClass= \"com.tersesystems.logback.ringbuffer.RingBufferAction\" /> <newRule pattern= \"*/ringBuffer-ref\" actionClass= \"com.tersesystems.logback.ringbuffer.RingBufferRefAction\" /> and then define the ring buffer itself: <ringBuffer name= \"JDBC_RINGBUFFER\" > <capacity> ${jdbc.ringBuffer.capacity} </capacity> </ringBuffer> The ring buffer uses a multi-producer/multi-consumer array queue from JCTools , which has better performance than java.util.concurrent.ArrayBlockingQueue .","title":"Using Ring Buffers"},{"location":"guide/ringbuffer/#adding-to-ring-buffer","text":"This works well with a JDBC appender. NOTE : The CorrelationIdJDBCAppender is used here so that the correlation_id and event_id fields are available, where the event_id is specified using a UniqueIdEventAppender . The UniqueIdEventAppender decorates logging events to have a flake id using idem . Because inserts into a database can happen out of order and the timestamp does not have enough resolution, a flake id is the only sure way to keep ordering consistent. < appender name = \"ASYNC_JDBC\" class = \"net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender\" > <!-- A JDBC appender that adds the correlation id as a field . --> < appender class = \"com.tersesystems.logback.correlationid.CorrelationIdJDBCAppender\" > <!-- ... --> </ appender > </ appender > <!-- assume something like this for unique event id appender --> < root > < appender class = \"com.tersesystems.logback.uniqueid.UniqueIdComponentAppender\" > < appender - ref ref = \"ASYNC_JDBC\" /> </ appender > </ root > Appending to a ring buffer is done with com.tersesystems.logback.ringbuffer.RingBufferAwareAppender . The logic on RingBufferAwareAppender is different from most other appenders in that any logging events that are rejected by the filter will be logged to the ringbuffer without further processing. <appender name= \"ASYNC_JDBC_WITH_RINGBUFFER\" class= \"com.tersesystems.logback.ringbuffer.RingBufferAwareAppender\" > <!-- log to jdbc if INFO or above, otherwise log to ring buffer --> <filter class= \"ch.qos.logback.classic.filter.ThresholdFilter\" > <level> ${jdbc.threshold} </level> </filter> <!-- anything denied goes to the ring buffer --> <ringBuffer-ref ref= \"JDBC_RINGBUFFER\" /> <!-- anything accepted goes to the JDBC appender --> <appender-ref ref= \"ASYNC_JDBC\" /> </appender>","title":"Adding to Ring Buffer"},{"location":"guide/ringbuffer/#dumping-ring-buffers","text":"Finally, there needs to be a way to dump the contents of a ring buffer. This is done from a targetted logger that contains an instance of com.tersesystems.logback.ringbuffer.DumpRingBufferAppender . The entire contents of the ring buffer is drained to the appender. NOTE : If you are draining to an asynchronous logger, you should ensure that the queue is large enough to accommodate the contents of the entire ring buffer in addition to normal traffic. This is because many asynchronous loggers such as AsyncAppender are lossy and will drop events if the queue fills up passes a comfortable threshold. <logger name= \"JDBC_RINGBUFFER_LOGGER\" level= \"TRACE\" additivity= \"false\" > <!-- This appender dumps contents of the ring buffer when an event is received. --> <appender class= \"com.tersesystems.logback.ringbuffer.DumpRingBufferAppender\" > <!-- Event source --> <ringBuffer-ref ref= \"JDBC_RINGBUFFER\" /> <!-- Event source --> <appender-ref ref= \"ASYNC_JDBC\" /> </appender> </logger> Once you've defined a logger, you can trigger a dump just by logging to that specific appender: Logger bufferControl = LoggerFactory . getLogger ( \"JDBC_RINGBUFFER_LOGGER\" ); bufferControl . error ( \"Dump the ringbuffer to JDBC here!\" );","title":"Dumping Ring Buffers"},{"location":"guide/ringbuffer/#ringbuffer-markers","text":"A marker factory that contains a ringbuffer and two inner classes, RecordMarker and DumpMarker. Using a ring buffer marker factory means that you can build up a thread of messages and dump the ring buffer at a later point, for example: public class Foo { public void logAndDump () { RingBuffer ringBuffer = getRingBuffer (); RingBufferMarkerFactory markerFactory = new RingBufferMarkerFactory ( ringBuffer ); Marker recordMarker = markerFactory . createRecordMarker (); Marker dumpMarker = markerFactory . createTriggerMarker (); Logger logger = loggerFactory . getLogger ( \"com.example.Test\" ); logger . debug ( recordMarker , \"debug one\" ); logger . debug ( recordMarker , \"debug two\" ); logger . debug ( recordMarker , \"debug three\" ); logger . debug ( recordMarker , \"debug four\" ); logger . error ( dumpMarker , \"Dump all the messages\" ); } }","title":"RingBuffer Markers"},{"location":"guide/ringbuffer/#further-reading","text":"See Triggering Diagnostic Logging on Exception and Diagnostic Logging: Citations and Sources for more details.","title":"Further Reading"},{"location":"guide/select/","text":"Select Appender \u00b6 Different appenders are useful in different environments. Development wants: Want colorized output on their consoles, with line oriented logs. Would also like to be able to read through logs with debug, info and warnings in them, to track control flow. If you have the logs seperated, that makes it harder. Generally don't want to run a local ELK stack or TCP appenders to see their logs. Operations wants: Really want centralized logging, and a way to drill out on it. Structured logging especially. May want to have everything write to STDOUT, as is case for Docker / 12 Factor Apps. May have duplicate logs from the underlying architecture, that need to be dedupped. May not want redundant / repeated messages, which developers are not as sensitive to. Really hate getting paged with the same error repeatedly. Logback is not aware of different environments. There's no out of the box way to say \"in this environment I want these sets of appenders, but in this other environment I want these other sets of appenders.\" Fortunately, adding this is pretty easy, by leveraging AppenderAttachable and pulling a key to select on: public class SelectAppender extends AppenderBase < ILoggingEvent > implements AppenderAttachable < ILoggingEvent > { private AppenderAttachableImpl < ILoggingEvent > aai = new AppenderAttachableImpl < ILoggingEvent > (); private String appenderKey ; @Override protected void append ( ILoggingEvent eventObject ) { Appender < ILoggingEvent > appender = aai . getAppender ( appenderKey ); if ( appender == null ) { addError ( \"No appender found for appenderKey \" + appenderKey ); } else { appender . doAppend ( eventObject ); } } // ... } The logback appenders under selection must have the name defined as an element, because Logback only looks for the name attribute at the top level, but otherwise they're the same. Here, we select the set of appenders we want based on the LOGBACK_ENVIRONMENT environment variable. <configuration> <appender name= \"SELECT\" class= \"com.tersesystems.logback.SelectAppender\" > <appenderKey> ${LOGBACK_ENVIRONMENT} </appenderKey> <appender class= \"com.tersesystems.logback.CompositeAppender\" > <name> test </name> <appender class= \"ch.qos.logback.core.read.ListAppender\" > <name> test-list </name> </appender> </appender> <appender class= \"com.tersesystems.logback.CompositeAppender\" > <name> development </name> <appender class= \"ch.qos.logback.core.ConsoleAppender\" > <name> development-console </name> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> </appender> <appender class= \"com.tersesystems.logback.CompositeAppender\" > <name> staging </name> <appender class= \"ch.qos.logback.core.ConsoleAppender\" > <name> staging-console </name> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <appender class= \"ch.qos.logback.core.FileAppender\" > <name> staging-file </name> <file> file.log </file> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> </appender> </appender> <root level= \"TRACE\" > <appender-ref ref= \"SELECT\" /> </root> </configuration> This is a much cleaner way to organize appenders than putting Janino logic into the configuration.","title":"Select Appender"},{"location":"guide/select/#select-appender","text":"Different appenders are useful in different environments. Development wants: Want colorized output on their consoles, with line oriented logs. Would also like to be able to read through logs with debug, info and warnings in them, to track control flow. If you have the logs seperated, that makes it harder. Generally don't want to run a local ELK stack or TCP appenders to see their logs. Operations wants: Really want centralized logging, and a way to drill out on it. Structured logging especially. May want to have everything write to STDOUT, as is case for Docker / 12 Factor Apps. May have duplicate logs from the underlying architecture, that need to be dedupped. May not want redundant / repeated messages, which developers are not as sensitive to. Really hate getting paged with the same error repeatedly. Logback is not aware of different environments. There's no out of the box way to say \"in this environment I want these sets of appenders, but in this other environment I want these other sets of appenders.\" Fortunately, adding this is pretty easy, by leveraging AppenderAttachable and pulling a key to select on: public class SelectAppender extends AppenderBase < ILoggingEvent > implements AppenderAttachable < ILoggingEvent > { private AppenderAttachableImpl < ILoggingEvent > aai = new AppenderAttachableImpl < ILoggingEvent > (); private String appenderKey ; @Override protected void append ( ILoggingEvent eventObject ) { Appender < ILoggingEvent > appender = aai . getAppender ( appenderKey ); if ( appender == null ) { addError ( \"No appender found for appenderKey \" + appenderKey ); } else { appender . doAppend ( eventObject ); } } // ... } The logback appenders under selection must have the name defined as an element, because Logback only looks for the name attribute at the top level, but otherwise they're the same. Here, we select the set of appenders we want based on the LOGBACK_ENVIRONMENT environment variable. <configuration> <appender name= \"SELECT\" class= \"com.tersesystems.logback.SelectAppender\" > <appenderKey> ${LOGBACK_ENVIRONMENT} </appenderKey> <appender class= \"com.tersesystems.logback.CompositeAppender\" > <name> test </name> <appender class= \"ch.qos.logback.core.read.ListAppender\" > <name> test-list </name> </appender> </appender> <appender class= \"com.tersesystems.logback.CompositeAppender\" > <name> development </name> <appender class= \"ch.qos.logback.core.ConsoleAppender\" > <name> development-console </name> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> </appender> <appender class= \"com.tersesystems.logback.CompositeAppender\" > <name> staging </name> <appender class= \"ch.qos.logback.core.ConsoleAppender\" > <name> staging-console </name> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <appender class= \"ch.qos.logback.core.FileAppender\" > <name> staging-file </name> <file> file.log </file> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> </appender> </appender> <root level= \"TRACE\" > <appender-ref ref= \"SELECT\" /> </root> </configuration> This is a much cleaner way to organize appenders than putting Janino logic into the configuration.","title":"Select Appender"},{"location":"guide/slf4jbridge/","text":"JUL to SLF4J Bridge \u00b6 It's easy to assume that all Java libraries will depend on SLF4J. But one of the oddities of Java logging is that there's a built-in logging framework called java.util.logging (JUL) which is rarely used but does appear in libraries such as Guice , GRPC , and Guava . When errors happen in these frameworks, they may never show up in logging at all, because JUL will write out to standard output and standard error by default. SLF4JBridgeHandler is a logging bridge, which is available in jul-to-slf4j . It does the job, but it does require some custom code to be added on startup to tell JUL that the handler is SLF4J: SLF4JBridgeHandler . removeHandlersForRootLogger (); SLF4JBridgeHandler . install (); This isn't ideal, as it's very easy to miss that you have to add these lines of code. Some frameworks such as Play Framework are smart enough are smart enough to handle this for you, but there are cases where you're not using those frameworks, and we'd like JUL to just work. This isn't so easy. JUL is very basic, and accepts configuration from system properties. The LogManager has two system properties: java.util.logging.config.class java.util.logging.config.file If it doesn't find either, then it looks in ${java.home}/conf/logging.properties if you're on JDK 11. There's no way to configure it from classpath, you have to do that by hand . There is discussion on Stack Overflow and the SLF4J mailing list suggesting that JUL looks for logging.properties in the classpath. This is incorrect -- the only way you'll see logging.properties is from setting java.util.logging.config.file or if you're overwriting ${java.home}/logging.properties . Here's the source code so you can check for yourself. However, since we're using Logback, we can leverage the fact that Logback searches through the classpath for logback.xml . All we need is a custom action to wrap SLF4JBridgeHandler and we can have a code free solution. This is what SLF4JBridgeHandlerAction does. You should also configure the LevelChangePropagator , to reduce the impact of logging , and you must make sure that the LoggerFactory is called before any JUL dependent code. You should set your logback.xml roughly as follows: <configuration> <!-- set up the rule --> <newRule pattern= \"configuration/slf4jBridgeHandler\" actionClass= \"com.tersesystems.logback.classic.SLF4JBridgeHandlerAction\" /> <!-- calls removeHandlersForRootLogger / install --> <slf4jBridgeHandler/> <!-- reset all previous level configurations of all j.u.l. loggers --> <contextListener class= \"ch.qos.logback.classic.jul.LevelChangePropagator\" > <resetJUL> true </resetJUL> </contextListener> <!-- Add Guice tracing --> <logger name= \"com.google.inject\" level= \"TRACE\" /> <root level= \"INFO\" > <appender class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %date{H:mm:ss.SSS} [%highlight(%-5level)] %logger - %message%ex%n </pattern> </encoder> </appender> </root> </configuration> As of 0.17.0, logback-classic has a dependency on jul-to-slf4j so the following will work in build.gradle : dependencies { implementation \"com.google.inject:guice:5.0.1\" implementation 'com.tersesystems.logback:logback-classic:0.17.0' } And then you should call org.slf4j.LoggerFactory.getLogger as a static final to prevent any initialization problems: package example ; import com.google.inject.* ; import org.slf4j.* ; public class App { // Ensure that logback.xml is parsed by LoggerFactory _before_ Guice calls JUL. private static final Logger logger = org . slf4j . LoggerFactory . getLogger ( App . class ); public String getGreeting () { return \"Hello World!\" ; } public static void main ( String [] args ) { final Injector injector = Guice . createInjector (); final App instance = injector . getInstance ( App . class ); logger . info ( instance . getGreeting ()); } } And that should render the following: 19:51:45.493 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Module execution: 64ms 19:51:45.494 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Interceptors creation: 2ms 19:51:45.496 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - TypeListeners & ProvisionListener creation: 1ms 19:51:45.511 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Scopes creation: 15ms 19:51:45.511 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Converters creation: 0ms 19:51:45.514 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding creation: 2ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Module annotated method scanners creation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Private environment creation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Injector construction: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding initialization: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding indexing: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Collecting injection requests: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding validation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Static validation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Instance member validation: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Provider verification: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Delayed Binding initialization: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Static member injection: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Instance injection: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Preloading singletons: 0ms 19:51:45.545 [INFO ] example.App - Hello World!","title":"JUL to SLF4J Bridge"},{"location":"guide/slf4jbridge/#jul-to-slf4j-bridge","text":"It's easy to assume that all Java libraries will depend on SLF4J. But one of the oddities of Java logging is that there's a built-in logging framework called java.util.logging (JUL) which is rarely used but does appear in libraries such as Guice , GRPC , and Guava . When errors happen in these frameworks, they may never show up in logging at all, because JUL will write out to standard output and standard error by default. SLF4JBridgeHandler is a logging bridge, which is available in jul-to-slf4j . It does the job, but it does require some custom code to be added on startup to tell JUL that the handler is SLF4J: SLF4JBridgeHandler . removeHandlersForRootLogger (); SLF4JBridgeHandler . install (); This isn't ideal, as it's very easy to miss that you have to add these lines of code. Some frameworks such as Play Framework are smart enough are smart enough to handle this for you, but there are cases where you're not using those frameworks, and we'd like JUL to just work. This isn't so easy. JUL is very basic, and accepts configuration from system properties. The LogManager has two system properties: java.util.logging.config.class java.util.logging.config.file If it doesn't find either, then it looks in ${java.home}/conf/logging.properties if you're on JDK 11. There's no way to configure it from classpath, you have to do that by hand . There is discussion on Stack Overflow and the SLF4J mailing list suggesting that JUL looks for logging.properties in the classpath. This is incorrect -- the only way you'll see logging.properties is from setting java.util.logging.config.file or if you're overwriting ${java.home}/logging.properties . Here's the source code so you can check for yourself. However, since we're using Logback, we can leverage the fact that Logback searches through the classpath for logback.xml . All we need is a custom action to wrap SLF4JBridgeHandler and we can have a code free solution. This is what SLF4JBridgeHandlerAction does. You should also configure the LevelChangePropagator , to reduce the impact of logging , and you must make sure that the LoggerFactory is called before any JUL dependent code. You should set your logback.xml roughly as follows: <configuration> <!-- set up the rule --> <newRule pattern= \"configuration/slf4jBridgeHandler\" actionClass= \"com.tersesystems.logback.classic.SLF4JBridgeHandlerAction\" /> <!-- calls removeHandlersForRootLogger / install --> <slf4jBridgeHandler/> <!-- reset all previous level configurations of all j.u.l. loggers --> <contextListener class= \"ch.qos.logback.classic.jul.LevelChangePropagator\" > <resetJUL> true </resetJUL> </contextListener> <!-- Add Guice tracing --> <logger name= \"com.google.inject\" level= \"TRACE\" /> <root level= \"INFO\" > <appender class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %date{H:mm:ss.SSS} [%highlight(%-5level)] %logger - %message%ex%n </pattern> </encoder> </appender> </root> </configuration> As of 0.17.0, logback-classic has a dependency on jul-to-slf4j so the following will work in build.gradle : dependencies { implementation \"com.google.inject:guice:5.0.1\" implementation 'com.tersesystems.logback:logback-classic:0.17.0' } And then you should call org.slf4j.LoggerFactory.getLogger as a static final to prevent any initialization problems: package example ; import com.google.inject.* ; import org.slf4j.* ; public class App { // Ensure that logback.xml is parsed by LoggerFactory _before_ Guice calls JUL. private static final Logger logger = org . slf4j . LoggerFactory . getLogger ( App . class ); public String getGreeting () { return \"Hello World!\" ; } public static void main ( String [] args ) { final Injector injector = Guice . createInjector (); final App instance = injector . getInstance ( App . class ); logger . info ( instance . getGreeting ()); } } And that should render the following: 19:51:45.493 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Module execution: 64ms 19:51:45.494 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Interceptors creation: 2ms 19:51:45.496 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - TypeListeners & ProvisionListener creation: 1ms 19:51:45.511 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Scopes creation: 15ms 19:51:45.511 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Converters creation: 0ms 19:51:45.514 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding creation: 2ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Module annotated method scanners creation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Private environment creation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Injector construction: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding initialization: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding indexing: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Collecting injection requests: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding validation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Static validation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Instance member validation: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Provider verification: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Delayed Binding initialization: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Static member injection: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Instance injection: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Preloading singletons: 0ms 19:51:45.545 [INFO ] example.App - Hello World!","title":"JUL to SLF4J Bridge"},{"location":"guide/structured-config/","text":"Structured Config \u00b6 This section deals with the specific configuration in terse-logback/logback-structured-config . Logback doesn't come with a default logback.xml file, and the configuration page is written at a very low level that is not very useful for people. The example has been written so that it doesn't \"overwhelm\" with too much detail, but in rough order of initialization: Logback XML with Custom Actions Loading Typesafe Config Log Levels and Properties through Typesafe Config High Performance Async Appenders Sensible Joran (Logback XML) Configuration Logback XML with Custom Actions \u00b6 The entry point of the system is a logback.xml file which has custom actions added to it to do additional configuration, TypesafeConfigAction and SetLoggerLevelsAction . This approach is not as fancy as using a service loader pattern, but there are issues integrating into web frameworks, as those frameworks may look directly for XML files and skip service loader patterns. Using a logback.xml file is the most well known pattern, and Joran makes adding custom actions fairly easy. High Performance Async Appenders \u00b6 The JSON and Text file appenders are wrapped in LMAX Disruptor async appenders . This example comes preconfigured with a shutdown hook to ensure the async appenders empty their queues before the application shuts down. To my knowledge, the logstash async appenders have not been benchmarked against Log4J2, but async logging is ridiculously performant, and will never be the bottleneck in your application . In general, you should only be concerned about the latency or throughput of your logging framework when you have sat down and done the math on how much logging it would take to stress out the system, asked about your operational requirements, and determined the operational costs, including IO and rate limits , and a budget for logging. Logging doesn't come for free. Sensible Joran (Logback XML) Configuration \u00b6 The XML configuration for the main file is in terse-logback.xml and is as follows: The UniqueIdEventAppender is an appender that decorates ILoggingEvent with a unique id that can be used to correlate the same log entry across different appenders. <configuration> <include resource= \"terse-logback/initial.xml\" /> <include resource= \"terse-logback/censor.xml\" /> <include resource= \"terse-logback/appenders/audio-appenders.xml\" /> <include resource= \"terse-logback/appenders/console-appenders.xml\" /> <include resource= \"terse-logback/appenders/jsonfile-appenders.xml\" /> <include resource= \"terse-logback/appenders/textfile-appenders.xml\" /> <appender name= \"development\" class= \"com.tersesystems.logback.core.CompositeAppender\" > <appender-ref ref= \"CONSOLE\" /> <appender-ref ref= \"AUDIO\" /> <appender-ref ref= \"ASYNC_TEXTFILE\" /> <appender-ref ref= \"ASYNC_JSONFILE\" /> </appender> <appender name= \"test\" class= \"com.tersesystems.logback.core.CompositeAppender\" > <appender-ref ref= \"ASYNC_TEXTFILE\" /> </appender> <appender name= \"production\" class= \"com.tersesystems.logback.core.CompositeAppender\" > <appender-ref ref= \"CONSOLE\" /> <appender-ref ref= \"ASYNC_JSONFILE\" /> </appender> <appender name= \"selector\" class= \"com.tersesystems.logback.core.SelectAppender\" > <!-- Set logback.conf or logback-test.conf with \"local.logback.environment=development\" --> <appenderKey> ${logback.environment} </appenderKey> <appender-ref ref= \"development\" /> <appender-ref ref= \"production\" /> <appender-ref ref= \"test\" /> </appender> <appender name= \"selector-with-unique-id\" class= \"com.tersesystems.logback.uniqueid.UniqueIdComponentAppender\" > <appender-ref ref= \"selector\" /> </appender> <root> <appender-ref ref= \"selector-with-unique-id\" /> </root> <include resource= \"terse-logback/ending.xml\" /> </configuration> All the encoders have been configured to use UTC as the timezone, and are packaged individually using file inclusion for ease of use. Console \u00b6 The console appender uses the following XML configuration: <included> <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.ConsoleAppender\" > <filter class= \"com.tersesystems.logback.EnabledFilter\" > <enabled> ${console.enabled} </enabled> </filter> <encoder> <pattern> ${console.encoder.pattern} </pattern> </encoder> <withJansi> ${console.withJansi} </withJansi> </appender> </included> with the HOCON settings as follows: console { enabled = true withJansi = true # allow colored logging on windows encoder { pattern = \"[%terseHighlight(%-5level)] %logger{15} - %censor(%message){text-censor}%n%xException{10}\" } } The console appender uses colored logging for the log level, but you can override config to set the colors you want for which levels. Jansi is included so that Windows can benefit from colored logging as well. The console does not use async logging, because it has to co-exist with System.out.println and System.err.println messages, and so must appear time-ordered with them. Text \u00b6 The text encoder uses the following configuration: <included> <appender name= \"TEXTFILE\" class= \"ch.qos.logback.core.FileAppender\" > <filter class= \"com.tersesystems.logback.EnabledFilter\" > <enabled> ${textfile.enabled} </enabled> </filter> <file> ${textfile.location} </file> <append> ${textfile.append} </append> <!-- This quadruples logging throughput (in theory) https://logback.qos.ch/manual/appenders.html#FileAppender --> <immediateFlush> ${textfile.immediateFlush} </immediateFlush> <encoder> <pattern> ${textfile.encoder.pattern} </pattern> <outputPatternAsHeader> ${textfile.encoder.outputPatternAsHeader} </outputPatternAsHeader> </encoder> </appender> <!-- https://github.com/logstash/logstash-logback-encoder/tree/logstash-logback-encoder-5.2#async-appenders --> <appender name= \"ASYNCTEXTFILE\" class= \"net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender\" > <appender-ref ref= \"TEXTFILE\" /> </appender> </included> with the HOCON settings as: // used in textfile-appenders.xml textfile { enabled = true location = ${properties.log.dir}/application.log append = true immediateFlush = true rollingPolicy { fileNamePattern = ${properties.log.dir}\"/application.log.%d{yyyy-MM-dd}\" maxHistory = 30 } encoder { outputPatternAsHeader = true // https://github.com/logstash/logstash-logback-encoder/blob/master/src/main/java/net/logstash/logback/stacktrace/ShortenedThrowableConverter.java#L58 // Options can be specified in the pattern in the following order: // - maxDepthPerThrowable = \"full\" or \"short\" or an integer value // - shortenedClassNameLength = \"full\" or \"short\" or an integer value // - maxLength = \"full\" or \"short\" or an integer value // //%msg%n%stack{5,1024,10,rootFirst,regex1,regex2,evaluatorName} pattern = \"%date{yyyy-MM-dd'T'HH:mm:ss.SSSZZ,UTC} [%-5level] %logger in %thread - %censor(%message){text-censor}%n%stack{full,full,short,rootFirst}\" } } Colored logging is not used in the file-based appender, because some editors tend to show ANSI codes specifically. JSON \u00b6 The JSON encoder uses net.logstash.logback.encoder.LogstashEncoder with pretty print options. The XML is as follows: <included> <appender name= \"ASYNC_JSONFILE\" class= \"net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender\" > <filter class= \"com.tersesystems.logback.core.EnabledFilter\" > <enabled> ${jsonfile.enabled} </enabled> </filter> <appender class= \"ch.qos.logback.core.rolling.RollingFileAppender\" > <file> ${jsonfile.location} </file> <append> ${jsonfile.append} </append> <!-- This quadruples logging throughput (in theory) https://logback.qos.ch/manual/appenders.html#FileAppender --> <immediateFlush> ${jsonfile.immediateFlush} </immediateFlush> <rollingPolicy class= \"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\" > <fileNamePattern> ${jsonfile.rollingPolicy.fileNamePattern} </fileNamePattern> <maxHistory> ${jsonfile.rollingPolicy.maxHistory} </maxHistory> </rollingPolicy> <encoder class= \"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\" > <providers> <pattern> <pattern> { \"id\": \"%uniqueId\" } </pattern> </pattern> <sequence/> <timestamp> <!-- UTC is the best server consistent timezone --> <timeZone> ${jsonfile.encoder.timeZone} </timeZone> <timestampPattern> ${jsonfile.encoder.timestampPattern} </timestampPattern> </timestamp> <version/> <message/> <loggerName/> <threadName/> <logLevel/> <stackHash/> <mdc/> <logstashMarkers/> <arguments/> <provider class= \"com.tersesystems.logback.exceptionmapping.json.ExceptionArgumentsProvider\" > <fieldName> exception </fieldName> </provider> <stackTrace> <!-- https://github.com/logstash/logstash-logback-encoder#customizing-stack-traces --> <throwableConverter class= \"net.logstash.logback.stacktrace.ShortenedThrowableConverter\" > <rootCauseFirst> ${jsonfile.shortenedThrowableConverter.rootCauseFirst} </rootCauseFirst> <inlineHash> ${jsonfile.shortenedThrowableConverter.inlineHash} </inlineHash> </throwableConverter> </stackTrace> </providers> <!-- https://github.com/logstash/logstash-logback-encoder/tree/logstash-logback-encoder-5.2#customizing-json-factory-and-generator --> <!-- XXX it would be much nicer to use OGNL rather than Janino, but out of scope... --> <if condition= 'p(\"jsonfile.prettyprint\").contains(\"true\")' > <then> <!-- Pretty print for better end user experience. --> <jsonGeneratorDecorator class= \"com.tersesystems.logback.censor.CensoringPrettyPrintingJsonGeneratorDecorator\" > <censor-ref ref= \"json-censor\" /> </jsonGeneratorDecorator> </then> <else> <jsonGeneratorDecorator class= \"com.tersesystems.logback.censor.CensoringJsonGeneratorDecorator\" > <censor-ref ref= \"json-censor\" /> </jsonGeneratorDecorator> </else> </if> </encoder> </appender> </appender> </included> with the following HOCON configuration: // Used in jsonfile-appenders.xml jsonfile { enabled = true location = ${properties.log.dir}\"/application.json\" append = true immediateFlush = true prettyprint = false rollingPolicy { fileNamePattern = ${properties.log.dir}\"/application.json.%d{yyyy-MM-dd}\" maxHistory = 30 } encoder { includeContext = false timeZone = \"UTC\" } # https://github.com/logstash/logstash-logback-encoder#customizing-stack-traces shortenedThrowableConverter { maxDepthPerThrowable = 100 maxLength = 100 shortenedClassNameLength = 50 exclusions = \"\"\"\\$\\$FastClassByCGLIB\\$\\$,\\$\\$EnhancerBySpringCGLIB\\$\\$,^sun\\.reflect\\..*\\.invoke,^com\\.sun\\.,^sun\\.net\\.,^net\\.sf\\.cglib\\.proxy\\.MethodProxy\\.invoke,^org\\.springframework\\.cglib\\.,^org\\.springframework\\.transaction\\.,^org\\.springframework\\.validation\\.,^org\\.springframework\\.app\\.,^org\\.springframework\\.aop\\.,^java\\.lang\\.reflect\\.Method\\.invoke,^org\\.springframework\\.ws\\..*\\.invoke,^org\\.springframework\\.ws\\.transport\\.,^org\\.springframework\\.ws\\.soap\\.saaj\\.SaajSoapMessage\\.,^org\\.springframework\\.ws\\.client\\.core\\.WebServiceTemplate\\.,^org\\.springframework\\.web\\.filter\\.,^org\\.apache\\.tomcat\\.,^org\\.apache\\.catalina\\.,^org\\.apache\\.coyote\\.,^java\\.util\\.concurrent\\.ThreadPoolExecutor\\.runWorker,^java\\.lang\\.Thread\\.run$\"\"\" rootCauseFirst = true inlineHash = true } } If you want to modify the format of the JSON encoder, you should use LoggingEventCompositeJsonEncoder . The level of detail in LoggingEventCompositeJsonEncoder is truly astounding and it's a powerful piece of work in its own right. See Application Logging in Java: Putting it all together for more details.","title":"Structured Config"},{"location":"guide/structured-config/#structured-config","text":"This section deals with the specific configuration in terse-logback/logback-structured-config . Logback doesn't come with a default logback.xml file, and the configuration page is written at a very low level that is not very useful for people. The example has been written so that it doesn't \"overwhelm\" with too much detail, but in rough order of initialization: Logback XML with Custom Actions Loading Typesafe Config Log Levels and Properties through Typesafe Config High Performance Async Appenders Sensible Joran (Logback XML) Configuration","title":"Structured Config"},{"location":"guide/structured-config/#logback-xml-with-custom-actions","text":"The entry point of the system is a logback.xml file which has custom actions added to it to do additional configuration, TypesafeConfigAction and SetLoggerLevelsAction . This approach is not as fancy as using a service loader pattern, but there are issues integrating into web frameworks, as those frameworks may look directly for XML files and skip service loader patterns. Using a logback.xml file is the most well known pattern, and Joran makes adding custom actions fairly easy.","title":"Logback XML with Custom Actions"},{"location":"guide/structured-config/#high-performance-async-appenders","text":"The JSON and Text file appenders are wrapped in LMAX Disruptor async appenders . This example comes preconfigured with a shutdown hook to ensure the async appenders empty their queues before the application shuts down. To my knowledge, the logstash async appenders have not been benchmarked against Log4J2, but async logging is ridiculously performant, and will never be the bottleneck in your application . In general, you should only be concerned about the latency or throughput of your logging framework when you have sat down and done the math on how much logging it would take to stress out the system, asked about your operational requirements, and determined the operational costs, including IO and rate limits , and a budget for logging. Logging doesn't come for free.","title":"High Performance Async Appenders"},{"location":"guide/structured-config/#sensible-joran-logback-xml-configuration","text":"The XML configuration for the main file is in terse-logback.xml and is as follows: The UniqueIdEventAppender is an appender that decorates ILoggingEvent with a unique id that can be used to correlate the same log entry across different appenders. <configuration> <include resource= \"terse-logback/initial.xml\" /> <include resource= \"terse-logback/censor.xml\" /> <include resource= \"terse-logback/appenders/audio-appenders.xml\" /> <include resource= \"terse-logback/appenders/console-appenders.xml\" /> <include resource= \"terse-logback/appenders/jsonfile-appenders.xml\" /> <include resource= \"terse-logback/appenders/textfile-appenders.xml\" /> <appender name= \"development\" class= \"com.tersesystems.logback.core.CompositeAppender\" > <appender-ref ref= \"CONSOLE\" /> <appender-ref ref= \"AUDIO\" /> <appender-ref ref= \"ASYNC_TEXTFILE\" /> <appender-ref ref= \"ASYNC_JSONFILE\" /> </appender> <appender name= \"test\" class= \"com.tersesystems.logback.core.CompositeAppender\" > <appender-ref ref= \"ASYNC_TEXTFILE\" /> </appender> <appender name= \"production\" class= \"com.tersesystems.logback.core.CompositeAppender\" > <appender-ref ref= \"CONSOLE\" /> <appender-ref ref= \"ASYNC_JSONFILE\" /> </appender> <appender name= \"selector\" class= \"com.tersesystems.logback.core.SelectAppender\" > <!-- Set logback.conf or logback-test.conf with \"local.logback.environment=development\" --> <appenderKey> ${logback.environment} </appenderKey> <appender-ref ref= \"development\" /> <appender-ref ref= \"production\" /> <appender-ref ref= \"test\" /> </appender> <appender name= \"selector-with-unique-id\" class= \"com.tersesystems.logback.uniqueid.UniqueIdComponentAppender\" > <appender-ref ref= \"selector\" /> </appender> <root> <appender-ref ref= \"selector-with-unique-id\" /> </root> <include resource= \"terse-logback/ending.xml\" /> </configuration> All the encoders have been configured to use UTC as the timezone, and are packaged individually using file inclusion for ease of use.","title":"Sensible Joran (Logback XML) Configuration"},{"location":"guide/structured-config/#console","text":"The console appender uses the following XML configuration: <included> <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.ConsoleAppender\" > <filter class= \"com.tersesystems.logback.EnabledFilter\" > <enabled> ${console.enabled} </enabled> </filter> <encoder> <pattern> ${console.encoder.pattern} </pattern> </encoder> <withJansi> ${console.withJansi} </withJansi> </appender> </included> with the HOCON settings as follows: console { enabled = true withJansi = true # allow colored logging on windows encoder { pattern = \"[%terseHighlight(%-5level)] %logger{15} - %censor(%message){text-censor}%n%xException{10}\" } } The console appender uses colored logging for the log level, but you can override config to set the colors you want for which levels. Jansi is included so that Windows can benefit from colored logging as well. The console does not use async logging, because it has to co-exist with System.out.println and System.err.println messages, and so must appear time-ordered with them.","title":"Console"},{"location":"guide/structured-config/#text","text":"The text encoder uses the following configuration: <included> <appender name= \"TEXTFILE\" class= \"ch.qos.logback.core.FileAppender\" > <filter class= \"com.tersesystems.logback.EnabledFilter\" > <enabled> ${textfile.enabled} </enabled> </filter> <file> ${textfile.location} </file> <append> ${textfile.append} </append> <!-- This quadruples logging throughput (in theory) https://logback.qos.ch/manual/appenders.html#FileAppender --> <immediateFlush> ${textfile.immediateFlush} </immediateFlush> <encoder> <pattern> ${textfile.encoder.pattern} </pattern> <outputPatternAsHeader> ${textfile.encoder.outputPatternAsHeader} </outputPatternAsHeader> </encoder> </appender> <!-- https://github.com/logstash/logstash-logback-encoder/tree/logstash-logback-encoder-5.2#async-appenders --> <appender name= \"ASYNCTEXTFILE\" class= \"net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender\" > <appender-ref ref= \"TEXTFILE\" /> </appender> </included> with the HOCON settings as: // used in textfile-appenders.xml textfile { enabled = true location = ${properties.log.dir}/application.log append = true immediateFlush = true rollingPolicy { fileNamePattern = ${properties.log.dir}\"/application.log.%d{yyyy-MM-dd}\" maxHistory = 30 } encoder { outputPatternAsHeader = true // https://github.com/logstash/logstash-logback-encoder/blob/master/src/main/java/net/logstash/logback/stacktrace/ShortenedThrowableConverter.java#L58 // Options can be specified in the pattern in the following order: // - maxDepthPerThrowable = \"full\" or \"short\" or an integer value // - shortenedClassNameLength = \"full\" or \"short\" or an integer value // - maxLength = \"full\" or \"short\" or an integer value // //%msg%n%stack{5,1024,10,rootFirst,regex1,regex2,evaluatorName} pattern = \"%date{yyyy-MM-dd'T'HH:mm:ss.SSSZZ,UTC} [%-5level] %logger in %thread - %censor(%message){text-censor}%n%stack{full,full,short,rootFirst}\" } } Colored logging is not used in the file-based appender, because some editors tend to show ANSI codes specifically.","title":"Text"},{"location":"guide/structured-config/#json","text":"The JSON encoder uses net.logstash.logback.encoder.LogstashEncoder with pretty print options. The XML is as follows: <included> <appender name= \"ASYNC_JSONFILE\" class= \"net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender\" > <filter class= \"com.tersesystems.logback.core.EnabledFilter\" > <enabled> ${jsonfile.enabled} </enabled> </filter> <appender class= \"ch.qos.logback.core.rolling.RollingFileAppender\" > <file> ${jsonfile.location} </file> <append> ${jsonfile.append} </append> <!-- This quadruples logging throughput (in theory) https://logback.qos.ch/manual/appenders.html#FileAppender --> <immediateFlush> ${jsonfile.immediateFlush} </immediateFlush> <rollingPolicy class= \"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\" > <fileNamePattern> ${jsonfile.rollingPolicy.fileNamePattern} </fileNamePattern> <maxHistory> ${jsonfile.rollingPolicy.maxHistory} </maxHistory> </rollingPolicy> <encoder class= \"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\" > <providers> <pattern> <pattern> { \"id\": \"%uniqueId\" } </pattern> </pattern> <sequence/> <timestamp> <!-- UTC is the best server consistent timezone --> <timeZone> ${jsonfile.encoder.timeZone} </timeZone> <timestampPattern> ${jsonfile.encoder.timestampPattern} </timestampPattern> </timestamp> <version/> <message/> <loggerName/> <threadName/> <logLevel/> <stackHash/> <mdc/> <logstashMarkers/> <arguments/> <provider class= \"com.tersesystems.logback.exceptionmapping.json.ExceptionArgumentsProvider\" > <fieldName> exception </fieldName> </provider> <stackTrace> <!-- https://github.com/logstash/logstash-logback-encoder#customizing-stack-traces --> <throwableConverter class= \"net.logstash.logback.stacktrace.ShortenedThrowableConverter\" > <rootCauseFirst> ${jsonfile.shortenedThrowableConverter.rootCauseFirst} </rootCauseFirst> <inlineHash> ${jsonfile.shortenedThrowableConverter.inlineHash} </inlineHash> </throwableConverter> </stackTrace> </providers> <!-- https://github.com/logstash/logstash-logback-encoder/tree/logstash-logback-encoder-5.2#customizing-json-factory-and-generator --> <!-- XXX it would be much nicer to use OGNL rather than Janino, but out of scope... --> <if condition= 'p(\"jsonfile.prettyprint\").contains(\"true\")' > <then> <!-- Pretty print for better end user experience. --> <jsonGeneratorDecorator class= \"com.tersesystems.logback.censor.CensoringPrettyPrintingJsonGeneratorDecorator\" > <censor-ref ref= \"json-censor\" /> </jsonGeneratorDecorator> </then> <else> <jsonGeneratorDecorator class= \"com.tersesystems.logback.censor.CensoringJsonGeneratorDecorator\" > <censor-ref ref= \"json-censor\" /> </jsonGeneratorDecorator> </else> </if> </encoder> </appender> </appender> </included> with the following HOCON configuration: // Used in jsonfile-appenders.xml jsonfile { enabled = true location = ${properties.log.dir}\"/application.json\" append = true immediateFlush = true prettyprint = false rollingPolicy { fileNamePattern = ${properties.log.dir}\"/application.json.%d{yyyy-MM-dd}\" maxHistory = 30 } encoder { includeContext = false timeZone = \"UTC\" } # https://github.com/logstash/logstash-logback-encoder#customizing-stack-traces shortenedThrowableConverter { maxDepthPerThrowable = 100 maxLength = 100 shortenedClassNameLength = 50 exclusions = \"\"\"\\$\\$FastClassByCGLIB\\$\\$,\\$\\$EnhancerBySpringCGLIB\\$\\$,^sun\\.reflect\\..*\\.invoke,^com\\.sun\\.,^sun\\.net\\.,^net\\.sf\\.cglib\\.proxy\\.MethodProxy\\.invoke,^org\\.springframework\\.cglib\\.,^org\\.springframework\\.transaction\\.,^org\\.springframework\\.validation\\.,^org\\.springframework\\.app\\.,^org\\.springframework\\.aop\\.,^java\\.lang\\.reflect\\.Method\\.invoke,^org\\.springframework\\.ws\\..*\\.invoke,^org\\.springframework\\.ws\\.transport\\.,^org\\.springframework\\.ws\\.soap\\.saaj\\.SaajSoapMessage\\.,^org\\.springframework\\.ws\\.client\\.core\\.WebServiceTemplate\\.,^org\\.springframework\\.web\\.filter\\.,^org\\.apache\\.tomcat\\.,^org\\.apache\\.catalina\\.,^org\\.apache\\.coyote\\.,^java\\.util\\.concurrent\\.ThreadPoolExecutor\\.runWorker,^java\\.lang\\.Thread\\.run$\"\"\" rootCauseFirst = true inlineHash = true } } If you want to modify the format of the JSON encoder, you should use LoggingEventCompositeJsonEncoder . The level of detail in LoggingEventCompositeJsonEncoder is truly astounding and it's a powerful piece of work in its own right. See Application Logging in Java: Putting it all together for more details.","title":"JSON"},{"location":"guide/systeminfo/","text":"System Information \u00b6 In What is Happening: Attempting to Understand Our Systems , there's a slide that suggests the following information should always be available as telemetry data: The user (and/or company), time, machine stats (CPU, Memory, etc), software version, configuration data, the calling request, any dependent requests The interesting bit here is the machine stats, such as CPU and memory, and how they relate to Logback. Machine status can be very relevant when it comes to resource failures, and providing a detailed view of CPU and memory tied to logs is an interesting concept. There's a tool, Hyperic Sigar , which is very good at exposing system metrics. Using the logback-sigar module, it's relatively easy to add Sigar into context using com.tersesystems.logback.sigar.SigarAction : <configuration> <newRule pattern= \"*/sigar\" actionClass= \"com.tersesystems.logback.sigar.SigarAction\" /> <sigar/> <conversionRule conversionWord= \"cpu\" converterClass= \"com.tersesystems.logback.sigar.CPUPercentageConverter\" /> <conversionRule conversionWord= \"mem\" converterClass= \"com.tersesystems.logback.sigar.MemoryPercentageConverter\" /> <conversionRule conversionWord= \"loadavg\" converterClass= \"com.tersesystems.logback.sigar.LoadAverageConverter\" /> <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> [%-5level] %logger{15} %cpu %mem %loadavg - %msg%n%xException{10} </pattern> </encoder> </appender> <root level= \"INFO\" > <appender-ref ref= \"CONSOLE\" /> </root> </configuration> And then render the CPU, memory and load average as follows: [ERROR] c.example.Test sys=0.007594936708860759 user=0.10379746835443038 used=9269886976 used%=24.923867415973078 total=25064484864 load1min=2.18 load5min=1.5 load15min=1.07 - I am very much under load Note that if you want to integrate this with Logstash StructuredArgument or Markers then you'll want to make your component implement SigarContextAware and then query appropriately. There are some very fun things you can do with Sigar like add Process Table Query Language together with some feature flag stuff to do dynamic queries into the machine.","title":"System Info"},{"location":"guide/systeminfo/#system-information","text":"In What is Happening: Attempting to Understand Our Systems , there's a slide that suggests the following information should always be available as telemetry data: The user (and/or company), time, machine stats (CPU, Memory, etc), software version, configuration data, the calling request, any dependent requests The interesting bit here is the machine stats, such as CPU and memory, and how they relate to Logback. Machine status can be very relevant when it comes to resource failures, and providing a detailed view of CPU and memory tied to logs is an interesting concept. There's a tool, Hyperic Sigar , which is very good at exposing system metrics. Using the logback-sigar module, it's relatively easy to add Sigar into context using com.tersesystems.logback.sigar.SigarAction : <configuration> <newRule pattern= \"*/sigar\" actionClass= \"com.tersesystems.logback.sigar.SigarAction\" /> <sigar/> <conversionRule conversionWord= \"cpu\" converterClass= \"com.tersesystems.logback.sigar.CPUPercentageConverter\" /> <conversionRule conversionWord= \"mem\" converterClass= \"com.tersesystems.logback.sigar.MemoryPercentageConverter\" /> <conversionRule conversionWord= \"loadavg\" converterClass= \"com.tersesystems.logback.sigar.LoadAverageConverter\" /> <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> [%-5level] %logger{15} %cpu %mem %loadavg - %msg%n%xException{10} </pattern> </encoder> </appender> <root level= \"INFO\" > <appender-ref ref= \"CONSOLE\" /> </root> </configuration> And then render the CPU, memory and load average as follows: [ERROR] c.example.Test sys=0.007594936708860759 user=0.10379746835443038 used=9269886976 used%=24.923867415973078 total=25064484864 load1min=2.18 load5min=1.5 load15min=1.07 - I am very much under load Note that if you want to integrate this with Logstash StructuredArgument or Markers then you'll want to make your component implement SigarContextAware and then query appropriately. There are some very fun things you can do with Sigar like add Process Table Query Language together with some feature flag stuff to do dynamic queries into the machine.","title":"System Information"},{"location":"guide/tapfilter/","text":"Tap Filter \u00b6 A tap filter is used to tap some amount of incoming process and pass them to a specially configured appender even if they do not qualify as a logging event under normal circumstances. This is a wiretap pattern from Enterprise Integration Patterns. Tap Filters are very useful as a way to send data to an appender. They completely bypass any kind of logging level configured on the front end, so you can set a logger to INFO level but still have access to all TRACE events when an error occurs, through the tap filter's appenders. For example, a tap filter can automatically log everything with a correlation id at a TRACE level, without requiring filters or altering the log level as a whole. Let's run a simple HTTP client program that calls out to Google and prints a result. package playwsclient ; import akka.actor.ActorSystem ; import akka.stream.Materializer ; import akka.stream.SystemMaterializer ; import com.typesafe.config.Config ; import com.typesafe.config.ConfigFactory ; import com.zaxxer.hikari.HikariConfig ; import com.zaxxer.hikari.HikariDataSource ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import org.slf4j.MDC ; import play.libs.ws.* ; import play.libs.ws.ahc.* ; import java.sql.* ; import java.util.ArrayList ; import java.util.List ; import java.util.Properties ; public class JavaClient implements DefaultBodyReadables { private final StandaloneAhcWSClient client ; private final ActorSystem system ; HikariDataSource createDataSource ( Config config ) { Config jdbcConfig = config . getConfig ( \"logback.jdbc\" ); String driver = jdbcConfig . getString ( \"driver\" ); String url = jdbcConfig . getString ( \"url\" ); String user = jdbcConfig . getString ( \"username\" ); String password = jdbcConfig . getString ( \"password\" ); return createDataSource ( driver , url , user , password ); } protected HikariDataSource createDataSource ( String driver , String url , String username , String password ) { HikariConfig config = new HikariConfig (); config . setDriverClassName ( driver ); config . setJdbcUrl ( url ); config . setUsername ( username ); config . setPassword ( password ); config . setPoolName ( \"client-pool\" ); config . setMaximumPoolSize ( 1 ); Properties props = new Properties (); // props.put(\"dataSource.logWriter\", new PrintWriter(System.out)); config . setDataSourceProperties ( props ); return new HikariDataSource ( config ); } public static void main ( String [] args ) { // Set up Akka materializer to handle streaming final String name = \"wsclient\" ; ActorSystem system = ActorSystem . create ( name ); system . registerOnTermination (() -> System . exit ( 0 )); Materializer materializer = SystemMaterializer . get ( system ). materializer (); // Create the WS client from the `application.conf` file, the current classloader and materializer. StandaloneAhcWSClient ws = StandaloneAhcWSClient . create ( AhcWSClientConfigFactory . forConfig ( ConfigFactory . load (), system . getClass (). getClassLoader ()), materializer ); JavaClient javaClient = new JavaClient ( system , ws ); javaClient . run (); } JavaClient ( ActorSystem system , StandaloneAhcWSClient client ) { this . system = system ; this . client = client ; } public void run () { String correlationId = \"12345\" ; MDC . put ( \"correlationId\" , correlationId ); Logger logger = LoggerFactory . getLogger ( getClass ()); logger . debug ( \"I am not important\" ); client . url ( \"http://www.google.com\" ). get () . whenComplete (( response , throwable ) -> { //CorrelationIdMarker correlationIdMarker = CorrelationIdMarker.create(\"12345\"); String statusText = response . getStatusText (); String body = response . getBody ( string ()); logger . info ( \"Got a response \" + statusText ); }) . thenRun (() -> { try { Config config = system . settings (). config (); HikariDataSource dataSource = createDataSource ( config ); List < String > results = queryDatabase ( dataSource , correlationId ); results . forEach ( System . out :: println ); client . close (); } catch ( Exception e ) { e . printStackTrace (); } }) . thenRun ( system :: terminate ); } List < String > queryDatabase ( javax . sql . DataSource datasource , String correlationId ) throws SQLException { try ( Connection conn = datasource . getConnection ()) { try ( PreparedStatement p = conn . prepareStatement ( \"select * from events where correlation_id = ? order by ts\" )) { p . setString ( 1 , correlationId ); try ( ResultSet rs = p . executeQuery ()) { List < String > results = new ArrayList <> (); while ( rs . next ()) { Timestamp ts = rs . getTimestamp ( \"ts\" ); String json = rs . getString ( \"evt\" ); String s = String . format ( \"ts = %s, json = %s\" , ts , json ); //int count = rs.getInt(1); //String s = String.format(\"count = %d\", count); results . add ( s ); } return results ; } } } } } The configuration here uses a tap filter with a correlation id match up, and writes out the correlation id to the in memory database: <configuration> <shutdownHook class= \"ch.qos.logback.core.hook.DelayingShutdownHook\" /> <newRule pattern= \"configuration/typesafeConfig\" actionClass= \"com.tersesystems.logback.typesafeconfig.TypesafeConfigAction\" /> <newRule pattern= \"configuration/turboFilter/appender-ref\" actionClass= \"ch.qos.logback.core.joran.action.AppenderRefAction\" /> <typesafeConfig> </typesafeConfig> <appender name= \"ASYNC_JDBC\" class= \"net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender\" > <appender class= \"com.tersesystems.logback.correlationid.CorrelationIdJDBCAppender\" > <mdcKey> correlationId </mdcKey> <driver> ${jdbc.driver} </driver> <url> ${jdbc.url} </url> <username> ${jdbc.username} </username> <password> ${jdbc.password} </password> <createStatements> ${jdbc.createStatements} </createStatements> <insertStatement> ${jdbc.insertStatement} </insertStatement> <reaperStatement> ${jdbc.reaperStatement} </reaperStatement> <reaperSchedule> ${jdbc.reaperSchedule} </reaperSchedule> <encoder class= \"net.logstash.logback.encoder.LogstashEncoder\" > </encoder> </appender> </appender> <turboFilter class= \"com.tersesystems.logback.correlationid.CorrelationIdTapFilter\" > <mdcKey> correlationId </mdcKey> <appender-ref ref= \"ASYNC_JDBC\" /> </turboFilter> <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <root level= \"INFO\" > <appender-ref ref= \"CONSOLE\" /> </root> </configuration> and the logback.conf file is: local { jdbc { url = \"jdbc:h2:mem:terse-logback;DB_CLOSE_DELAY=-1\" driver = \"org.h2.Driver\" username = \"sa\" password = \"\" insertStatement = \"insert into events(ts, tse_ms, start_ms, level_value, level, evt, correlation_id) values(?, ?, ?, ?, ?, ?, ?)\" createStatements = \"\"\" CREATE TABLE IF NOT EXISTS events ( ID NUMERIC NOT NULL PRIMARY KEY AUTO_INCREMENT, ts TIMESTAMP(9) WITH TIME ZONE NOT NULL, tse_ms numeric NOT NULL, start_ms numeric NULL, level_value int NOT NULL, level VARCHAR(7) NOT NULL, evt JSON NOT NULL, correlation_id VARCHAR(255) NULL ); CREATE INDEX correlation_id_idx ON events(correlation_id); \"\"\" reaperStatement = \"delete from events where ts < ?\" reaperSchedule = PT30 } } The output from this program shows that we can log at a regular INFO level, and still get access to all the DEBUG information that was posted \"under the hood\" to the in memory database if we need to: 514 INFO com.zaxxer.hikari.HikariDataSource - jdbc-appender-pool-1581912533237 - Starting... 699 INFO com.zaxxer.hikari.HikariDataSource - jdbc-appender-pool-1581912533237 - Start completed. 761 INFO playwsclient.JavaClient - Got a response OK 765 INFO com.zaxxer.hikari.HikariDataSource - client-pool - Starting... 766 INFO com.zaxxer.hikari.HikariDataSource - client-pool - Start completed. ts = 2020-02-16 20:08:53.652, json = {\"@timestamp\":\"2020-02-16T20:08:53.652-08:00\",\"@version\":\"1\",\"message\":\"I am not important\",\"logger_name\":\"playwsclient.JavaClient\",\"thread_name\":\"main\",\"level\":\"DEBUG\",\"level_value\":10000,\"correlationId\":\"12345\"} ts = 2020-02-16 20:08:53.692, json = {\"@timestamp\":\"2020-02-16T20:08:53.692-08:00\",\"@version\":\"1\",\"message\":\"-Dio.netty.processId: 31802 (auto-detected)\",\"logger_name\":\"play.shaded.ahc.io.netty.channel.DefaultChannelId\",\"thread_name\":\"main\",\"level\":\"DEBUG\",\"level_value\":10000,\"correlationId\":\"12345\"} ts = 2020-02-16 20:08:53.693, json = {\"@timestamp\":\"2020-02-16T20:08:53.693-08:00\",\"@version\":\"1\",\"message\":\"-Djava.net.preferIPv4Stack: false\",\"logger_name\":\"play.shaded.ahc.io.netty.util.NetUtil\",\"thread_name\":\"main\",\"level\":\"DEBUG\",\"level_value\":10000,\"correlationId\":\"12345\"} ts = 2020-02-16 20:08:53.693, json = {\"@timestamp\":\"2020-02-16T20:08:53.693-08:00\",\"@version\":\"1\",\"message\":\"-Djava.net.preferIPv6Addresses: false\",\"logger_name\":\"play.shaded.ahc.io.netty.util.NetUtil\",\"thread_name\":\"main\",\"level\":\"DEBUG\",\"level_value\":10000,\"correlationId\":\"12345\"} ts = 2020-02-16 20:08:53.694, json = {\"@timestamp\":\"2020-02-16T20:08:53.694-08:00\",\"@version\":\"1\",\"message\":\"Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)\",\"logger_name\":\"play.shaded.ahc.io.netty.util.NetUtil\",\"thread_name\":\"main\",\"level\":\"DEBUG\",\"level_value\":10000,\"correlationId\":\"12345\"} ts = 2020-02-16 20:08:53.695, json = {\"@timestamp\":\"2020-02-16T20:08:53.695-08:00\",\"@version\":\"1\",\"message\":\"/proc/sys/net/core/somaxconn: 128\",\"logger_name\":\"play.shaded.ahc.io.netty.util.NetUtil\",\"thread_name\":\"main\",\"level\":\"DEBUG\",\"level_value\":10000,\"correlationId\":\"12345\"} ts = 2020-02-16 20:08:53.696, json = {\"@timestamp\":\"2020-02-16T20:08:53.696-08:00\",\"@version\":\"1\",\"message\":\"-Dio.netty.machineId: 08:00:27:ff:fe:5a:5f:59 (auto-detected)\",\"logger_name\":\"play.shaded.ahc.io.netty.channel.DefaultChannelId\",\"thread_name\":\"main\",\"level\":\"DEBUG\",\"level_value\":10000,\"correlationId\":\"12345\"} This is only one approach to storing diagnostic information -- the other approach is to use turbo filters and markers based on ring buffers.","title":"Tap Filters"},{"location":"guide/tapfilter/#tap-filter","text":"A tap filter is used to tap some amount of incoming process and pass them to a specially configured appender even if they do not qualify as a logging event under normal circumstances. This is a wiretap pattern from Enterprise Integration Patterns. Tap Filters are very useful as a way to send data to an appender. They completely bypass any kind of logging level configured on the front end, so you can set a logger to INFO level but still have access to all TRACE events when an error occurs, through the tap filter's appenders. For example, a tap filter can automatically log everything with a correlation id at a TRACE level, without requiring filters or altering the log level as a whole. Let's run a simple HTTP client program that calls out to Google and prints a result. package playwsclient ; import akka.actor.ActorSystem ; import akka.stream.Materializer ; import akka.stream.SystemMaterializer ; import com.typesafe.config.Config ; import com.typesafe.config.ConfigFactory ; import com.zaxxer.hikari.HikariConfig ; import com.zaxxer.hikari.HikariDataSource ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import org.slf4j.MDC ; import play.libs.ws.* ; import play.libs.ws.ahc.* ; import java.sql.* ; import java.util.ArrayList ; import java.util.List ; import java.util.Properties ; public class JavaClient implements DefaultBodyReadables { private final StandaloneAhcWSClient client ; private final ActorSystem system ; HikariDataSource createDataSource ( Config config ) { Config jdbcConfig = config . getConfig ( \"logback.jdbc\" ); String driver = jdbcConfig . getString ( \"driver\" ); String url = jdbcConfig . getString ( \"url\" ); String user = jdbcConfig . getString ( \"username\" ); String password = jdbcConfig . getString ( \"password\" ); return createDataSource ( driver , url , user , password ); } protected HikariDataSource createDataSource ( String driver , String url , String username , String password ) { HikariConfig config = new HikariConfig (); config . setDriverClassName ( driver ); config . setJdbcUrl ( url ); config . setUsername ( username ); config . setPassword ( password ); config . setPoolName ( \"client-pool\" ); config . setMaximumPoolSize ( 1 ); Properties props = new Properties (); // props.put(\"dataSource.logWriter\", new PrintWriter(System.out)); config . setDataSourceProperties ( props ); return new HikariDataSource ( config ); } public static void main ( String [] args ) { // Set up Akka materializer to handle streaming final String name = \"wsclient\" ; ActorSystem system = ActorSystem . create ( name ); system . registerOnTermination (() -> System . exit ( 0 )); Materializer materializer = SystemMaterializer . get ( system ). materializer (); // Create the WS client from the `application.conf` file, the current classloader and materializer. StandaloneAhcWSClient ws = StandaloneAhcWSClient . create ( AhcWSClientConfigFactory . forConfig ( ConfigFactory . load (), system . getClass (). getClassLoader ()), materializer ); JavaClient javaClient = new JavaClient ( system , ws ); javaClient . run (); } JavaClient ( ActorSystem system , StandaloneAhcWSClient client ) { this . system = system ; this . client = client ; } public void run () { String correlationId = \"12345\" ; MDC . put ( \"correlationId\" , correlationId ); Logger logger = LoggerFactory . getLogger ( getClass ()); logger . debug ( \"I am not important\" ); client . url ( \"http://www.google.com\" ). get () . whenComplete (( response , throwable ) -> { //CorrelationIdMarker correlationIdMarker = CorrelationIdMarker.create(\"12345\"); String statusText = response . getStatusText (); String body = response . getBody ( string ()); logger . info ( \"Got a response \" + statusText ); }) . thenRun (() -> { try { Config config = system . settings (). config (); HikariDataSource dataSource = createDataSource ( config ); List < String > results = queryDatabase ( dataSource , correlationId ); results . forEach ( System . out :: println ); client . close (); } catch ( Exception e ) { e . printStackTrace (); } }) . thenRun ( system :: terminate ); } List < String > queryDatabase ( javax . sql . DataSource datasource , String correlationId ) throws SQLException { try ( Connection conn = datasource . getConnection ()) { try ( PreparedStatement p = conn . prepareStatement ( \"select * from events where correlation_id = ? order by ts\" )) { p . setString ( 1 , correlationId ); try ( ResultSet rs = p . executeQuery ()) { List < String > results = new ArrayList <> (); while ( rs . next ()) { Timestamp ts = rs . getTimestamp ( \"ts\" ); String json = rs . getString ( \"evt\" ); String s = String . format ( \"ts = %s, json = %s\" , ts , json ); //int count = rs.getInt(1); //String s = String.format(\"count = %d\", count); results . add ( s ); } return results ; } } } } } The configuration here uses a tap filter with a correlation id match up, and writes out the correlation id to the in memory database: <configuration> <shutdownHook class= \"ch.qos.logback.core.hook.DelayingShutdownHook\" /> <newRule pattern= \"configuration/typesafeConfig\" actionClass= \"com.tersesystems.logback.typesafeconfig.TypesafeConfigAction\" /> <newRule pattern= \"configuration/turboFilter/appender-ref\" actionClass= \"ch.qos.logback.core.joran.action.AppenderRefAction\" /> <typesafeConfig> </typesafeConfig> <appender name= \"ASYNC_JDBC\" class= \"net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender\" > <appender class= \"com.tersesystems.logback.correlationid.CorrelationIdJDBCAppender\" > <mdcKey> correlationId </mdcKey> <driver> ${jdbc.driver} </driver> <url> ${jdbc.url} </url> <username> ${jdbc.username} </username> <password> ${jdbc.password} </password> <createStatements> ${jdbc.createStatements} </createStatements> <insertStatement> ${jdbc.insertStatement} </insertStatement> <reaperStatement> ${jdbc.reaperStatement} </reaperStatement> <reaperSchedule> ${jdbc.reaperSchedule} </reaperSchedule> <encoder class= \"net.logstash.logback.encoder.LogstashEncoder\" > </encoder> </appender> </appender> <turboFilter class= \"com.tersesystems.logback.correlationid.CorrelationIdTapFilter\" > <mdcKey> correlationId </mdcKey> <appender-ref ref= \"ASYNC_JDBC\" /> </turboFilter> <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <root level= \"INFO\" > <appender-ref ref= \"CONSOLE\" /> </root> </configuration> and the logback.conf file is: local { jdbc { url = \"jdbc:h2:mem:terse-logback;DB_CLOSE_DELAY=-1\" driver = \"org.h2.Driver\" username = \"sa\" password = \"\" insertStatement = \"insert into events(ts, tse_ms, start_ms, level_value, level, evt, correlation_id) values(?, ?, ?, ?, ?, ?, ?)\" createStatements = \"\"\" CREATE TABLE IF NOT EXISTS events ( ID NUMERIC NOT NULL PRIMARY KEY AUTO_INCREMENT, ts TIMESTAMP(9) WITH TIME ZONE NOT NULL, tse_ms numeric NOT NULL, start_ms numeric NULL, level_value int NOT NULL, level VARCHAR(7) NOT NULL, evt JSON NOT NULL, correlation_id VARCHAR(255) NULL ); CREATE INDEX correlation_id_idx ON events(correlation_id); \"\"\" reaperStatement = \"delete from events where ts < ?\" reaperSchedule = PT30 } } The output from this program shows that we can log at a regular INFO level, and still get access to all the DEBUG information that was posted \"under the hood\" to the in memory database if we need to: 514 INFO com.zaxxer.hikari.HikariDataSource - jdbc-appender-pool-1581912533237 - Starting... 699 INFO com.zaxxer.hikari.HikariDataSource - jdbc-appender-pool-1581912533237 - Start completed. 761 INFO playwsclient.JavaClient - Got a response OK 765 INFO com.zaxxer.hikari.HikariDataSource - client-pool - Starting... 766 INFO com.zaxxer.hikari.HikariDataSource - client-pool - Start completed. ts = 2020-02-16 20:08:53.652, json = {\"@timestamp\":\"2020-02-16T20:08:53.652-08:00\",\"@version\":\"1\",\"message\":\"I am not important\",\"logger_name\":\"playwsclient.JavaClient\",\"thread_name\":\"main\",\"level\":\"DEBUG\",\"level_value\":10000,\"correlationId\":\"12345\"} ts = 2020-02-16 20:08:53.692, json = {\"@timestamp\":\"2020-02-16T20:08:53.692-08:00\",\"@version\":\"1\",\"message\":\"-Dio.netty.processId: 31802 (auto-detected)\",\"logger_name\":\"play.shaded.ahc.io.netty.channel.DefaultChannelId\",\"thread_name\":\"main\",\"level\":\"DEBUG\",\"level_value\":10000,\"correlationId\":\"12345\"} ts = 2020-02-16 20:08:53.693, json = {\"@timestamp\":\"2020-02-16T20:08:53.693-08:00\",\"@version\":\"1\",\"message\":\"-Djava.net.preferIPv4Stack: false\",\"logger_name\":\"play.shaded.ahc.io.netty.util.NetUtil\",\"thread_name\":\"main\",\"level\":\"DEBUG\",\"level_value\":10000,\"correlationId\":\"12345\"} ts = 2020-02-16 20:08:53.693, json = {\"@timestamp\":\"2020-02-16T20:08:53.693-08:00\",\"@version\":\"1\",\"message\":\"-Djava.net.preferIPv6Addresses: false\",\"logger_name\":\"play.shaded.ahc.io.netty.util.NetUtil\",\"thread_name\":\"main\",\"level\":\"DEBUG\",\"level_value\":10000,\"correlationId\":\"12345\"} ts = 2020-02-16 20:08:53.694, json = {\"@timestamp\":\"2020-02-16T20:08:53.694-08:00\",\"@version\":\"1\",\"message\":\"Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)\",\"logger_name\":\"play.shaded.ahc.io.netty.util.NetUtil\",\"thread_name\":\"main\",\"level\":\"DEBUG\",\"level_value\":10000,\"correlationId\":\"12345\"} ts = 2020-02-16 20:08:53.695, json = {\"@timestamp\":\"2020-02-16T20:08:53.695-08:00\",\"@version\":\"1\",\"message\":\"/proc/sys/net/core/somaxconn: 128\",\"logger_name\":\"play.shaded.ahc.io.netty.util.NetUtil\",\"thread_name\":\"main\",\"level\":\"DEBUG\",\"level_value\":10000,\"correlationId\":\"12345\"} ts = 2020-02-16 20:08:53.696, json = {\"@timestamp\":\"2020-02-16T20:08:53.696-08:00\",\"@version\":\"1\",\"message\":\"-Dio.netty.machineId: 08:00:27:ff:fe:5a:5f:59 (auto-detected)\",\"logger_name\":\"play.shaded.ahc.io.netty.channel.DefaultChannelId\",\"thread_name\":\"main\",\"level\":\"DEBUG\",\"level_value\":10000,\"correlationId\":\"12345\"} This is only one approach to storing diagnostic information -- the other approach is to use turbo filters and markers based on ring buffers.","title":"Tap Filter"},{"location":"guide/tracing/","text":"Tracing to Honeycomb \u00b6 You can connect Logback to Honeycomb directly through the Honeycomb Logback appender. The appender is split into the appender and an HTTP client implementation, which can be OKHTTP or Play WS. Add the appender module 'logback-honeycomb-appender' and the implementation 'logback-honeycomb-okhttp': compile group: 'com.tersesystems.logback', name: 'logback-tracing' compile group: 'com.tersesystems.logback', name: 'logback-honeycomb-appender' compile group: 'com.tersesystems.logback', name: 'logback-honeycomb-okhttp' The appender is of type com.tersesystems.logback.honeycomb.HoneycombAppender , and makes use of the client under the hood. Because the honeycomb appender uses an HTTP client under the hood, there are a couple of important notes. NOTE : Because the HTTP client runs on a different thread, you must have a shutdown hook configured so that shutting down can be delayed until the events are posted. and NOTE : Because the HTTP client may have non-daemon threads running, you should call System.exit explicitly to stop the application if you are not in a long-running service. The appender is as follows: <configuration> <shutdownHook class= \"ch.qos.logback.core.hook.DelayingShutdownHook\" > <delay> 1000 </delay> </shutdownHook> <conversionRule conversionWord= \"startTime\" converterClass= \"com.tersesystems.logback.classic.StartTimeConverter\" /> <appender name= \"HONEYCOMB\" class= \"com.tersesystems.logback.honeycomb.HoneycombAppender\" > <apiKey> ${HONEYCOMB_API_KEY} </apiKey> <dataSet> terse-logback </dataSet> <sampleRate> 1 </sampleRate> <queueSize> 10 </queueSize> <batch> true </batch> <includeCallerData> false </includeCallerData> <encoder class= \"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\" > <providers> <message/> <loggerName/> <threadName/> <logLevel/> <stackHash/> <mdc/> <logstashMarkers/> <pattern> <pattern> { \"start_ms\": \"#asLong{%startTime}\" } </pattern> </pattern> <arguments/> <stackTrace> <throwableConverter class= \"net.logstash.logback.stacktrace.ShortenedThrowableConverter\" > <rootCauseFirst> true </rootCauseFirst> </throwableConverter> </stackTrace> </providers> </encoder> </appender> <!-- don't send the logs from the http engine to the appender or you may end up in a loop --> <logger name= \"okhttp\" level= \"ERROR\" /> <root level= \"INFO\" > <appender-ref ref= \"HONEYCOMB\" /> </root> </configuration> You can also send tracing information to Honeycomb through SLF4J markers, using the SpanMarkerFactory . Underneath the hood, the SpanInfo puts together logstash markers according to manual tracing . The way this works in practice is that you start up a SpanInfo at the beginning of a request, and call buildNow to mark the start of the span. At the end of the operation, you log with a marker, by passing through the marker factory: SpanInfo spanInfo = builder . setRootSpan ( \"index\" ). buildNow (); // ... logger . info ( markerFactory . apply ( spanInfo ), \"completed successfully!\" ); If you want to create a child span, you can do it from the parent using withChild : return spanInfo . withChild ( \"doSomething\" , childInfo -> { return doSomething ( childInfo ); }); or asking for a child builder that you can build yourself: SpanInfo childInfo = spanInfo . childBuilder (). setSpanName ( \"doSomething\" ). buildNow (); The start time information is captured in a StartTimeMarker which can be extracted by StartTime.from and is used in building the Honeycomb Request. The event timestamp serves as the span's end time. This is useful in Honeycomb Tracing, as the timestamp is the start time, not the time that the log entry was posted. For example, in Play you might run a controller as follows: import com.tersesystems.logback.tracing.SpanMarkerFactory import com.tersesystems.logback.tracing.SpanInfo import javax.inject._ import org.slf4j.LoggerFactory import play.api.libs.concurrent.Futures import play.api.mvc._ import scala.concurrent. { ExecutionContext , Future } import scala.util. { Failure , Success } import scala.concurrent.duration._ @Singleton class HomeController @Inject ()( cc : ControllerComponents , futures : Futures ) ( implicit ec : ExecutionContext ) extends AbstractController ( cc ) { private val markerFactory = new SpanMarkerFactory () private val logger = LoggerFactory . getLogger ( getClass ) private def builder : SpanInfo.Builder = SpanInfo . builder (). setServiceName ( \"play_hello_world\" ) def index () : Action [ AnyContent ] = Action . async { implicit request : Request [ AnyContent ] => val spanInfo = builder . setRootSpan ( \"index\" ). buildNow () val f : Future [ Result ] = spanInfo . withChild ( \"renderPage\" , renderPage ( _ )) f . andThen { case Success ( _ ) => logger . info ( markerFactory ( spanInfo ), \"index completed successfully!\" ) case Failure ( e ) => logger . error ( markerFactory ( spanInfo ), \"index completed with error\" , e ) } } def renderPage ( spanInfo : SpanInfo ) : Future [ Result ] = { futures . delay ( 5. seconds ). map { _ => Ok ( views . html . index ()) }. andThen { case Success ( _ ) => logger . info ( markerFactory ( spanInfo ), \"renderPage completed successfully!\" ) case Failure ( e ) => logger . error ( markerFactory ( spanInfo ), \"renderPage completed with error\" , e ) } } } This generates a trace with a root span of \"index\", a child span of \"renderPage\" each with their own durations. You can also send span events and span links using the LinkMarkerFactory and EventMarkerFactory , similar to the SpanMarkerFactory . See Tracing With Logback and Honeycomb and Hierarchical Instrumented Tracing with Logback for more details.","title":"Tracing with Honeycomb"},{"location":"guide/tracing/#tracing-to-honeycomb","text":"You can connect Logback to Honeycomb directly through the Honeycomb Logback appender. The appender is split into the appender and an HTTP client implementation, which can be OKHTTP or Play WS. Add the appender module 'logback-honeycomb-appender' and the implementation 'logback-honeycomb-okhttp': compile group: 'com.tersesystems.logback', name: 'logback-tracing' compile group: 'com.tersesystems.logback', name: 'logback-honeycomb-appender' compile group: 'com.tersesystems.logback', name: 'logback-honeycomb-okhttp' The appender is of type com.tersesystems.logback.honeycomb.HoneycombAppender , and makes use of the client under the hood. Because the honeycomb appender uses an HTTP client under the hood, there are a couple of important notes. NOTE : Because the HTTP client runs on a different thread, you must have a shutdown hook configured so that shutting down can be delayed until the events are posted. and NOTE : Because the HTTP client may have non-daemon threads running, you should call System.exit explicitly to stop the application if you are not in a long-running service. The appender is as follows: <configuration> <shutdownHook class= \"ch.qos.logback.core.hook.DelayingShutdownHook\" > <delay> 1000 </delay> </shutdownHook> <conversionRule conversionWord= \"startTime\" converterClass= \"com.tersesystems.logback.classic.StartTimeConverter\" /> <appender name= \"HONEYCOMB\" class= \"com.tersesystems.logback.honeycomb.HoneycombAppender\" > <apiKey> ${HONEYCOMB_API_KEY} </apiKey> <dataSet> terse-logback </dataSet> <sampleRate> 1 </sampleRate> <queueSize> 10 </queueSize> <batch> true </batch> <includeCallerData> false </includeCallerData> <encoder class= \"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\" > <providers> <message/> <loggerName/> <threadName/> <logLevel/> <stackHash/> <mdc/> <logstashMarkers/> <pattern> <pattern> { \"start_ms\": \"#asLong{%startTime}\" } </pattern> </pattern> <arguments/> <stackTrace> <throwableConverter class= \"net.logstash.logback.stacktrace.ShortenedThrowableConverter\" > <rootCauseFirst> true </rootCauseFirst> </throwableConverter> </stackTrace> </providers> </encoder> </appender> <!-- don't send the logs from the http engine to the appender or you may end up in a loop --> <logger name= \"okhttp\" level= \"ERROR\" /> <root level= \"INFO\" > <appender-ref ref= \"HONEYCOMB\" /> </root> </configuration> You can also send tracing information to Honeycomb through SLF4J markers, using the SpanMarkerFactory . Underneath the hood, the SpanInfo puts together logstash markers according to manual tracing . The way this works in practice is that you start up a SpanInfo at the beginning of a request, and call buildNow to mark the start of the span. At the end of the operation, you log with a marker, by passing through the marker factory: SpanInfo spanInfo = builder . setRootSpan ( \"index\" ). buildNow (); // ... logger . info ( markerFactory . apply ( spanInfo ), \"completed successfully!\" ); If you want to create a child span, you can do it from the parent using withChild : return spanInfo . withChild ( \"doSomething\" , childInfo -> { return doSomething ( childInfo ); }); or asking for a child builder that you can build yourself: SpanInfo childInfo = spanInfo . childBuilder (). setSpanName ( \"doSomething\" ). buildNow (); The start time information is captured in a StartTimeMarker which can be extracted by StartTime.from and is used in building the Honeycomb Request. The event timestamp serves as the span's end time. This is useful in Honeycomb Tracing, as the timestamp is the start time, not the time that the log entry was posted. For example, in Play you might run a controller as follows: import com.tersesystems.logback.tracing.SpanMarkerFactory import com.tersesystems.logback.tracing.SpanInfo import javax.inject._ import org.slf4j.LoggerFactory import play.api.libs.concurrent.Futures import play.api.mvc._ import scala.concurrent. { ExecutionContext , Future } import scala.util. { Failure , Success } import scala.concurrent.duration._ @Singleton class HomeController @Inject ()( cc : ControllerComponents , futures : Futures ) ( implicit ec : ExecutionContext ) extends AbstractController ( cc ) { private val markerFactory = new SpanMarkerFactory () private val logger = LoggerFactory . getLogger ( getClass ) private def builder : SpanInfo.Builder = SpanInfo . builder (). setServiceName ( \"play_hello_world\" ) def index () : Action [ AnyContent ] = Action . async { implicit request : Request [ AnyContent ] => val spanInfo = builder . setRootSpan ( \"index\" ). buildNow () val f : Future [ Result ] = spanInfo . withChild ( \"renderPage\" , renderPage ( _ )) f . andThen { case Success ( _ ) => logger . info ( markerFactory ( spanInfo ), \"index completed successfully!\" ) case Failure ( e ) => logger . error ( markerFactory ( spanInfo ), \"index completed with error\" , e ) } } def renderPage ( spanInfo : SpanInfo ) : Future [ Result ] = { futures . delay ( 5. seconds ). map { _ => Ok ( views . html . index ()) }. andThen { case Success ( _ ) => logger . info ( markerFactory ( spanInfo ), \"renderPage completed successfully!\" ) case Failure ( e ) => logger . error ( markerFactory ( spanInfo ), \"renderPage completed with error\" , e ) } } } This generates a trace with a root span of \"index\", a child span of \"renderPage\" each with their own durations. You can also send span events and span links using the LinkMarkerFactory and EventMarkerFactory , similar to the SpanMarkerFactory . See Tracing With Logback and Honeycomb and Hierarchical Instrumented Tracing with Logback for more details.","title":"Tracing to Honeycomb"},{"location":"guide/turbomarker/","text":"Turbo Markers \u00b6 Turbo filters are filters that decide whether a logging event should be created or not. They are are not appender specific in the way that normal filters are, and so are used to override logger levels. However, there's a problem with the way that the turbo filter is set up: the two implementing classes are ch.qos.logback.classic.turbo.MarkerFilter and ch.qos.logback.classic.turbo.MDCFilter . The marker filter will always log if the given marker is applied, and the MDC filter relies on an attribute being populated in the MDC map. What we'd really like to do is say \"for this particular user, log everything he does at DEBUG level\" and not have it rely on thread-local state at all, and carry out an arbitrary computation at call time. We start by pulling the decide method to an interface, TurboFilterDecider : public interface TurboFilterDecider { FilterReply decide ( Marker marker , Logger logger , Level level , String format , Object [] params , Throwable t ); } And have the turbo filter delegate to markers that implement the TurboFilterDecider interface : public class TurboMarkerTurboFilter extends TurboFilter { @Override public FilterReply decide ( Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ) { // ... } private FilterReply evaluateMarker ( Marker marker , Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ) { if ( marker instanceof TurboFilterDecider ) { TurboFilterDecider decider = ( TurboFilterDecider ) marker ; return decider . decide ( rootMarker , logger , level , format , params , t ); } return FilterReply . NEUTRAL ; } } This gets us part of the way there. We can then set up a ContextAwareTurboFilterDecider , which does the same thing but assumes that you have a type C that is your external context. public interface ContextAwareTurboFilterDecider < C > { FilterReply decide ( ContextAwareTurboMarker < C > marker , C context , Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ); } Then we add a marker class that incorporates that context in decision making : public class ContextAwareTurboMarker < C > extends TurboMarker implements TurboFilterDecider { private final C context ; private final ContextAwareTurboFilterDecider < C > contextAwareDecider ; // ... initializers and such @Override public FilterReply decide ( Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ) { return contextAwareDecider . decide ( this , context , rootMarker , logger , level , format , params , t ); } } This may look good in the abstract, but it may make more sense to see it in action. To do this, we'll set up an example application context: public class ApplicationContext { private final String userId ; public ApplicationContext ( String userId ) { this . userId = userId ; } public String currentUserId () { return userId ; } } and a factory that contains the decider: import com.tersesystems.logback.turbomarker.* ; public class UserMarkerFactory { private final Set < String > userIdSet = new ConcurrentSkipListSet <> (); private final ContextDecider < ApplicationContext > decider = context -> userIdSet . contains ( context . currentUserId ()) ? FilterReply . ACCEPT : FilterReply . NEUTRAL ; public void addUserId ( String userId ) { userIdSet . add ( userId ); } public void clear () { userIdSet . clear (); } public UserMarker create ( ApplicationContext applicationContext ) { return new UserMarker ( \"userMarker\" , applicationContext , decider ); } } and a UserMarker , which is only around for the logging evaluation: public class UserMarker extends ContextAwareTurboMarker < ApplicationContext > { public UserMarker ( String name , ApplicationContext applicationContext , ContextAwareTurboFilterDecider < ApplicationContext > decider ) { super ( name , applicationContext , decider ); } } and then we can set up logging that will only work for user \"28\": String userId = \"28\" ; ApplicationContext applicationContext = new ApplicationContext ( userId ); UserMarkerFactory userMarkerFactory = new UserMarkerFactory (); userMarkerFactory . addUserId ( userId ); // say we want logging events created for this user id UserMarker userMarker = userMarkerFactory . create ( applicationContext ); logger . info ( userMarker , \"Hello world, I am info and log for everyone\" ); logger . debug ( userMarker , \"Hello world, I am debug and only log for user 28\" ); This works especially well with a configuration management service like Launch Darkly , where you can target particular users and set up logging based on the user variation. The LaunchDarkly blog has best practices for operational flags : Verbose logs are great for debugging and troubleshooting but always running an application in debug mode is not viable. The amount of log data generated would be overwhelming. Changing logging levels on the fly typically requires changing a configuration file and restarting the application. A multivariate operational flag enables you to change the logging level from WARNING to DEBUG in real-time. But we can give an example using the Java SDK. You can set up a factory like so: import ch.qos.logback.classic.Logger ; import ch.qos.logback.core.spi.FilterReply ; import com.launchdarkly.client.LDClientInterface ; import com.launchdarkly.client.LDUser ; public class LDMarkerFactory { private final LaunchDarklyDecider decider ; public LDMarkerFactory ( LDClientInterface client ) { this . decider = new LaunchDarklyDecider ( requireNonNull ( client )); } public LDMarker create ( String featureFlag , LDUser user ) { return new LDMarker ( featureFlag , user , decider ); } static class LaunchDarklyDecider implements MarkerContextDecider < LDUser > { private final LDClientInterface ldClient ; LaunchDarklyDecider ( LDClientInterface ldClient ) { this . ldClient = ldClient ; } @Override public FilterReply apply ( ContextAwareTurboMarker < LDUser > marker , LDUser ldUser ) { return ldClient . boolVariation ( marker . getName (), ldUser , false ) ? FilterReply . ACCEPT : FilterReply . NEUTRAL ; } } public static class LDMarker extends ContextAwareTurboMarker < LDUser > { LDMarker ( String name , LDUser context , ContextAwareTurboFilterDecider < LDUser > decider ) { super ( name , context , decider ); } } } and then use the feature flag as the marker name and target the beta testers group: public class LDMarkerTest { private static LDClientInterface client ; @BeforeAll public static void setUp () { client = new LDClient ( \"sdk-key\" ); } @AfterAll public static void shutDown () { try { client . close (); } catch ( IOException e ) { e . printStackTrace (); } } public void testMatchingMarker () throws JoranException { LoggerContext loggerContext = ( LoggerContext ) LoggerFactory . getILoggerFactory (); ch . qos . logback . classic . Logger logger = loggerContext . getLogger ( org . slf4j . Logger . ROOT_LOGGER_NAME ); LDMarkerFactory markerFactory = new LDMarkerFactory ( client ); LDUser ldUser = new LDUser . Builder ( \"UNIQUE IDENTIFIER\" ) . firstName ( \"Bob\" ) . lastName ( \"Loblaw\" ) . customString ( \"groups\" , singletonList ( \"beta_testers\" )) . build (); LDMarkerFactory . LDMarker ldMarker = markerFactory . create ( \"turbomarker\" , ldUser ); logger . info ( ldMarker , \"Hello world, I am info\" ); logger . debug ( ldMarker , \"Hello world, I am debug\" ); ListAppender < ILoggingEvent > appender = ( ListAppender < ILoggingEvent > ) logger . getAppender ( \"LIST\" ); assertThat ( appender . list . size ()). isEqualTo ( 2 ); appender . list . clear (); } } This is also a reason why diagnostic logging is better than a debugger . Debuggers are ephemeral, can't be used in production, and don't produce a consistent record of events: debugging log statements are the single best way to dump internal state and manage code flows in an application. See Targeted Diagnostic Logging in Production for more details.","title":"Turbo Markers"},{"location":"guide/turbomarker/#turbo-markers","text":"Turbo filters are filters that decide whether a logging event should be created or not. They are are not appender specific in the way that normal filters are, and so are used to override logger levels. However, there's a problem with the way that the turbo filter is set up: the two implementing classes are ch.qos.logback.classic.turbo.MarkerFilter and ch.qos.logback.classic.turbo.MDCFilter . The marker filter will always log if the given marker is applied, and the MDC filter relies on an attribute being populated in the MDC map. What we'd really like to do is say \"for this particular user, log everything he does at DEBUG level\" and not have it rely on thread-local state at all, and carry out an arbitrary computation at call time. We start by pulling the decide method to an interface, TurboFilterDecider : public interface TurboFilterDecider { FilterReply decide ( Marker marker , Logger logger , Level level , String format , Object [] params , Throwable t ); } And have the turbo filter delegate to markers that implement the TurboFilterDecider interface : public class TurboMarkerTurboFilter extends TurboFilter { @Override public FilterReply decide ( Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ) { // ... } private FilterReply evaluateMarker ( Marker marker , Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ) { if ( marker instanceof TurboFilterDecider ) { TurboFilterDecider decider = ( TurboFilterDecider ) marker ; return decider . decide ( rootMarker , logger , level , format , params , t ); } return FilterReply . NEUTRAL ; } } This gets us part of the way there. We can then set up a ContextAwareTurboFilterDecider , which does the same thing but assumes that you have a type C that is your external context. public interface ContextAwareTurboFilterDecider < C > { FilterReply decide ( ContextAwareTurboMarker < C > marker , C context , Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ); } Then we add a marker class that incorporates that context in decision making : public class ContextAwareTurboMarker < C > extends TurboMarker implements TurboFilterDecider { private final C context ; private final ContextAwareTurboFilterDecider < C > contextAwareDecider ; // ... initializers and such @Override public FilterReply decide ( Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ) { return contextAwareDecider . decide ( this , context , rootMarker , logger , level , format , params , t ); } } This may look good in the abstract, but it may make more sense to see it in action. To do this, we'll set up an example application context: public class ApplicationContext { private final String userId ; public ApplicationContext ( String userId ) { this . userId = userId ; } public String currentUserId () { return userId ; } } and a factory that contains the decider: import com.tersesystems.logback.turbomarker.* ; public class UserMarkerFactory { private final Set < String > userIdSet = new ConcurrentSkipListSet <> (); private final ContextDecider < ApplicationContext > decider = context -> userIdSet . contains ( context . currentUserId ()) ? FilterReply . ACCEPT : FilterReply . NEUTRAL ; public void addUserId ( String userId ) { userIdSet . add ( userId ); } public void clear () { userIdSet . clear (); } public UserMarker create ( ApplicationContext applicationContext ) { return new UserMarker ( \"userMarker\" , applicationContext , decider ); } } and a UserMarker , which is only around for the logging evaluation: public class UserMarker extends ContextAwareTurboMarker < ApplicationContext > { public UserMarker ( String name , ApplicationContext applicationContext , ContextAwareTurboFilterDecider < ApplicationContext > decider ) { super ( name , applicationContext , decider ); } } and then we can set up logging that will only work for user \"28\": String userId = \"28\" ; ApplicationContext applicationContext = new ApplicationContext ( userId ); UserMarkerFactory userMarkerFactory = new UserMarkerFactory (); userMarkerFactory . addUserId ( userId ); // say we want logging events created for this user id UserMarker userMarker = userMarkerFactory . create ( applicationContext ); logger . info ( userMarker , \"Hello world, I am info and log for everyone\" ); logger . debug ( userMarker , \"Hello world, I am debug and only log for user 28\" ); This works especially well with a configuration management service like Launch Darkly , where you can target particular users and set up logging based on the user variation. The LaunchDarkly blog has best practices for operational flags : Verbose logs are great for debugging and troubleshooting but always running an application in debug mode is not viable. The amount of log data generated would be overwhelming. Changing logging levels on the fly typically requires changing a configuration file and restarting the application. A multivariate operational flag enables you to change the logging level from WARNING to DEBUG in real-time. But we can give an example using the Java SDK. You can set up a factory like so: import ch.qos.logback.classic.Logger ; import ch.qos.logback.core.spi.FilterReply ; import com.launchdarkly.client.LDClientInterface ; import com.launchdarkly.client.LDUser ; public class LDMarkerFactory { private final LaunchDarklyDecider decider ; public LDMarkerFactory ( LDClientInterface client ) { this . decider = new LaunchDarklyDecider ( requireNonNull ( client )); } public LDMarker create ( String featureFlag , LDUser user ) { return new LDMarker ( featureFlag , user , decider ); } static class LaunchDarklyDecider implements MarkerContextDecider < LDUser > { private final LDClientInterface ldClient ; LaunchDarklyDecider ( LDClientInterface ldClient ) { this . ldClient = ldClient ; } @Override public FilterReply apply ( ContextAwareTurboMarker < LDUser > marker , LDUser ldUser ) { return ldClient . boolVariation ( marker . getName (), ldUser , false ) ? FilterReply . ACCEPT : FilterReply . NEUTRAL ; } } public static class LDMarker extends ContextAwareTurboMarker < LDUser > { LDMarker ( String name , LDUser context , ContextAwareTurboFilterDecider < LDUser > decider ) { super ( name , context , decider ); } } } and then use the feature flag as the marker name and target the beta testers group: public class LDMarkerTest { private static LDClientInterface client ; @BeforeAll public static void setUp () { client = new LDClient ( \"sdk-key\" ); } @AfterAll public static void shutDown () { try { client . close (); } catch ( IOException e ) { e . printStackTrace (); } } public void testMatchingMarker () throws JoranException { LoggerContext loggerContext = ( LoggerContext ) LoggerFactory . getILoggerFactory (); ch . qos . logback . classic . Logger logger = loggerContext . getLogger ( org . slf4j . Logger . ROOT_LOGGER_NAME ); LDMarkerFactory markerFactory = new LDMarkerFactory ( client ); LDUser ldUser = new LDUser . Builder ( \"UNIQUE IDENTIFIER\" ) . firstName ( \"Bob\" ) . lastName ( \"Loblaw\" ) . customString ( \"groups\" , singletonList ( \"beta_testers\" )) . build (); LDMarkerFactory . LDMarker ldMarker = markerFactory . create ( \"turbomarker\" , ldUser ); logger . info ( ldMarker , \"Hello world, I am info\" ); logger . debug ( ldMarker , \"Hello world, I am debug\" ); ListAppender < ILoggingEvent > appender = ( ListAppender < ILoggingEvent > ) logger . getAppender ( \"LIST\" ); assertThat ( appender . list . size ()). isEqualTo ( 2 ); appender . list . clear (); } } This is also a reason why diagnostic logging is better than a debugger . Debuggers are ephemeral, can't be used in production, and don't produce a consistent record of events: debugging log statements are the single best way to dump internal state and manage code flows in an application. See Targeted Diagnostic Logging in Production for more details.","title":"Turbo Markers"},{"location":"guide/typesafeconfig/","text":"Typesafe Config \u00b6 The TypesafeConfigAction will search in a variety of places for configuration using standard fallback behavior for Typesafe Config, which gives a richer experience to end users. Config config = systemProperties // Look for a property from system properties first... . withFallback ( file ) // if we don't find it, then look in an explicitly defined file... . withFallback ( testResources ) // if not, then if logback-test.conf exists, look for it there... . withFallback ( resources ) // then look in logback.conf... . withFallback ( reference ) // and then finally in logback-reference.conf. . resolve (); // Tell config that we want to use ${?ENV_VAR} type stuff. The configuration is then placed in the LoggerContext which is available to all of Logback. lc . putObject ( ConfigConstants . TYPESAFE_CONFIG_CTX_KEY , config ); And then all properties are made available to Logback, either at the local scope or at the context scope. Properties must be strings, but you can also provide Maps and Lists to the Logback Context, through context.getObject . Log Levels and Properties through Typesafe Config \u00b6 Configuration of properties and setting log levels is done through Typesafe Config , using TypesafeConfigAction Here's the logback.conf from the example application. It's in Human-Optimized Config Object Notation or HOCON . # Set logger levels here. levels = { # Override the default root log level with ROOT_LOG_LEVEL environment variable, if defined... ROOT = ${?ROOT_LOG_LEVEL} # You can set a logger with a simple package name. example = DEBUG # You can also do nested overrides here. deeply.nested { package = TRACE } } # Overrides the properties from logback-reference.conf local { logback.environment=production censor { regex = \"\"\"hunter2\"\"\" // http://bash.org/?244321 replacementText = \"*******\" json.keys += \"password\" // adding password key will remove the key/value pair entirely } # Overwrite text file on every run. textfile { append = false } # Override the color code in console for info statements highlight { info = \"black\" } } # You can also include settings from other places include \"myothersettings\" For tests, there's a logback-test.conf that will override (rather than completely replace) any settings that you have in logback.conf : include \"logback-reference\" levels { example = TRACE } local { logback.environment=test textfile { location = \"log/test/application-test.log\" append = false } jsonfile { location = \"log/test/application-test.json\" prettyprint = true } } There is also a logback-reference.conf file that handles the default configuration for the appenders, and those settings can be overridden. They are written out individually in the encoder configuration so I won't go over it here. Note that appender logic is not available here -- it's all defined through the structured-config in logback.xml . Using Typesafe Config is not a requirement -- the point here is to show that there are more options to configuring Logback than using a straight XML file. See Application Logging in Java: Adding Configuration for more details.","title":"Typesafe Config"},{"location":"guide/typesafeconfig/#typesafe-config","text":"The TypesafeConfigAction will search in a variety of places for configuration using standard fallback behavior for Typesafe Config, which gives a richer experience to end users. Config config = systemProperties // Look for a property from system properties first... . withFallback ( file ) // if we don't find it, then look in an explicitly defined file... . withFallback ( testResources ) // if not, then if logback-test.conf exists, look for it there... . withFallback ( resources ) // then look in logback.conf... . withFallback ( reference ) // and then finally in logback-reference.conf. . resolve (); // Tell config that we want to use ${?ENV_VAR} type stuff. The configuration is then placed in the LoggerContext which is available to all of Logback. lc . putObject ( ConfigConstants . TYPESAFE_CONFIG_CTX_KEY , config ); And then all properties are made available to Logback, either at the local scope or at the context scope. Properties must be strings, but you can also provide Maps and Lists to the Logback Context, through context.getObject .","title":"Typesafe Config"},{"location":"guide/typesafeconfig/#log-levels-and-properties-through-typesafe-config","text":"Configuration of properties and setting log levels is done through Typesafe Config , using TypesafeConfigAction Here's the logback.conf from the example application. It's in Human-Optimized Config Object Notation or HOCON . # Set logger levels here. levels = { # Override the default root log level with ROOT_LOG_LEVEL environment variable, if defined... ROOT = ${?ROOT_LOG_LEVEL} # You can set a logger with a simple package name. example = DEBUG # You can also do nested overrides here. deeply.nested { package = TRACE } } # Overrides the properties from logback-reference.conf local { logback.environment=production censor { regex = \"\"\"hunter2\"\"\" // http://bash.org/?244321 replacementText = \"*******\" json.keys += \"password\" // adding password key will remove the key/value pair entirely } # Overwrite text file on every run. textfile { append = false } # Override the color code in console for info statements highlight { info = \"black\" } } # You can also include settings from other places include \"myothersettings\" For tests, there's a logback-test.conf that will override (rather than completely replace) any settings that you have in logback.conf : include \"logback-reference\" levels { example = TRACE } local { logback.environment=test textfile { location = \"log/test/application-test.log\" append = false } jsonfile { location = \"log/test/application-test.json\" prettyprint = true } } There is also a logback-reference.conf file that handles the default configuration for the appenders, and those settings can be overridden. They are written out individually in the encoder configuration so I won't go over it here. Note that appender logic is not available here -- it's all defined through the structured-config in logback.xml . Using Typesafe Config is not a requirement -- the point here is to show that there are more options to configuring Logback than using a straight XML file. See Application Logging in Java: Adding Configuration for more details.","title":"Log Levels and Properties through Typesafe Config"},{"location":"guide/uniqueid/","text":"Unique ID Appenders \u00b6 The unique id appender allows the logging event to carry a unique id. When used in conjunction with SelectAppender or CompositeAppender , this allows for a log record to use the same id across different logs. For example, in application.log , you'll see a single line that starts with FfwJtsNHYSw6O0Qbm7EAAA : FfwJtsNHYSw6O0Qbm7EAAA 2020-03-14T05:30:14.965+0000 [INFO ] play.api.db.HikariCPConnectionPool in play-dev-mode-akka.actor.default-dispatcher-7 - Creating Pool for datasource 'logging' You can search for this string in application.json and see more detail on the log record: { \"id\" : \"FfwJtsNHYSw6O0Qbm7EAAA\" , \"relative_ns\" : 20921024 , \"tse_ms\" : 1584163814965 , \"start_ms\" : null , \"@timestamp\" : \"2020-03-14T05:30:14.965Z\" , \"@version\" : \"1\" , \"message\" : \"Creating Pool for datasource 'logging'\" , \"logger_name\" : \"play.api.db.HikariCPConnectionPool\" , \"thread_name\" : \"play-dev-mode-akka.actor.default-dispatcher-7\" , \"level\" : \"INFO\" , \"level_value\" : 20000 } See the showcase for an example. Usage \u00b6 <appender name= \"selector-with-unique-id\" class= \"com.tersesystems.logback.uniqueid.UniqueIdComponentAppender\" > <appender ... > </appender> </appender> To extract the unique ID, register a converter: <!-- available as \"%uniqueId\" in a pattern layout --> <conversionRule conversionWord= \"uniqueId\" converterClass= \"com.tersesystems.logback.uniqueid.UniqueIdConverter\" /> ID Generators \u00b6 Unique IDs come with two different options. Flake ID is the default. Random UUID \u00b6 This implementation uses RandomBasedGenerator from Java UUID Generator . This is faster than using java.util.UUID.randomUUID , because it avoids the synchronization lock . Flake ID \u00b6 Flake IDs are decentralized and k-ordered, meaning that they are \"roughly time-ordered when sorted lexicographically.\" This implementation uses idem with Flake128S .","title":"Unique ID Appender"},{"location":"guide/uniqueid/#unique-id-appenders","text":"The unique id appender allows the logging event to carry a unique id. When used in conjunction with SelectAppender or CompositeAppender , this allows for a log record to use the same id across different logs. For example, in application.log , you'll see a single line that starts with FfwJtsNHYSw6O0Qbm7EAAA : FfwJtsNHYSw6O0Qbm7EAAA 2020-03-14T05:30:14.965+0000 [INFO ] play.api.db.HikariCPConnectionPool in play-dev-mode-akka.actor.default-dispatcher-7 - Creating Pool for datasource 'logging' You can search for this string in application.json and see more detail on the log record: { \"id\" : \"FfwJtsNHYSw6O0Qbm7EAAA\" , \"relative_ns\" : 20921024 , \"tse_ms\" : 1584163814965 , \"start_ms\" : null , \"@timestamp\" : \"2020-03-14T05:30:14.965Z\" , \"@version\" : \"1\" , \"message\" : \"Creating Pool for datasource 'logging'\" , \"logger_name\" : \"play.api.db.HikariCPConnectionPool\" , \"thread_name\" : \"play-dev-mode-akka.actor.default-dispatcher-7\" , \"level\" : \"INFO\" , \"level_value\" : 20000 } See the showcase for an example.","title":"Unique ID Appenders"},{"location":"guide/uniqueid/#usage","text":"<appender name= \"selector-with-unique-id\" class= \"com.tersesystems.logback.uniqueid.UniqueIdComponentAppender\" > <appender ... > </appender> </appender> To extract the unique ID, register a converter: <!-- available as \"%uniqueId\" in a pattern layout --> <conversionRule conversionWord= \"uniqueId\" converterClass= \"com.tersesystems.logback.uniqueid.UniqueIdConverter\" />","title":"Usage"},{"location":"guide/uniqueid/#id-generators","text":"Unique IDs come with two different options. Flake ID is the default.","title":"ID Generators"},{"location":"guide/uniqueid/#random-uuid","text":"This implementation uses RandomBasedGenerator from Java UUID Generator . This is faster than using java.util.UUID.randomUUID , because it avoids the synchronization lock .","title":"Random UUID"},{"location":"guide/uniqueid/#flake-id","text":"Flake IDs are decentralized and k-ordered, meaning that they are \"roughly time-ordered when sorted lexicographically.\" This implementation uses idem with Flake128S .","title":"Flake ID"},{"location":"reading/","text":"Further Reading \u00b6 Everything I write on logging is going to be here: Terse Systems Best Practices \u00b6 Many of these are logback specific, but still good overall. 9 Logging Best Practices Based on Hands-on Experience Woofer: logging in (best) practices : Spring Boot A whole product concern logging implementation There is more to logging than meets the eye Monitoring demystified: A guide for logging, tracing, metrics Application-Level Logging Best Practices Stack Overflow has a couple of good tips on SLF4J and Logging: When to use the different log levels Why does the TRACE level exist, and when should I use it rather than DEBUG? Best practices for using Markers in SLF4J/Logback Stackoverflow: Logging best practices in multi-node environment Level Up Logs \u00b6 Alberto Navarro has a great series Introduction (Everyone) JSON as logs format (Everyone) Logging best practices with Logback (Targetting Java DEVs) Logging cutting-edge practices (Targetting Java DEVs) Contract first log generator (Targetting Java DEVs) ElasticSearch VRR Estimation Strategy (Targetting OPS) VRR Java + Logback configuration (Targetting OPS) VRR FileBeat configuration (Targetting OPS) VRR Logstash configuration and Index templates (Targetting OPS) VRR Curator configuration (Targetting OPS) Logstash Grok, JSON Filter and JSON Input performance comparison (Targetting OPS) Logging Anti Patterns \u00b6 Logging Anti-Patterns by Rolf Engelhard : Logging Anti-Patterns Logging Anti-Patterns, Part II Logging Anti-Patterns, Part III Clean Code, clean logs \u00b6 Tomasz Nurkiewicz has a great series on logging: Clean code, clean logs: use appropriate tools (1/10) Clean code, clean logs: logging levels are there for you (2/10) Clean code, clean logs: do you know what you are logging? (3/10) Clean code, clean logs: avoid side effects (4/10) Clean code, clean logs: concise and descriptive (5/10) Clean code, clean logs: tune your pattern (6/10) Clean code, clean logs: log method arguments and return values (7/10) Clean code, clean logs: watch out for external systems (8/10) Clean code, clean logs: log exceptions properly (9/10) Clean code, clean logs: easy to read, easy to parse (10/10) Condensed 10 Tips on javacodegeeks JSON Logging \u00b6 Logging in JSON Write Logs for Machines, not Humans Maple \u00b6 Maple Eliot \u00b6 Eliot Eliot Tree TreeLog \u00b6 Treelog Bunyan \u00b6 Bunyan stands out for a number of innovations: ring buffers and JSON specifically. Bunyan Comparison of Winston and Bunyan Service logging in JSON with Bunyan Bunyan Logging in Production at Joyent Timbre \u00b6 Timbre Logback Encoders and Appenders \u00b6 concurrent-build-logger (encoders and appenders both) logzio-logback-appender logback-elasticsearch-appender logback-more-appenders logback-steno logslack Lessons Learned Writing New Logback Appender Extending logstash-logback-encoder","title":"Further Reading"},{"location":"reading/#further-reading","text":"Everything I write on logging is going to be here: Terse Systems","title":"Further Reading"},{"location":"reading/#best-practices","text":"Many of these are logback specific, but still good overall. 9 Logging Best Practices Based on Hands-on Experience Woofer: logging in (best) practices : Spring Boot A whole product concern logging implementation There is more to logging than meets the eye Monitoring demystified: A guide for logging, tracing, metrics Application-Level Logging Best Practices Stack Overflow has a couple of good tips on SLF4J and Logging: When to use the different log levels Why does the TRACE level exist, and when should I use it rather than DEBUG? Best practices for using Markers in SLF4J/Logback Stackoverflow: Logging best practices in multi-node environment","title":"Best Practices"},{"location":"reading/#level-up-logs","text":"Alberto Navarro has a great series Introduction (Everyone) JSON as logs format (Everyone) Logging best practices with Logback (Targetting Java DEVs) Logging cutting-edge practices (Targetting Java DEVs) Contract first log generator (Targetting Java DEVs) ElasticSearch VRR Estimation Strategy (Targetting OPS) VRR Java + Logback configuration (Targetting OPS) VRR FileBeat configuration (Targetting OPS) VRR Logstash configuration and Index templates (Targetting OPS) VRR Curator configuration (Targetting OPS) Logstash Grok, JSON Filter and JSON Input performance comparison (Targetting OPS)","title":"Level Up Logs"},{"location":"reading/#logging-anti-patterns","text":"Logging Anti-Patterns by Rolf Engelhard : Logging Anti-Patterns Logging Anti-Patterns, Part II Logging Anti-Patterns, Part III","title":"Logging Anti Patterns"},{"location":"reading/#clean-code-clean-logs","text":"Tomasz Nurkiewicz has a great series on logging: Clean code, clean logs: use appropriate tools (1/10) Clean code, clean logs: logging levels are there for you (2/10) Clean code, clean logs: do you know what you are logging? (3/10) Clean code, clean logs: avoid side effects (4/10) Clean code, clean logs: concise and descriptive (5/10) Clean code, clean logs: tune your pattern (6/10) Clean code, clean logs: log method arguments and return values (7/10) Clean code, clean logs: watch out for external systems (8/10) Clean code, clean logs: log exceptions properly (9/10) Clean code, clean logs: easy to read, easy to parse (10/10) Condensed 10 Tips on javacodegeeks","title":"Clean Code, clean logs"},{"location":"reading/#json-logging","text":"Logging in JSON Write Logs for Machines, not Humans","title":"JSON Logging"},{"location":"reading/#maple","text":"Maple","title":"Maple"},{"location":"reading/#eliot","text":"Eliot Eliot Tree","title":"Eliot"},{"location":"reading/#treelog","text":"Treelog","title":"TreeLog"},{"location":"reading/#bunyan","text":"Bunyan stands out for a number of innovations: ring buffers and JSON specifically. Bunyan Comparison of Winston and Bunyan Service logging in JSON with Bunyan Bunyan Logging in Production at Joyent","title":"Bunyan"},{"location":"reading/#timbre","text":"Timbre","title":"Timbre"},{"location":"reading/#logback-encoders-and-appenders","text":"concurrent-build-logger (encoders and appenders both) logzio-logback-appender logback-elasticsearch-appender logback-more-appenders logback-steno logslack Lessons Learned Writing New Logback Appender Extending logstash-logback-encoder","title":"Logback Encoders and Appenders"}]}